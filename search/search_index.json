{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"NREL HPC Resources","text":""},{"location":"#intro","title":"Intro","text":"<p>A collection of various resources, examples, and executables for the general NREL HPC user community's benefit.</p> <p>Documentation</p>"},{"location":"#contributing","title":"Contributing","text":"<p>These docs are driven by the NREL HPC community. They are currently under active development. If you would like to contribute or recommend a topic to be covered please open an issue or pull request in the repository. </p> <p>Docs repository </p>"},{"location":"#workshops","title":"Workshops","text":"<p>The HPC community also hosts workshops covering various topics. Check the training calendar below as well as the Computational Sciences Tutorials team to view all tutorials and workshops put on by the Computational Science Center. Join the team to receive notifications, seek out new or upcoming tutorials, see past slide desks, and access recordings of past trainings. For more information on joining this channel as a user external to NREL, see our CSC Tutorials Team - External Users. </p>"},{"location":"#additional-nrel-resources","title":"Additional NREL resources","text":"<ul> <li>User Basics</li> <li>Eagle Slurm partitions</li> </ul>"},{"location":"#calendar","title":"Calendar","text":"<p>Calendar of training events and office hours for NREL's HPC. </p> <p>"},{"location":"Announcements/2020-11-03-announcement/","title":"ESIF-HPC-3 Project Update","text":"<p>The ESIF-HPC-3 project has begun! The effort to acquire Eagle\u2019s successor involves ongoing engagement with stakeholders (EERE, Lab Program Management, and the HPC user community), tracking industry trends, analysis of Eagle\u2019s workload over its life to date, external design review, and a carefully managed Request for Proposals targeted for release later this year. We are currently in the process of completing the draft technical specifications for the ESIF-HPC-3 system, and are targeting the start of FY23 for general production access.</p> <p>If you would like to weigh in on how your work would benefit from existing or new features you could envision, please feel free to send a note to hpc-help@nrel.gov. We will open a discussion on the draft design in its current form if there is sufficient interest.</p>"},{"location":"Announcements/2020-11-03-announcement/#lustre-quotas","title":"Lustre Quotas","text":"<p>Effective with the new Fiscal Year 2021 Project allocations for Eagle, quotas for approved storage allocations' capacities have been implemented on /projects and MSS on Eagle. This was to encourage users to manage their /projects data usage and usage of /scratch for jobs. HPC Operations is developing reporting capabilities of usage, but in the mean time, users may request help from the HPC Help Desk, or utilize these procedures from an Eagle Login node:</p> <p>To view project quotas and usage:</p> <p>Get the ProjectID for your /projects directory: lfs project -d /projects/csc000</p> <p>110255 P /projects/csc000</p> <p>Get the usage and quota in kbytes: lfs quota -p 110255 /projects/csc000</p> <p>Disk quotas for prj 110255 (pid 110255):</p> <pre><code>   Filesystem  kbytes   quota   limit   grace   files   quota   limit   grace\n</code></pre> <p>/projects/csc000 3165308*   3072    4096       -      48  1073741824 2147483648       - An * means you have exceeded your soft quota of 3072kb, the hard limit of 4096kb reached means no more writes are allowed. Grace period is set to default of 7 days but will show time until writes are suspended. \"files\" indicate the number of inodes used and soft and hard limits.</p> <p>We encourage users to run their jobs in Eagle /scratch and copy results and other necessary project files to /projects, possibly using tar and zip to conserve space (tar czvf tar-file-name.tgz source-directory-files-to-tar-and zip).</p> <p>If you are over your project quota, we recommend removing unneeded files and directories or moving them to your /scratch directory until no longer needed.  Remember /scratch files are purged regularly per NREL\u2019s HPC storage retention policies.</p>"},{"location":"Announcements/2020-11-03-announcement/#changes-to-eagle-mass-storage-system-mss","title":"Changes to Eagle Mass Storage System (MSS)","text":"<p>What: NREL HPC Operations is in the process of retiring the on-premise MSS capability and has started using cloud-based data storage capability.</p> <p>Why: The vendor has announced end-of-life for the technology previously used to provide MSS.  In exploring alternatives, a cloud-based solution leverages expertise of the Advanced Computing Operations team and is significantly more cost effective. Adequate bandwith is available to/from Cloud (10x more bandwith than to/from the previous on-premise MSS).  Also a very small percentage of data written to MSS is ever read again, thus prompting a change to a fixed 15-month retention from when data is written to MSS.</p> <p>When/How: On December 1st, all new writes to MSS will be to cloud-based storage.  Reading data from the existing on-premise MSS capability will be supported through March 31, 2021.  Active data (written or last read within the last 15 months prior to December 1st, 2020) will be migrated from the on-premise MSS to the new cloud-based storage.</p> <p>What stays the same: MSS is an additional location available to projects active on Eagle to keep and protect important data in addition to the Eagle high-performance storage (/projects, /shared-projects, /datasets)</p> <p>What changes: Data will be retained for 15 months from the date written.  This differs from the current retention policy of minimum 15 months with deletion if needed. Restore requests of MSS data that are cloud-based will initially require a request to the HPC Help Desk, and may require 48 hours to be able to recover.</p> <p>Also a reminder:  Project data (/projects) for FY20 projects not continuing into FY21 will have until December 31, 2020 to move data off Eagle to MSS or other long-term storage, before it is purged from Eagle on January 1st, 2021.</p>"},{"location":"Announcements/2020-12-03-announcement/","title":"FY20 Expired Projects' Data","text":"<p>Reminder that FY20 expired Projects' data will be removed from Eagle on January 1st, 2021.  Any data needed needs to either be copied to the new AWS MSS or other arrangements made outside of HPC.</p> <p>Due to vendor ending support for the old MSS equipment, the new HPC Mass Storage System (MSS) environment will reside on Amazon Web Services. The old Gyrfalcon MSS data will be made read-only on December 1st, 2020. Any data 15 months old or less will be migrated to AWS MSS. </p>"},{"location":"Announcements/2020-12-03-announcement/#changes-to-eagle-mass-storage-system-mss","title":"Changes to Eagle Mass Storage System (MSS)","text":"<p>What: NREL HPC Operations is in the process of retiring the on-premise MSS capability and has started using cloud-based data storage capability.</p> <p>Why: The vendor has announced end-of-life for the technology previously used to provide MSS.  In exploring alternatives, a cloud-based solution leverages expertise of the Advanced Computing Operations team and is significantly more cost effective. Adequate bandwith is available to/from Cloud (10x more bandwith than to/from the previous on-premise MSS).  Also a very small percentage of data written to MSS is ever read again, thus prompting a change to a fixed 15-month retention from when data is written to MSS.</p> <p>When/How: On December 1st, all new writes to MSS will be to cloud-based storage.  Reading data from the existing on-premise MSS capability will be supported through March 31, 2021.  Active data (written or last read within the last 15 months prior to December 1st, 2020) will be migrated from the on-premise MSS to the new cloud-based storage.</p> <p>What stays the same: MSS is an additional location available to projects active on Eagle to keep and protect important data in addition to the Eagle high-performance storage (/projects, /shared-projects, /datasets)</p> <p>What changes: Data will be retained for 15 months from the date written.  This differs from the current retention policy of minimum 15 months with deletion if needed. Restore requests of MSS data that are cloud-based will initially require a request to the HPC Help Desk, and may require 48 hours to be able to recover.</p>"},{"location":"Announcements/2020-12-03-announcement/#esif-hpc-3-project-update","title":"ESIF-HPC-3 Project Update","text":"<p>The ESIF-HPC-3 project moves on. The Request for Proposals and its many pieces (including the technical specifications, benchmark suite and specifications, and workload analysis) should be live by the time you read this. At this time, our hope is that vendors are busy designing the next-generation computing and storage systems that will serve EERE research starting in FY23, and preparing proposals for review by a cross-Directorate NREL Source Evaluation Team starting in mid-January 2021.</p>"},{"location":"Announcements/2021-01-04-announcement/","title":"Arbiter2","text":"<p>On Tuesday, January 12, we will be upgrading the Arbiter2 software on the Eagle login nodes. The upgrade improves stability of the program, as well as fixes some broken features.</p> <p>Arbiter2 limits individual resources on these shared resources within a range. Certain processes (for example, those related to code compilation) are exempt from these limits. For other processes, consistently high processor utilization leads to resource throttling in order to equalize the net amount of resource users have access to over time. As usage returns below a level consistent with the smooth operation of the shared login node, the throttling is relaxed. Users exceeding per-user resource limits on login nodes (\"in violation\") will receive emails when they trigger a violation, and when their usage returns below limits. From users' perspective the upgrade will not change limits or throttling behavior, it will just turn on notifications.</p>"},{"location":"Announcements/2021-01-04-announcement/#esif-hpc-3-project-update","title":"ESIF-HPC-3 Project Update","text":"<p>The ESIF-HPC-3 Request for Proposals is live! For those interested, the content can be found on SAM.gov.</p>"},{"location":"Announcements/2021-01-04-announcement/#application-updates","title":"Application Updates","text":"<ul> <li>Q-Chem has been upgraded to version 5.3.2. See changes here.</li> <li>Star-CCM version 15.06.008 is available on Eagle.</li> <li>ARM Forge version 20.2 is available on Eagle.</li> </ul> <p>We are working on acquiring a Maintenance license for VASP 6. Once we have this in place, users will need to have an upgraded VASP 6 Research workgroup license in order to use our VASP 6 builds on Eagle.</p>"},{"location":"Announcements/2021-02-02-announcement/","title":"Eagle Outage","text":"<p>Eagle will be taking an outage on March 2nd. We will be making time critical hardware repairs  to the Lustre file system and doing some security patching. Small tweaks are being made to slurm to  reduce the amount of advertised memory and reduce the number of E-cells jobs run on.  Both should make  nodes and jobs more reliable and increase performance for larger jobs.</p>"},{"location":"Announcements/2021-02-02-announcement/#jupyterhub-documentation","title":"Jupyterhub Documentation","text":"<p>We have written up a how-to guide for using the Europa Jupyterhub server, including setting up custom Python,  Julia, and R kernels and interacting with Eagle. See https://www.nrel.gov/hpc/jupyterhub.html for more.</p>"},{"location":"Announcements/2021-02-02-announcement/#esif-hpc-3-project-update","title":"ESIF-HPC-3 Project Update","text":"<p>We have pushed the proposal deadline out to February 18th and expect to have reviews completed sometime  in mid-April. We have been fielding questions from various interested offerors, and are looking forward  to seeing what they've got!</p>"},{"location":"Announcements/2021-03-03-announcement/","title":"Elevate Your Work With New Tracking for Advanced Computing in the NREL Publishing Tracker","text":"<p>There is a new question on the User Facilities &amp; Program Areas page when you enter a publication into the Pub Tracker  \u2013 \"The High Performance Computing Facility was used to produce results or data used in this publication.\" Please be  sure to check Yes on this question for your work that made use of the HPC User Facility or other systems in the ESIF  HPC Data Center. In addition, there are three new Program Areas to use to tag your publication under the Advanced  Computing heading: Cloud, HPC and Visualization &amp; Insight Center. Making use of these metadata will enable us to  elevate your work through communications highlights, feature stories, and reporting to EERE.</p> <p>More information about the NREL Publishing Tracker can  be found by visiting the  Access and Use the NREL Publishing Tracker page on the Source. </p>"},{"location":"Announcements/2021-03-03-announcement/#fiscal-year-2021-quarterly-allocation-reductions","title":"Fiscal Year 2021 Quarterly Allocation Reductions","text":"<p>You may have noticed that NREL did not make any reductions to allocations that were under-using their AUs during Q1.  This is for two reasons. First, we were in the process of putting together a new, more transparent, fairer allocation  reduction policy. Second, we are aware that many users were inconvenienced by the fact that allocation decisions were  issued on October 1.</p> <p>We realize allocation reductions for low use are not popular with our users. However, they are physically necessary. AUs  are an \"expiring resource.\" If an AU is not used in Q1, it cannot be stored and saved for use in Q4. Because of this,  we have to remove some percentage of the unused AUs every quarter. Otherwise we can hit a situation where we have many  more AUs available to users than the machine can physically provide as the year progresses. This creates long queue times  that make Eagle physically unusable.</p> <p>Our new allocation policy is given below.  We had an informal discussion with users across multiple centers before designing  this policy. Users emphasized the need for a policy to be clear enough so they could see what they might lose at the end of  the quarter. Users have also increasingly requested allocations that had different usages in different quarters to deal with  their project needs, and we wanted a policy that treated these fairly.     </p> <p>Shortly after Q1, Q2, and Q3 ends, allocations will be automatically adjusted to account for low utilization against  planned usage. At the end of each quarter, the total allocation units used for the year to date will be compared to  the total planned usage for the year to date for each allocation. At the end of Q1, the total used for the year to  date is compared to the Q1 planned usage; at the end of Q2, the total used for the year to date is compared to the  Q1+Q2 planned usage, and at the end of Q3, the toal used for the year to date is compared to the Q1+Q2+Q3 planned usage. </p> <p>Allocation units are then removed based on the table below. Note that allocation reductions are meant to be cumulative  over the course of a year, and not compounding. If, for instance, a project was reduced by 10,000 AUs for low usage  in Q1, and the reduction table suggests the project should be reduced by 25,000 AUs at the end of Q2, 15,000  (25,000-15,000) AUs should be removed at the end of Q2 to make the total removal for the year to date equal to 25,000 AUs.</p> Percentage of planned AUs used to date Percentage of planned to date AUs removed More than 70% 0% (No reduction) Less than 70% but greater than 55% 20% Less than 55% but greater than 40% 35% Less than 40% but greater than 20% 55% Less than 20% 80% <p>To understand how this process would work, we consider the following two 100,000 AU allocations, one with a  uniform distribution of planned AUs throughout the year, and one with a distribution designed to enable development in Q1 and production runs in Q2 through Q4. The two allocations are described in the table below.</p> Quarter Allocation \"Renewables\" Allocation \"Efficiency\" Q1 25,000 10,000 Q2 25,000 30,000 Q3 25,000 30,000 Q4 25,000 30,000 <p>If \"Renewables\" uses 9,000 AUs in Q1, 20,000 AUs in Q2, and 25,000 AUs in Q3, it would be reduced in the following manner: </p> <p>After Q1, the project will have used 36% (9,000/25,000) if its allocation, leading to 13,750 (55% x 25,000) AUs being removed.    </p> <p>After Q2, the project will have used 58% [(9,000+20,000)/(25,000+25,000)] of its allocation, leading to 10,000 AUs  (20% x 50,000) being potentially removed. However, because 12,500 AUs were removed in Q1, no removal is performed. Note  that AUs are not restored in this case. </p> <p>After Q3, the project will have used 72 % [(9,000+20,000+25,000)/(25,000+25,000+25,000)] of its allocation. No reduction  would be made. The project would then begin Q4 with 33,500 AUs (100,000-9,000-20,000-25,000-12,500). </p> <p>If \"Efficiency\" uses AUs under the same schedule, after Q1, the project will have used 90% (9,000/10,000) of its  allocation and will not be penalized. </p> <p>After Q2, the project will have used 72.5% [(9,000+20,000)/(10,000+30,000)] of its allocation and will not be penalized. </p> <p>After Q3, the project will have used 77% [(9,000+20,000+25,000)/(10,000+30,000+30,000)] of its allocation, and will not be  penalized. The project would then begin Q4 with 46,000 AUs.  </p> <p>Note that \"Renewables\" and \"Efficiency\" have the same total (100,000 AUs) but lose very different amounts of AUs over  the course of the year. This is because the allocation request \"Efficiency\" is more closely tuned to the user\u2019s actual  us of HPC resources. NREL allows users to tune their allocation request through the use of \"profiles\" in the allocation  request to avoid this sorts of reductions.</p> <p>This information can also be found by visiting the  Fiscal Year 2021 Quarterly Allocation Reductions page on our website.</p>"},{"location":"Announcements/2021-04-06-announcement/","title":"Eagle File System Usage","text":"<p>The Lustre file systems that hosts /projects and /scratch works most efficiently when it is under 80% full. Please do your part to  keep the file system under 80% by cleaning up your /projects and /scratch spaces. </p>"},{"location":"Announcements/2021-04-06-announcement/#eagle-system-time","title":"Eagle System Time","text":"<p>Eagle will have a system time for a work week starting May 3rd. There will be a power outage in the data center at this time, and  we will need to do some work on Eagle as well. </p>"},{"location":"Announcements/2021-04-06-announcement/#fy22-hpc-allocation-process","title":"FY22 HPC Allocation Process","text":"<p>The Eagle allocation process for FY22 will open up in May, with applications due in June (the exact dates are still to be decided.) The  application process will be very similar to FY21. HPC Operations will again host a seminar to explain the application process. Watch  this space for announcements.</p>"},{"location":"Announcements/2021-04-06-announcement/#fy22-cloud-allocation-process","title":"FY22 Cloud Allocation Process","text":"<p>HPC and Cloud are both supported by Advanced Computing Operations (ACO) in the Computational Sciences Center. We are aligning the  request process for both computing and cloud resources. </p> <p>The Cloud allocation process for FY22 will open up in May, with applications due in June (the exact dates are still to be decided.)  The  annual allocation process for cloud resources will be updated this year, and a new web interface will be provided for submitting your  request. Cloud allocations have a different funding model from high performance computing: baseline services and administration of the cloud environment is funded by NREL, and project-specific computing and services is funded directly by the project. This means it\u2019s  important to work with the cloud team to determine your needs in the cloud and to generate cost estimates for your project. Requests  for cloud resources are reviewed and approved by the IACAC.</p> <p>Watch for the Call for Requests notification, and attend the upcoming Cloud Allocation Request webinar in May to learn more!</p>"},{"location":"Announcements/2021-04-06-announcement/#coming-soon-new-cloud-user-website","title":"Coming Soon: New Cloud User Website","text":"<p>We are transitioning from the CSC Cloud Team Wiki to a website modeled after https://hpc.nrel.gov. The website will have a similar structure and look and feel that the user community is accustomed to.</p>"},{"location":"Announcements/2021-05-05-announcement/","title":"Slurm Fairshare Refresher","text":"<p>FY21 saw the introduction of the \"fairshare\" priority algorithm in Eagle's job scheduler, Slurm. Queue times have been high during the Q2-Q3 rush and we've received some questions, so here's a quick refresher on Fairshare and what it means in regards to job scheduling.</p> <p>The fairshare algorithm is a part of the Slurm \"multi-factor priority\" plugin that determines when a job should run. This algorithm is designed to help moderate queue usage by promoting jobs from under-utilized allocations, while over-utilized allocations get shifted towards CPU time that would otherwise be idle. The base fairshare value for an allocation is determined by the number of AUs allocated to a project, and is currently re-calculated on a quarterly basis. Every job that runs will affect the fairshare value, reducing the priority of future jobs. Larger jobs will have a larger impact, running smaller jobs will have less of an impact. The effects of any job on fairshare value will reduce by half every two weeks. And most importantly, fairshare only accounts for about half of job priority calculations--the rest relies on other factors, including the job's size, QOS setting, and partition.</p>"},{"location":"Announcements/2021-05-05-announcement/#queue-times","title":"Queue Times","text":"<p>The allocation year transitioned from Q2 to Q3 on April 1st. The job queue leading up to the end of Q2 saw a very large spike in jobs submitted, and queue depth (job wait time) rose accordingly. A few projects saw some effect of fairshare, but much of the pressure came from over a third of all jobs being submitted as qos=high. Because of the large surge in jobs submitted, interactions with fairshare and a few projects that have used up their allocation we have been analyzing the scheduling algorithms. Based on some recommendations from SchedMD and internal analysis we have made a few adjustments to the slurm configuration. Those changes thus far appear to have alleviated some of the pressure on the queues as well as a reduction in the number of jobs submitted with qos=high.</p>"},{"location":"Announcements/2021-05-05-announcement/#advanced-jupyter-workshop-10am-may-13th-2021","title":"Advanced Jupyter workshop (10am May 13th, 2021)","text":"<p>Beyond the basics: this advanced Jupyter workshop will survey topics which enable you to get more out of your interactive notebooks. It will build on the recent Intro to Jupyter workshop and introduce additional Magic commands. Interacting with Slurm from a notebook will also be covered, and how this can be used to achieve multi-node parallelism. Additional topics include utilizing GPUs from a notebook, and parameterized notebook execution with Papermill.</p>"},{"location":"Announcements/2021-06-01-announcement/","title":"Fiscal Year 2022 HPC Annual Call for Allocation Requests","text":"<p>The deadline for requests for HPC requests is next Monday, June 7, at Midnight Mountain Time. The submission portal will remain open after this date.  However, late requests will receive lower priority than on-time requests. </p> <p>Please submit a request if you are a researcher at any national laboratory, university, or other organization pursing EERE-funded research, or if you  are an NREL-affiliated researcher performing research aligned with the EERE mission funded through other organizations. Requests are welcome for  current projects, projects where a funding request has been submitted, and projects where a funding request for FY21 is in preparation.</p> <p>Additional information on the Eagle allocation process is available at our Resource Allocation Requests page. Please e-mail hpc-requests@nrel.gov if you have any additional questions.</p>"},{"location":"Announcements/2021-06-01-announcement/#csc-user-and-applications-support","title":"CSC User and Applications Support","text":"<p>A new Anaconda installation is in testing, and will be put into production shortly. To access the test installation, as always just enable the Test modules collection via</p> <p><code>module use /nopt/nrel/apps/modules/test/modulefiles</code></p> <p>and you should then see a conda/4.9.2 module.</p> <p>Unlike with previous Anaconda installations, we have enabled the <code>conda activate</code> and <code>conda deactivate</code> syntax without requiring <code>conda init</code> (which  creates \"environmental\" problems, something we're all against). The \"source\"ing syntax still works, but you now have the option to use either.  Our hope is that enabling the conda commands will permit greater interoperability with scripts developed elsewhere (where conda activate may have  worked), perhaps prove slightly faster, and eliminate awkward error messages.</p> <p>This Conda deployment includes a new faster command for setting up new environments and installing packages. Instead of <code>conda ...</code> ,  you can try <code>mamba ...</code> , e.g., <code>mamba install tensorflow</code>.</p> <p>Other application upgrades have been or will be deployed shortly. If they are not already in production, you may access the installations via the module use statement above.</p> App Version ANSYS 2021R1 CMake 3.18.2 COMSOL 5.6 CUDA 11, includes cudnn and development tools and libraries GAMS 34.3.0 MATLAB R2020b MPT 2.23 OpenMPI 4.1.0, including Java support"},{"location":"Announcements/2021-06-01-announcement/#aup-renewals","title":"AUP Renewals","text":"<p>You may have received an email from DocuSign (dse_NA3@docusign.net) requesting that you renew your NREL HPC Appropriate Use Policy (AUP).  We  are required to maintain these agreements and, should you receive one, it will be necessary for you to complete it within 30 days in order to  continue accessing HPC systems.  We appreciate your cooperation.</p>"},{"location":"Announcements/2021-07-06-announcement/","title":"CSC User &amp; Applications Support","text":"<p>We will be making the conda/4.9.2 module the default module for loading (i.e., without a version number specified). By way of reminders,</p> <ul> <li>If you need to reference the existing default Anaconda installation, add the version number to your module load statement in job scripts,  i.e., <code>module load conda/mini_py37_4.8.3</code> rather than just <code>module load conda</code>. Custom environments should interoperate with either version, though.</li> <li>This module permits <code>conda activate</code> and <code>conda deactivate</code> functionality without conda init. Don't use conda init, as it breaks login shell setup.</li> <li>Consider trying mamba instead of conda when setting up environments. For example, <code>mamba install</code> to add a package to a custom environment.</li> </ul> <p>Julia modules are now available on Eagle. The module files are available at /nopt/nrel/ecom/modulefiles. Currently, versions 1.5.4 and 1.6.1 are available.</p> <p>If you plan on using these module files regularly, you may wish to add this directory to your module search path with the command: <code>module use -a /nopt/nrel/ecom/modulefiles</code></p> <p>You can add this command to your .bash_profile or .bashrc file with the following command: <code>echo 'module use -a /nopt/nrel/ecom/modulefiles' &gt;&gt; .bash_profile</code></p> <p>(or .bashrc in place of .bash_profile).</p> <p>Once your module path is updated, simply load the desired Julia version module: <code>module load julia</code></p> <p>Questions or problems regarding Julia on Eagle can be sent to jonathan.maack@nrel.gov.</p>"},{"location":"Announcements/2021-07-06-announcement/#eagle-job-queue","title":"Eagle Job Queue","text":"<p>The number of running jobs on Eagle has been dipping on the weekends. Please think about submitting jobs to run over the weekend,  especially long weekends, so we can keep the system full.</p>"},{"location":"Announcements/2021-07-06-announcement/#eagle-system-time","title":"Eagle System Time","text":"<p>The next Eagle system time is scheduled for the week of August 2nd. This will be a multi day outage to do updates to the parallel  file system as well as take care of some hardware issues affecting the compute nodes.  Eagle and related file systems will be unavailable at during this system time. </p>"},{"location":"Announcements/2021-08-06-announcement/","title":"August CSC User &amp; Applications Support","text":"<p>Q-Chem 5.4 will be made generally available. Look for the q-chem/5.4 module to appear in module avail output.</p>"},{"location":"Announcements/2021-08-06-announcement/#eagle-compute-node-local-storage-and-limitations-in-tmp","title":"Eagle Compute Node Local Storage and Limitations in /tmp","text":"<p>The compute nodes on Eagle have local disks that may benefit your workload. Disk sizes  start at 1TB, with a limited number of nodes available with more storage. The storage is available at /tmp/scratch. </p> <p>/tmp is in memory, and is limited in size. We occasionally see jobs filling /tmp, and recommend users adjust their  environment to use /tmp/scratch.</p>"},{"location":"Announcements/2021-09-08-announcement/","title":"Reminder of Eagle Data Storage Policies for FY21 projects ending 9/30/2021","text":"<p>Eagle usage policies can be found at: HPC Policies</p> <p>Users are always strongly encouraged to remove any data on Eaglefs that is not needed, to benefit other users of this shared resource.</p> <p>In summary, data in /projects for allocations that end on 9/30/2021 will be purged on 12/31/2021.</p> <p>Users may continue to log in to HPC systems for a period of 3 months after the project enters the Expired state to move relevant data off of HPC primary storage (primarily /projects/) to another storage location. <p>Users may continue to request  MSS (Mass Storage System) files that have been archived, for a period of 15 months after the files have been initially archived.</p> <p>Eagle's /scratch files have a policy of potentially being purged if not accessed within 28 days.</p> <p>Please contact the HPC Help Desk with any questions at hpc-help@nrel.gov</p>"},{"location":"Announcements/2021-09-08-announcement/#october-is-the-start-of-the-2022-allocation-year","title":"October is the start of the 2022 allocation year","text":"<p>Scheduling will be paused on Eagle on October 1st to implement the new allocations.</p>"},{"location":"Announcements/2021-11-03-announcement/","title":"Reminder of Eagle Data Storage Policies for FY21 projects that ended on 9/30/2021","text":"<p>Eagle usage policies can be found at: https://www.nrel.gov/hpc/policies.html Users are always strongly encouraged to remove any data on Eaglefs that is not needed, to benefit other users of this shared resource. Eaglefs consists of /shared-projects, /datasets, /scratch and /projects.</p> <p>In summary, data in /projects for allocations that end on 9/30/2021 will be purged after 12/31/2021.</p> <p>Users may continue to log in to HPC systems for a period of 3 months after the project enters the Expired state to move relevant data off of HPC primary storage (primarily /projects/) to another storage location. The ability to write new data to /projects has been disabled for those projects that have expired. Instructions on how to archive data using AWS MSS (Mass Storage System) can be found here. <p>Users may continue to request MSS files that have been archived, for a period of 15 months after the files have been initially archived.  Eagle's /scratch files have a policy of potentially being purged if not accessed within 28 days.</p>"},{"location":"Announcements/2021-11-03-announcement/#fast-eagle-queue-times","title":"Fast Eagle Queue Times","text":"<p>Eagle is now one month into the FY22 allocation cycle, and many projects are still in the process of preparing their software, data, and experiments. For those who are prepared to start computational work right away, though, now is a great opportunity to submit your jobs and beat the quarterly rush. Please also keep in mind that Allocation Units (AUs) are a function of how many hours there are in a year. In other words, an AU that goes unspent now cannot be saved for later, it is lost forever--so start your computational work on Eagle as early as possible in the allocation cycle.</p>"},{"location":"Announcements/2021-12-08-announcement/","title":"Data Security Policy Reminder","text":"<p>Eagle and systems in the ESIF data center are managed under a \"low\" Authority to Operate (ATO) per the FIPS 199 standard. End users should be familiar with NREL's HPC Data Security Policy for this class of systems. The potential impact rating of data is the responsibility of the data owner.</p> <p>The most common data security risk on Eagle is misconfiguration or misunderstanding of file permissions. This may involve accidentally setting UNIX ownership and/or permissions or ACLs on a directory that make files readable outside of the intended audience, or failing to remove permissions from users that should no longer have access to files.</p> <p>HPC Leads and PI\u2019s can mitigate any potential data exposure or leaks by checking file and directory ownerships and permissions, as well as updating their project entitlement in Lex as needed. Alternatively, HPC Leads or PI's could consider utilizing the AWS cloud environment \"Stratus\" as it has been classified for \"moderate impact\" data.</p> <p>File and folder permissions are an important way to you projects data from unintended access outside the project. Due to the number of people with privileged access to on ESIF data center systems, they are not a reasonable control for data rated outside of low. Data that is covered under a CRADA must be agreed upon by the legal entities that signed the CRADA as to what are the appropriate controls for that data.</p> <p>We understand that data classification can be challenging, and security requirements vary from project to project. We recommend project PI's contact the NREL Legal Department or their respective legal department with any questions regarding the classification of their data and where it may be safely stored.</p>"},{"location":"Announcements/2021-12-08-announcement/#eagle-second-quarter-system-time","title":"Eagle Second Quarter System Time","text":"<p>The Eagle cluster will be offline for regular scheduled maintenance for the week beginning, January 10th at approximately 6:00am (Mountain), and will return to service on January 13th. Eagle login nodes, DAV/FastX nodes, all filesystems (including lustre and /home), Globus, and related support systems will be unavailable during this time.</p> <p>Network maintenance is planned during this time as well, and access to certain internet/external-facing HPC services (including eagle.nrel.gov, eagle-dav.nrel.gov, and the self-service password tool will be temporarily unavailable and all outbound traffic will have no access to the internet from the HPC datacenter. </p>"},{"location":"Announcements/2021-12-08-announcement/#reminder-of-eagle-data-storage-policies-for-fy21-projects-that-ended-on-9302021","title":"Reminder of Eagle Data Storage Policies for FY21 projects that ended on 9/30/2021","text":"<p>Eagle usage policies can be found on the Policies Page</p> <p>Users are always strongly encouraged to remove any data on Eaglefs that is not needed, to benefit other users of this shared resource. Eaglefs consists of /shared-projects, /datasets, /scratch and /projects.</p> <p>In summary, data in /projects for allocations that end on 9/30/2021 will be purged after 12/31/2021.</p> <p>Users may continue to log in to HPC systems for a period of 3 months after the project enters the Expired state to move relevant data off of HPC primary storage (primarily /projects/) to another storage location. The ability to write new data to /projects has been disabled for those projects that have expired. Instructions on how to archive data using AWS MSS (Mass Storage System) can be found here. <p>Users may continue to request MSS files that have been archived, for a period of 15 months after the files have been initially archived.</p> <p>Eagle's /scratch files have a policy of potentially being purged if not accessed within 28 days.</p>"},{"location":"Announcements/2022-01-05-announcement/","title":"January 2022 System Time Reminder","text":"<p>Eagle will be offline for regularly scheduled maintenance starting on Monday, January 10th, 2022 at 6:00am (Mountain Time.) The system is expected to return to service by January 13th, 2022.</p> <p>During this outage, the Eagle login nodes, DAV/FastX nodes, all filesystems (including lustre and /home), Globus, and other related support systems will be unavailable.  Notable tasks for this system time include an upgrade to the Slurm scheduler, filesystem maintenance, cooling systems maintenance and repair, updates to FastX on ed7, and adjustments to the Arbiter2 resource monitor.</p> <p>Network maintenance is planned during this time as well. Access to certain internet/external-facing HPC services (including eagle.nrel.gov, eagle-dav.nrel.gov, and the self-service password tool at https://hpcusers.nrel.gov/) will be temporarily unavailable, and all outbound traffic will have no access to the internet from the HPC data center.</p>"},{"location":"Announcements/2022-01-05-announcement/#final-reminder-of-eagle-data-storage-policies-for-fy21-projects-that-ended-on-9302021","title":"Final Reminder of Eagle Data Storage Policies for FY21 projects that ended on 9/30/2021","text":"<p>Eagle usage policies can be found on the HPC website policy page.</p> <p>In summary, data in /projects for allocations that end on 9/30/2021 will be purged after 12/31/2021.  Users may continue to log in to HPC systems for a period of 3 months after the project enters the Expired state to move relevant data off of HPC primary storage (primarily /projects/) to another storage location. The ability to write new data to /projects has been disabled for those projects that have expired. Instructions on how to archive data using AWS MSS (Mass Storage System) can be found on the Mass Storage page.  Users may continue to request MSS files that have been archived, for a period of 15 months after the files have been initially archived.  Eagle's /scratch files have a policy of potentially being purged if not accessed within 28 days. <p>Users are always strongly encouraged to remove any data on Eaglefs that is not needed, to benefit other users of this shared resource. Eaglefs consists of /shared-projects, /datasets, /scratch and /projects.</p>"},{"location":"Announcements/2022-01-05-announcement/#thank-you-for-participating-in-the-annual-survey","title":"Thank you for participating in the annual survey","text":"<p>We are extremely grateful to you for contributing your valuable time, your honest feedback, and your thoughtful suggestions.  We are committed to utilizing the information to implement worthwhile improvements to the environment and our processes to make the cloud experience more efficient.  We will share these implementations with you through our monthly newsletter.  </p>"},{"location":"Announcements/2022-01-05-announcement/#applications","title":"Applications","text":"<p>COMSOL will provide NREL an overview of the new COMSOL 6.0 release on January 25, 2022, from 11:00 AM - 12:00 PM MST.</p> <p>To register, go to the COMSOL event registration page.</p> <p>Attend to learn about the latest features, and ask questions about COMSOL and NREL-HPC.</p>"},{"location":"Announcements/2022-02-02-announcement/","title":"Changes to Slurm \"srun\" for Interactive Jobs","text":"<p>During the recent system time the Slurm job scheduler was upgraded. One of the side effects of this was a change in the way Slurm handles job steps internally in certain cases. This may affect the way some users run job steps with srun inside of interactive jobs (srun --pty $SHELL), so we wanted to provide some guidance as we work on updating our documentation to reflect this change. </p> <p>When running an interactive job with srun --pty $SHELL and then launching job steps on a node, a second srun is often used \"inside\" the first srun to launch certain software. For example, for users of Paraview, a Paraview server may be launched on an interactive node with \"srun -n 8 pvserver --force-offscreen-rendering\". (Certain GPU-enabled or MPI-enabled interactive software also functions in a similar manner.)</p> <p>This \"srun-inside-an-srun\" process will no longer function in the same way as in the past. Instead, the \"outer\" srun should be replaced with an salloc command. Salloc will accept the same arguments as srun, but \"--pty $SHELL\" will no longer be required. Salloc will automatically open a shell to the node once the job starts, and the \"inner\" srun can then be run successfully as normal.</p> <p>Other regular uses of srun and srun inside sbatch scripts should continue to behave as expected. </p> <p>For further technical details on this Slurm change, please see the Slurm 20.11 Release Notes regarding job steps, srun, and the new --overlap flag.</p>"},{"location":"Announcements/2022-02-02-announcement/#upcoming-outage","title":"Upcoming Outage","text":"<p>There is a whole campus power outage planned for April 1 for NREL's South Table Mesa (STM) campus. All Computational Science Center systems will be affected. More details will follow as the date approaches.</p>"},{"location":"Announcements/2022-03-02-announcement/","title":"March HPC Systems Power Outage","text":"<p>Eagle, Swift, Vermillion, Meridian, and all other related HPC systems, services, and filesystems will be unavailable beginning on Thursday, March 31st, 2022 due to scheduled facilities maintenance. The Eagle Operations Team will also be performing firmware updates, security patches, and updates to the GPU node images during this time. The outage is anticipated to start at 7:00am on Thursday, March 31st and last at least through Friday, April 1st, 2022, but it may extend through to the following Monday, April 4th, 2022. We will provide updates as more information about the full outage period becomes available.</p>"},{"location":"Announcements/2022-03-02-announcement/#standby-qos-now-available-on-eagle-swift","title":"Standby QOS now Available on Eagle &amp; Swift","text":"<p>Through the annual user survey and direct feedback, we have received requests to submit jobs to the standby queue on demand. We are now pleased to announce the capability to submit jobs to the standby Quality of Service (QoS) queue by using the <code>--qos=standby</code> flag. Please remember that standby jobs run with a very low priority, so these jobs may wait for a considerable period of time. However, the job will not incur any AU charges against your project's allocation.</p>"},{"location":"Announcements/2022-03-02-announcement/#eagle-local-io-performance","title":"Eagle local I/O performance","text":"<p>We\u2019ve received questions recently about disk types and performance on compute nodes. Eagle has two network file systems. Qumulo provides /home and /nopt. It is NFS and is not considered a performance file system. /home has snapshots for restoration of lost data, but should not be used as a replacement for a source code repository like Git. Lustre is our performance file system and provides /scratch, /projects, /shared-projects and /datasets.</p> <p>Eagle also has two options on nodes. /dev/shm, which is an in-memory space, which is fast but you need to balance its usage with your jobs memory usage. /tmp/scratch is physical storage. The type of storage and performance differ depending on node and that\u2019s what we hope to clarify. If we look under Compute Node Hardware Details on the central NREL HPC website, there are nodes listed as having SATA drives, and nodes listed as having SSDs. Our SATA drives are still spinning disks, while SAS (serial attached SCSI) is how the SSD\u2019s are connected to the node. We would generally expect the nodes with SSDs to perform better. Let\u2019s test that out with a simple test. This is a command we regularly use to verify Lustre OST (object storage target) performance. It\u2019s designed to write enough information so that you are seeing disk performance, and not just the performance of the storage controller of the disk: dd if=/dev/zero of=X bs=1M count=10k</p> <p>This is writing in file in chunks of 1M, 10k times, to X. It writes an 11GB file:</p> <p>Lustre: 1.6 GB/s per OST</p> <p>Node /dev/shm: 2.8 GB/s</p> <p>Node SATA spinning disk: 2.4 GB/s</p> <p>Node SAS SSD: 2.4 GB/s</p> <p>Surprising! There is not a difference between the two local disks. Let\u2019s do the same test, but instead of writing in 1M chunks, we will write in 10M chunks which will write a 107GB file. For this case, Lustre and /dev/shm maintain performance, but here\u2019s what we get for the two local disk types:</p> <p>Node SATA spinning disk: 146 MB/s</p> <p>Node SAS SSD: 1.9 GB/s</p> <p>That is a rather drastic drop off in performance for the SATA disk. So how your data writes to disk can drastically affect performance. A lot of tiny files will look the same between the two disk types, one large continuous write would differ.</p>"},{"location":"Announcements/2022-04-06-announcement/","title":"FY23 HPC Allocation Process","text":"<p>The Eagle allocation process for FY23 is scheduled to open up on May 11, with applications due June 8. The application process will be an update of the process used in FY23, with additional information requested to help manage the transition from Eagle to Kestrel. HPC Operations will host a webinar on May 17 to explain the application process.  Watch for announcements.</p>"},{"location":"Announcements/2022-04-06-announcement/#documentation","title":"Documentation","text":"<p>We would like to announce our user-contributed documentation repository and website for Eagle and other NREL HPC systems that is open to both NREL and non-NREL users. This repository serves as a collection of code examples, executables, and utilities to benefit the NREL HPC community. It also hosts a site that provides more verbose documentation and examples.  If you would like to contribute or recommend a topic to be covered please open an issue or a pull request in the repository. Our contribution guidelines offer more detailed instructions on how to add content to the pages.</p>"},{"location":"Announcements/2022-04-06-announcement/#eagle-login-node-etiquette","title":"Eagle login node etiquette","text":"<p>Eagle logins are shared resources that are heavily utilized. We have some controls in place to limit per user process use of memory and CPU that will ramp down your processes usage over time. We recommend any sustained heavy usage of memory and CPU take place on compute nodes, where these limits aren't in place. If you only need a node for an hour, nodes in the debug partition are available. We permit compiles and file operations on the logins, but discourage multi-threaded operations or long, sustained operations against the file system. We cannot put the same limits on file system operations as memory and CPU, therefore if you slow the file system on the login node, you slow it for everyone on that login. Lastly, Fastx, the remote windowing package on the ED nodes, is a licensed product. When you are done using FastX, please log all the way out to ensure licenses are available for all users.</p>"},{"location":"Announcements/2022-04-06-announcement/#csc-tutorials-team-external-users","title":"CSC Tutorials Team - External Users","text":"<p>Staff in the Computational Science Center host multiple tutorials and workshops on various computational science topics throughout the year, such as Visualization, Cloud, HPC, and others.  In Microsoft Teams, a \u201cComputational Sciences Tutorials\u201d public team was just created to be the hub for all such tutorials and workshops.</p> <p>As an external user, you will be able to view discussion board posts, resource files, our SharePoint Calendar, and lists of the upcoming schedule and related repo links. Unfortunately, you will not be able to access recordings or the survey.  The SharePoint Calendar provides a month view for upcoming tutorials, their descriptions, and links to join. If you miss the monthly announcements in our newsletters, you can access calendar events and find meeting information to join the tutorials in the Teams channel. The Upcoming Schedule provides a list view of the upcoming events and their tentative dates. </p> <p>For external users, you will receive an invite from the team.  Should you decide to join the public team, there a few steps you will need to take.  First, you need go through the steps to register a free Office365 account (or login if you already have an account). Next, you will need to download Microsoft Authenticator or another authenticator application.  The process is straightforward, and you will be prompted during each step of the process.  If you do not accept the invite or do not wish to go through the process of joining the public team, you can rely on the monthly newsletters or visit the Training Page on https://hpc.nrel.gov for meeting information. </p>"},{"location":"Announcements/2022-04-06-announcement/#instructions","title":"Instructions:","text":"<ul> <li>You will receive a welcome email from the team owner (sometime this week), with information about the team.  Click on accept. </li> <li>If you have never created a MS Office 365 account, you will prompted to create one. If you already have a MS Office 365 account, login. </li> <li>The first time you log in, you will be prompted to set up Microsoft Authenticator or other authenticator app.</li> <li>From your mobile device, Download and install the app from the Apple Store (for iOS) or the Google Play Store (for Android) and Open the app.</li> <li>On your mobile device, you will be prompted to allow notifications. Select Allow.</li> <li>On your mobile device, click OK on the screen for what information Microsoft gathers.</li> <li>Click Skip on the \"Add personal account\" page.</li> <li>Click Skip on the \"Add non-Microsoft account\" page.</li> <li>Click Add Work Account on the \"Add work account\" page.</li> <li>Click OK to allow access to the camera.</li> <li>Going forward, anytime you login, you will get a prompt on your phone to authenticate. </li> </ul>"},{"location":"Announcements/2022-05-04-announcement/","title":"FY23 HPC Allocation Process","text":"<p>The Eagle allocation process for FY23 is scheduled to open up on May 11, with applications due June 8. The application process will be an update of the process used in FY22, with additional information requested to help manage the transition from Eagle to Kestrel. Be sure to sign-up for the webinar on May 17 from 11-12 a.m. MT. Michael Martin, Staff Scientist in the Computational Science Center at NREL, will be presenting on the allocation process, key dates, changes from last year, and how to submit a request.  </p> <p>The registration site for the webinar is now available.</p>"},{"location":"Announcements/2022-05-04-announcement/#nrel-hpc-workshops-intro-to-hpc-series-save-the-dates","title":"NREL HPC Workshops - Intro to HPC Series (Save the Dates)","text":"<p>NREL HPC Operations and Application Support teams will host an Intro to HPC workshop series this June every Wednesday from 12-1 p.m.  Webinar information to follow.</p> <p>Topics/Schedule:</p> <ul> <li>Linux Fundamentals: Utilizing the Command Line Interface on June 1st 12-1</li> <li>NREL HPC Systems on June 8th 12-1</li> <li>Resource Management: Slurm on June 15th 12-1</li> <li>Software Environments on June 22nd 12-1</li> <li>JupyterHub on June 29th 12-1</li> </ul>"},{"location":"Announcements/2022-05-04-announcement/#workaround-for-windows-ssh-users","title":"Workaround for Windows SSH Users","text":"<p>Some people who use Windows 10/11 computers to ssh to Eagle from a Windows command prompt, powershell, or via Visual Studio Code's SSH extension have received a new error message about a \"Corrupted MAC on input\" or \"message authentication code incorrect.\" This error is due to an outdated OpenSSL library included in Windows and a recent security-mandated change to ssh on Eagle. However, there is a functional workaround for this issue. (Note: If you are not experiencing the above error, you do not need and should not use the following workaround.)</p> <p>For command-line and Powershell ssh users, adding \"-m hmac-sha2-512\" to your ssh command will resolve the issue. For example: \"ssh -m hmac-sha2-512 @eagle.hpc.nrel.gov\" <p>For VS Code SSH extension users, you will need to create an ssh config file on your local computer (~/.ssh/config), with a host entry for Eagle that specifies a new message authentication code:  <pre><code>Host eagle\n    HostName eagle.hpc.nrel.gov\n    MACs hmac-sha2-512\n</code></pre></p> <p>The configuration file will also apply to command-line ssh in Windows, as well. This Visual Studio Blog post has further instructions on how to create the ssh configuration file for Windows and VS Code.</p>"},{"location":"Announcements/2022-05-04-announcement/#lustre-filesystem-usage-reminder","title":"Lustre Filesystem Usage Reminder","text":"<p>The Lustre file systems that hosts /projects, /scratch, /shared-projects and /datasets works most efficiently when it is under 80% full. Please do your part to keep the file system under 80% by cleaning up your /projects, /scratch and /shared-projects spaces.</p>"},{"location":"Announcements/2022-05-04-announcement/#documentation","title":"Documentation","text":"<p>We would like to announce our user-contributed documentation repository and website for Eagle and other NREL HPC systems that is open to both NREL and non-NREL users. This repository serves as a collection of code examples, executables, and utilities to benefit the NREL HPC community. It also hosts a site that provides more verbose documentation and examples.  If you would like to contribute or recommend a topic to be covered please open an issue or a pull request in the repository. Our contribution guidelines offer more detailed instructions on how to add content to the pages.</p>"},{"location":"Announcements/2022-08-11-announcement/","title":"Annual Reports due August 31","text":"<p>FY22 HPC annual reports are due by August 31, 2022. If you have received an email prompting you to submit an annual report for any of your FY22 projects please follow the instructions provided. If you have any questions please contact hpc-reporting@nrel.gov.</p>"},{"location":"Announcements/2022-08-11-announcement/#notification-of-full-nsrdb-data-update","title":"Notification of Full NSRDB Data Update","text":"<p>Over the past few years, the NSRDB team has been hard at work improving their solar data product. There have been numerous improvements in software, data quality, cloud property algorithms, solar position algorithms, and surface albedo data. The current data and software version is v3.2.2, and you can see the NSRDB version history for more details. For the new and improved NSRDB data to proliferate, the NSRDB team will be manipulating the /datasets/NSRDB/ directory on Eagle and will eventually deprecate the data in the /datasets/NSRDB/v3/ directory. Here is an outline of the changes that will be made along with an estimated timeline:</p> <ul> <li>Before 8/13, the new NSRDB data will be copied to the /datasets/NSRDB/current/ directory (including data for 2021!)</li> <li>On 8/20, the old NSRDB data in /datasets/NSRDB/v3/ will be moved to /datasets/NSRDB/deprecated_v3/</li> <li>On 9/3, the data in /datasets/NSRDB/conus/ and /datasets/NSRDB/full_disc/ will be replaced with the new data (this high-res data is too big to keep two copies).</li> <li>On 10/1, the old NSRDB data in /datasets/NSRDB/deprecated_v3/ will be removed permanently.</li> </ul> <p>You should be aware of one significant change if you have hard-coded your site location index values into any code: the NSRDB team is updating the meta data in the standard 4km 30min NSRDB product. The old meta data had several errors and inconsistencies which will be fixed in the new meta. A mapping of site index values from the \u201cv3\u201d meta to the new meta can be found here and is also copied at /datasets/NSRDB/nsrdb_v3_to_current_map.csv on Eagle.</p> <p>Minor differences in the data should be expected, but please reach out to Grant Buster and Manajit Sengupta with anything you see that looks like a true error.</p> <p>Thanks for your cooperation and thanks for using the NSRDB!</p>"},{"location":"Documentation/","title":"Documentation Home","text":"<p>Welcome to the central source of user-contributed documentation for Eagle and other NREL HPC systems. This repository is open to both NREL and non-NREL HPC users. You can browse the documentation here, or start contributing by visiting the repository in Git for more information.</p>"},{"location":"Documentation/#where-to-begin","title":"Where to Begin","text":"<p>Please use the navigation bar on the left to explore the available documentation by category.</p>"},{"location":"Documentation/#highlights","title":"Highlights","text":"<ul> <li>Systems Guide to learn about our HPC systems</li> <li>Jupyterhub to get started with Jupyter Notebooks </li> <li>Conda environment howto and Eagle-specific information</li> </ul>"},{"location":"Documentation/#other-nrel-documentation-resources","title":"Other NREL Documentation Resources","text":"<ul> <li>The NREL HPC Website is the home of Advanced Computing at NREL</li> <li>Our Github Repository for specific application examples, scripts, workshop content, the contributor guide, and more. </li> <li>The gh-pages branch (this site) is also open for contribution.</li> </ul>"},{"location":"Documentation/Data-and-File-Systems/File-Systems/","title":"File systems","text":"<p>Eagle has three primary file systems available for compute nodes. Understanding the usage of these is important for achieving the best performance. </p>"},{"location":"Documentation/Data-and-File-Systems/File-Systems/#nrel-file-systems","title":"NREL file systems","text":"<ul> <li>Home file system<ul> <li>Quota of 50 GB</li> <li>Used to hold scripts, source code, executables</li> </ul> </li> <li>Lustre parallel file system: Accessiblle across all nodes. When using this file system please familiarize yourself with the best practices section <ul> <li>/scratch/username</li> <li>/projects</li> <li>/shared-projects</li> <li>/datasets</li> </ul> </li> <li>Node file system: The local drive on each node, these are accessible only on a given node. <ul> <li>/tmp/scratch<ul> <li>1 TB HDD (spinning disk, average performance) on compute nodes with 196GB or less RAM</li> <li>1.6 TB SSD (higher performance) on 78 bigmem/GPU nodes</li> <li>25.6 TB SSD (higher performance, maximum local storage) on 20 bigmem/GPU \"bigscratch\" nodes</li> </ul> </li> </ul> </li> </ul> <p>For more information on the file systems available on Eagle please see: Eagle System Configuration</p>"},{"location":"Documentation/Data-and-File-Systems/File-Systems/Lustre/lustrebestpractices/","title":"Lustre Best Practices","text":""},{"location":"Documentation/Data-and-File-Systems/File-Systems/Lustre/lustrebestpractices/#lustre-best-practices","title":"Lustre best practices","text":"<p>In some cases special care must be taken while using Lustre so as not to affect the performance of the filesystem for yourself and other users. The below Do's and Don'ts are provided as guidance. </p> <ul> <li> <p>Do</p> <ul> <li>Use the <code>lfs find</code><ul> <li>e.g.  <pre><code>lfs find /scratch/username -type f -name \"*.py\"\n</code></pre></li> </ul> </li> <li>Break up directories with many files into more directories if possible</li> <li>Store small files and directories of small files on a single OST (Object Storage Target) </li> <li>Limit the number of processes accessing a file. It may be better to read in a file once and then broadcast necessary information to other processes</li> <li>Change your stripecount based on the filesize</li> <li>Write many files to the node filesystem <code>/tmp/scratch/</code>: this is local storage on each node, and is not a part of the Lustre filesystem. Once your work is complete, the files can then be added to a tar archive and transferred to the <code>/project/project_name</code> for later use, or deleted from /tmp/scratch if no longer needed</li> <li>Store data and run executables from <code>/projects</code><ul> <li>Storing your conda environments in <code>/projects</code> can ensure that your data and executables are on the same filesystem, improving performance</li> </ul> </li> </ul> </li> <li> <p>Do Not</p> <ul> <li>Use <code>ls -l</code></li> <li>Have a file accessed by multiple processes</li> <li>In Python, avoid using <code>os.walk</code> or <code>os.scandir</code></li> <li>List files instead of using wildcards<ul> <li>e.g. don't use <code>cp * dir/</code></li> <li>If you need to tar/rm/cp a large number of files use xargs or similar: <pre><code>lfs find /scratch/username/old_data/ -t f -print0 | xargs -0 rm\n</code></pre></li> </ul> </li> <li>Have many small files in a single directory</li> <li>Store important files in <code>/scratch</code><ul> <li>e.g. don't keep data, libraries or programs in <code>/scratch/username</code>, as <code>/scratch</code> directories are subject to automated purging based on the Data Retention Policy</li> </ul> </li> </ul> </li> </ul>"},{"location":"Documentation/Data-and-File-Systems/File-Systems/Lustre/lustrebestpractices/#useful-lustre-commands","title":"Useful Lustre commands","text":"<ul> <li>Check your storage usage:<ul> <li><code>lfs quota -h -u &lt;username&gt; /scratch</code></li> </ul> </li> <li>See which MDT a directory is located on<ul> <li><code>lfs getstripe --mdt-index /scratch/&lt;username&gt;</code></li> <li>This will return an index 0-2 indicating the MDT</li> </ul> </li> <li>Create a folder on a specific MDT (admin only)<ul> <li><code>lfs mkdir \u2013i &lt;mdt_index&gt; /dir_path</code></li> </ul> </li> </ul>"},{"location":"Documentation/Data-and-File-Systems/File-Systems/Lustre/lustrebestpractices/#striping","title":"Striping","text":"<p>Lustre provides a way to stripe files, this spreads them across multiple OSTs. Striping a large file being accessed by many processes can greatly improve the performace. See Lustre file striping for more details. </p> <p><pre><code>lfs setstripe &lt;file&gt; -c &lt;count&gt; -s &lt;size&gt;\n</code></pre> * The stripecount determines how many OST the data is spread across * The stripe size is how large each of the stripes are in KB, MB, GB</p>"},{"location":"Documentation/Data-and-File-Systems/File-Systems/Lustre/lustrebestpractices/#references","title":"References","text":"<ul> <li>Lustre manual</li> <li>CU Boulder - Lustre Do's and Don'ts</li> <li>NASA - Lustre Best Practices</li> <li>NASA - Lustre basics</li> <li>UMBC - Lustre Best Practices</li> <li>NICS - I/O and Lustre Usage</li> <li>NERSC - Lustre</li> </ul>"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/FileZilla/","title":"Transferring files using FileZilla","text":"<p>FileZilla can be used to securely transfer files between your local computer running Windows, Linux or MacOS to a remote computer running Linux.</p>"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/FileZilla/#setting-up-filezilla","title":"Setting Up FileZilla","text":"<ul> <li>Download and install FileZilla.</li> </ul>"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/FileZilla/#connecting-to-a-host","title":"Connecting to a Host","text":"<ul> <li>Decide which host you wish to connect to such as, eagle.hpc.nrel.gov</li> <li>Enter your username in the Username field.</li> <li>Enter your password or Password+OTP Token in the Password field.</li> <li>Use 22 as the Port.</li> <li>Click the 'Quickconnect' button.</li> </ul>"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/FileZilla/#transferring-files","title":"Transferring Files","text":"<p>You may use FileZilla to transfer individual files or directories from the Local Directory to the Remote Directory or vice versa.</p> <p>Transfer files by dragging them from the Local Directory (left pane) to the Remote Directory (right pane) or vice versa.  Once the transfer is complete the selected file will be visible in the pane it was transferred to.</p>"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/file-transfers/","title":"Transferring files","text":"<p>Learn how to transfer data within, to and from NREL's high-performance computing (HPC) systems.</p> <p>A supported set of instructions for data transfer using NREL HPC systems is provided on the HPC NREL Website.</p>"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/file-transfers/#checking-usage-and-quota","title":"Checking Usage and Quota","text":"<p>The below command is used to check your quota from an Eagle login node.  <code>hours_report</code> will display your usage and quota for each filesystem.</p> <pre><code>$ hours_report\n</code></pre>"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/file-transfers/#best-practices-for-transferring-files","title":"Best Practices for Transferring Files","text":""},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/file-transfers/#file-transfers-between-filesystems-on-the-nrel-network","title":"File Transfers Between Filesystems on the NREL network","text":"<p>rsync is the recommended tool for transferring data between NREL systems. It allows you to easily restart transfers if they fail, and also provides more consistency when dealing with symbolic links, hard links, and sparse files than either scp or cp. It is recommended you do not use compression for transfers within NREL systems. An example command is:</p> <pre><code>$ rsync -aP --no-g /scratch/username/dataset1/ /mss/users/username/dataset1/\n</code></pre> <p>Mass Storage has quotas that limit the number of individual files you can store. If you are copying hundreds of thousands of files then it is best to archive these files prior to copying to Mass Storage. See the guide on how to archive files.</p> <p>Mass Storage quotas rely on the group of the file and not the directory path. It is best to use the <code>--no-g</code> option when rsyncing to MSS so you use the destination group rather than the group permissions of your source.  You can also <code>chgrp</code> your files to the appropriate group prior to rsyncing to MSS.</p>"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/file-transfers/#small-transfers-100gb-outside-of-the-nrel-network","title":"Small Transfers (&lt;100GB) outside of the NREL network","text":"<p><code>rsync</code>, <code>scp</code>, and <code>curl</code> will be your best option for small transfers (&lt;100GB) outside of the NREL network. If your rsync/scp/curl transfers are taking hours to complete then you should consider using Globus.</p> <p>If you're transferring many files then you should use rsync:</p> <pre><code>$ rsync -azP --no-g /mss/users/username/dataset1/ user@desthost:/home/username/dataset1/\n</code></pre> <p>If you're transferring an individual file then use scp:</p> <pre><code>$ scp /home/username/example.tar.gz user@desthost:/home/username/\n</code></pre> <p>You can use curl or wget to download individual files: <pre><code>$ curl -O https://URL\n$ wget https://URL\n</code></pre></p>"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/file-transfers/#large-transfers-100gb-outside-of-the-nrel-network","title":"Large Transfers (&gt;100GB) outside of the NREL network","text":"<p>Globus is optimized for file transfers between data centers and anything outside of the NREL network. It will be several times faster than any other tools you will have available. Documentation about requesting a HPC Globus account is available on the Globus Services page on the HPC website.  See Transfering files using Globus for instructions on transfering files with Globus.</p>"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/file-transfers/#transfering-files-using-windows","title":"Transfering files using Windows","text":"<p>For Windows you will need to download WinSCP to transfer files to and from HPC systems over SCP. See Transfering using WinSCP.</p>"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/file-transfers/#archiving-files-and-directories","title":"Archiving files and directories","text":"<p>Learn various techniques to combine and compress multiple files or directories into a single file to reduce storage footprint or simplify sharing.</p>"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/file-transfers/#tar","title":"tar","text":"<p><code>tar</code>, along with <code>zip</code>, is one of the basic commands to combine multiple individual files into a single file (called a \"tarball\"). <code>tar</code> requires at least one command line option. A typical usage would be: <pre><code>$ tar -cf newArchiveName.tar file1 file2 file3\n# or\n$ tar -cf newArchiveName.tar /path/to/folder/\n</code></pre></p> <p>The <code>-c</code> flag denotes creating an archive, and <code>-f</code> denotes that the next argument given will be the archive name\u2014in this case it means the name you would prefer for the resulting archive file. </p> <p>To extract files from a tar, it's recommended to use: <pre><code>$ tar -xvf existingArchiveName.tar\n</code></pre> <code>-x</code> is for extracting, <code>-v</code> uses verbose mode which will print the name of each file as it is extracted from the archive.</p>"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/file-transfers/#compressing","title":"Compressing","text":"<p><code>tar</code> can also generate compressed tarballs which reduce the size of the resulting archive. This can be done with the <code>-z</code> flag (which just calls <code>gzip</code> on the resulting archive automatically, resulting in a <code>.tar.gz</code> extension) or <code>-j</code> (which uses <code>bzip2</code>, creating a <code>.tar.bz2</code>).</p> <p>For example:</p> <pre><code># gzip\n$ tar -czvf newArchive.tar.gz file1 file2 file3\n$ tar -xvzf newArchive.tar.gz\n\n# bzip2\n$ tar -czjf newArchive.tar.bz2 file1 file2 file3\n$ tar -xvjf newArchive.tar.bz2\n</code></pre>"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/globus/","title":"Transfering Files with Globus","text":"<p>For large data transfers between NREL\u2019s high-performance computing (HPC) systems and another data center, or even a laptop off-site, we recommend using Globus.</p> <p>A supported set of instructions for requesting a HPC Globus account and data transfer using Globus is available on the HPC NREL Website</p>"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/globus/#what-is-globus","title":"What Is Globus?","text":"<p>Globus provides services for research data management, including file transfer. It enables you to quickly, securely and reliably move your data to and from locations you have access to.</p> <p>Globus transfers files using GridFTP. GridFTP is a high-performance data transfer protocol which is optimized for high-bandwidth wide-area networks.  It provides more reliable high performance file transfer and synchronization than scp or rsync. It automatically tunes parameters to maximize bandwidth while providing automatic fault recovery and notification of completion or problems.</p>"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/globus/#globus-personal-endpoints","title":"Globus Personal endpoints","text":"<p>You can set up a \"Globus Connect Personal EndPoint\", which turns your personal computer into an endpoint, by downloading and installing the Globus Connect Personal application on your system. We use a personal endpoint to demonstrate how to transfer files to and from Eagle.</p>"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/globus/#set-up-a-personal-endpoint","title":"Set Up a Personal EndPoint","text":"<ul> <li>Login to the Globus website. From the Manage Data drop down menu, select Transfer Files. Then click Get Globus Connect Personal.</li> <li>Pick a name for your personal endpoint and select Generate Startup Key. Follow the instructions on the web page to save your key.</li> <li>Download and install the Globus Connect Personal software on your personal system.</li> <li>Copy the startup key from the Globus web page to this application.</li> </ul>"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/globus/#configure-permissions-on-a-personal-endpoint","title":"Configure Permissions on a Personal EndPoint","text":"<p>Once Globus Connect Personal is installed on your system, set up the permissions for reading or writing files to your local system.</p> <ul> <li>If you are using a Mac, click on the \"g\" icon on the upper right portion of your screen to access the Globus Connect Personal application.</li> <li>Select Preferences. To allow Globus to copy files to your local system, make sure that the directory (folder) they will go in is Writable.</li> </ul>"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/globus/#transferring-files","title":"Transferring Files","text":"<p>You can transfer files with Globus through the Globus Online website or via a CLI (command line interface).</p>"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/globus/#globus-online","title":"Globus Online","text":"<p>Globus Online is a hosted service that allows you to use a browser to transfer files between trusted sites called \"endpoints\".  To use it, the Globus software must be installed on the systems at both ends of the data transfer. The NREL endpoint is nrel#eglobus.</p> <ul> <li>Click Login on the Globus web site. On the login page select \"Globus ID\" as the login method and click continue.  Use the Globus credentials you used to register your Globus.org account.</li> <li>Go to the Transfer Files page, the link is located under the Manage Data tab at the top of the page.</li> <li>Select nrel#eglobus as the endpoint on one right side. In the box asking for authentication, enter your Eagle (NREL HPC) username and password. Do not use your globus.org username and password when authenticating with the nrel#eglobus endpoint.</li> <li>Select another Globus endpoint, such as a personal endpoint or an endpoint at another institution that you have access to. To use your personal endpoint, first start the Globus Connect Personal application. Then enter \"USERNAME#ENDPOINT\" on the left side or use the drop down menu to find it. Click \"go\".</li> <li>To transfer files</li> <li>Select the files you want to transfer someplace else from the system from the dialog box on the left.</li> <li>Select the destination location (a folder or directory) from the dialog box on right right.</li> <li>Click the large blue button at the top of the screen to begin to transfer the files.</li> </ul> <p>When your transfer is complete, you will be notified by email.</p>"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/globus/#globus-cli-command-line-interface","title":"Globus CLI (Command line Interface)","text":"<p>Configuring your Globus.org account to allow ssh CLI access</p> <p>To use the CLI you must have a Globus account with ssh access enabled. To enable your account for ssh access you must add your ssh public key to your Globus account by visiting the Manage Identities page and clicking \"manage SSH and X.509 keys\" and then \"Add a New Key\". If you do not have an ssh key, follow the directions here to create one.</p> <p>Globus.org CLI examples</p> <pre><code>$ ssh &lt;globus username&gt;@cli.globusonline.org &lt;command&gt;  &lt;options&gt; &lt;params&gt;\n$ ssh &lt;globus username&gt;@cli.globusonline.org help\n</code></pre> <p>A one-liner can be used to integrate globus.org CLI commands into shell scripts</p> <pre><code>$ ssh &lt;globus username&gt;@cli.globusonline.org scp nrel#eglobus:/globusro/file1.txt myuser#laptop:/tmp/myfile.txt\n</code></pre> <p>The globus.org CLI can be used interactively</p> <pre><code>$ ssh &lt;globus username&gt;@cli.globusonline.org\nWelcome to globusonline.org, &lt;globus username&gt;. Type 'help' for help.\n$ help\n$ scp nrel#globus:/globusro/file1.txt myuser#laptop:/tmp/myfile.txt\n$ exit\n</code></pre> <p>You can find more information on the Globus CLI from the official Globus CLI documentation.</p>"},{"location":"Documentation/MachineLearning/tensorflow/","title":"Containerized TensorFlow","text":""},{"location":"Documentation/MachineLearning/tensorflow/#tensorflow-with-gpu-support-singularity-container","title":"TensorFlow with GPU support singularity container","text":"<p>This Singularity container supplies TensorFlow 2.3.0 optimized for use with GPU nodes.  It also has opencv, numpy, pandas, seaborn, scikit-learn python libraries.</p> <p>For more information on Singularity on please see: Containers </p>"},{"location":"Documentation/MachineLearning/tensorflow/#quickstart","title":"Quickstart","text":"<pre><code># Get allocation\nsalloc --gres=gpu:2 -N 1 -A hpcapps -t 0:10:00 -p debug\n# Run singularity in srun environment\nmodule load singularity-container\nunset LD_PRELOAD\nsrun --gpus=2 --pty singularity shell --nv /nopt/nrel/apps/singularity/images/tensorflow_gpu_extras_2.3.0.sif\n</code></pre>"},{"location":"Documentation/MachineLearning/tensorflow/#building-a-custom-image-based-on-tensorflow","title":"Building a custom image based on TensorFlow","text":"<p>In order to build a custom Singularity image based on this one, docker must be installed on your local computer.  Docker documentation shows how to install docker.</p> <ol> <li>Update Dockerfile shown below to represent the changes desired and save to working directory. <pre><code>FROM tensorflow/tensorflow:2.3.0-gpu-jupyter\nRUN apt-get update\nRUN DEBIAN_FRONTEND=\"noninteractive\" apt-get -y install python3-opencv\nRUN mkdir /custom_env\nCOPY requirements.txt /custom_env\nRUN pip install -r /custom_env/requirements.txt\n</code></pre></li> <li>Update requirements.txt shown below for changing the python library list and save to working directory. <pre><code>seaborn\npandas\nnumpy\nscikit-learn\ngit+https://github.com/tensorflow/docs\n</code></pre></li> <li>Build new docker image <pre><code>docker build -t tensorflow-custom-tag-name .\n</code></pre></li> <li>Create Singularity image file.  Note the ./images directory must be created before running this command. <pre><code>docker run -v /var/run/docker.sock:/var/run/docker.sock \\\n-v $(PWD)/images:/output \\\n--privileged -t --rm \\\nquay.io/singularity/docker2singularity --name tensorflow_custom.sif \\\ntensorflow-custom-tag-name\n</code></pre></li> <li>Transfer image file to Eagle.  For this example I created a directory named /scratch/$(USER)/tensorflow on eagle <pre><code>rsync -v images/tensorflow_custom.sif eagle.hpc.nrel.gov:/scratch/$(USER)/tensorflow/\n</code></pre></li> </ol>"},{"location":"Documentation/MachineLearning/ReinforcementLearning/","title":"Reinforcement Learning on Eagle","text":"<p>Welcome to the first NREL HPC tutorial for Reinforcement Learning (RL)! </p> <p>This tutorial covers an extended, albeit simplified, introduction of OpenAI Gym and Ray/RLlib which you can use to effortlessly design, create, and run your own RL experiments on Eagle. </p> <p>You can find the full material of this tutorial in the NREL/HPC GitHub repo.</p> <p>The tutorial covers the following:</p> <ul> <li>Brief introduction to RL and Ray</li> <li>Agent training with Ray/RLlib:<ul> <li>Experimenting with Ray Tune</li> <li>Single node/Single core.</li> <li>Single node/Multiple cores. </li> <li>Multiple nodes.</li> </ul> </li> <li>Run experiments using GPUs for policy learning (helpful for large-scale observation and/or action spaces)</li> </ul>"},{"location":"Documentation/MachineLearning/ReinforcementLearning/#run-openai-gym-on-a-single-nodesingle-core","title":"Run OpenAI Gym on a single node/single core","text":"<p>Login on your Eagle account, create a new Anaconda environment as described in the tutorial repo, and test your installation by running a small example using one of the standard Gym environments (e.g. <code>CartPole-v0</code>).</p> <p>Activate the Anaconda enironment and start a Python session <pre><code>module purge\nconda activate /scratch/$USER/conda-envs/myenv\npython\n</code></pre> Then, run the following: <pre><code>import gym\n\nenv = gym.ens.make(\"CartPole-v0\")\nenv.reset()\n\ndone = False\n\nwhile not done:\n    action = env.action_space.sample()\n    obs, rew, done, _ = env.step(action)\n    print(action, obs, rew, done)\n</code></pre> If everything works correctly, you will see an output similar to: <pre><code>0 [-0.04506794 -0.22440939 -0.00831435  0.26149667] 1.0 False\n1 [-0.04955613 -0.02916975 -0.00308441 -0.03379707] 1.0 False\n0 [-0.05013952 -0.22424733 -0.00376036  0.2579111 ] 1.0 False\n0 [-0.05462447 -0.4193154   0.00139787  0.54940559] 1.0 False\n0 [-0.06301078 -0.61445696  0.01238598  0.84252861] 1.0 False\n1 [-0.07529992 -0.41950623  0.02923655  0.55376634] 1.0 False\n0 [-0.08369004 -0.61502627  0.04031188  0.85551538] 1.0 False\n0 [-0.09599057 -0.8106737   0.05742218  1.16059658] 1.0 False\n0 [-0.11220404 -1.00649474  0.08063412  1.47071687] 1.0 False\n1 [-0.13233393 -0.81244634  0.11004845  1.20427076] 1.0 False\n1 [-0.14858286 -0.61890536  0.13413387  0.94800442] 1.0 False\n0 [-0.16096097 -0.8155534   0.15309396  1.27964413] 1.0 False\n1 [-0.17727204 -0.62267747  0.17868684  1.03854806] 1.0 False\n0 [-0.18972559 -0.81966549  0.1994578   1.38158021] 1.0 False\n0 [-0.2061189  -1.0166379   0.22708941  1.72943365] 1.0 True\n</code></pre> Note that the above process does not involve any training.</p>"},{"location":"Documentation/MachineLearning/ReinforcementLearning/#agent-training-with-rayrllib","title":"Agent training with Ray/RLlib","text":"<p>RL algorithms are notorious for the amount of data they need to collect in order to learn policies. The more data collected, the better the training will (usually) be. The best way to do it is to run many Gym instances in parallel and collecting experience, and this is where RLlib assists.</p> <p>RLlib is an open-source library for reinforcement learning that offers both high scalability and a unified API for a variety of applications. It supports all known deep learning frameworks such as Tensorflow, Pytorch, although most parts are framework-agnostic and can be used by either one.</p> <p>The RL policy learning examples provided in this tutorial demonstrate the RLlib abilities. For convenience, the <code>CartPole-v0</code> OpenAI Gym environment will be used.</p> <p>The most straightforward way is to create a Python \"trainer\" script. It will call the necessary packages, setup flags, and run the experiments, all nicely put in a few lines of Python code.</p>"},{"location":"Documentation/MachineLearning/ReinforcementLearning/#import-packages","title":"Import packages","text":"<p>Begin trainer by importing the <code>ray</code> package: <pre><code>import ray\nfrom ray import tune\n</code></pre> <code>Ray</code> consists of an API readily available for building distributed applications. On top of it, there are several problem-solving libraries, one of which is RLlib.</p> <p><code>Tune</code> is also one of <code>Ray</code>'s libraries for scalable hyperparameter tuning. All RLlib trainers (scripts for RL agent training) are compatible with Tune API, making experimenting easy and streamlined.</p> <p>Import also the <code>argparse</code> package and setup some flags. Although that step is not mandatory, these flags will allow controlling of certain hyperparameters, such as:</p> <ul> <li>RL algorithm utilized (e.g. PPO, DQN)</li> <li>Number of CPUs/GPUs</li> <li>...and others</li> </ul> <pre><code>import argparse\n</code></pre>"},{"location":"Documentation/MachineLearning/ReinforcementLearning/#create-flags","title":"Create flags","text":"<p>Begin by defining the following flags: <pre><code>parser.add_argument(\"--num-cpus\", type=int, default=0)\nparser.add_argument(\"--num-gpus\", type=int, default=0)\nparser.add_argument(\"--name-env\", type=str, default=\"CartPole-v0\")\nparser.add_argument(\"--run\", type=str, default=\"DQN\")\nparser.add_argument(\"--local-mode\", action=\"store_true\")\n</code></pre> All of them are self-explanatory, however let's see each one separately.</p> <ol> <li><code>--num-cpus</code>: Defines the number of CPU cores used for experience collection (Default value 0 means allocation of a single CPU core).</li> <li><code>--num-gpus</code>: Allocates a GPU node for policy learning (works only for Tensorflow-GPU). Except whole values (1,2,etc.), it also accepts partial values, in case 100% of the GPU is not necessary.</li> <li><code>--name-env</code>: The name of the OpenAI Gym environment.</li> <li><code>--run</code>: Specifies the RL algorithm for agent training.</li> <li><code>--local-mode</code>: Helps defining whether experiments running on a single core or multiple cores.</li> </ol>"},{"location":"Documentation/MachineLearning/ReinforcementLearning/#initialize-ray","title":"Initialize Ray","text":"<p>Ray is able to run either on a local mode (e.g. laptop, personal computer), or on a cluster.</p> <p>For the first experiment, only a single core is needed, therefore, setup ray to run on a local mode. Then, set the number of CPU cores to be used.</p>"},{"location":"Documentation/MachineLearning/ReinforcementLearning/#run-experiments-with-tune","title":"Run experiments with Tune","text":"<p>This is the final step in this basic trainer. Tune's <code>tune.run</code> function initiates the agent training process. There are three main arguments in this function:</p> <ul> <li>RL algorithm (string): It is defined in the <code>--run</code> flag (PPO, DQN, etc.).</li> <li><code>stop</code> (dictionary): Provides a criterion to stop training (in this example is the number of training iterations; stop training when iterations reach 10,000).</li> <li><code>config</code> (dictionary): Basic information for training, contains the OpenAI Gym environment name, number of CPUs/GPUs, and others.</li> </ul> <p><pre><code>tune.run(\n    args.run,\n    name=args.name_env,\n    stop={\"training_iteration\": 10000},\n    config={\n        \"env\": args.name_env,\n        \"num_workers\": args.num_cpus, \n        \"num_gpus\": args.num_gpus,\n        \"ignore_worker_failures\": True\n        }\n    )\n</code></pre> The RLlib trainer is ready!</p> <p>Except the aforementioned default hyperparameters, every RL algorithm provided by RLlib has its own hyperparameters and their default values that can be tuned in advance.</p> <p>The code of the trainer in this example can be found in the tutorial repo.</p>"},{"location":"Documentation/MachineLearning/ReinforcementLearning/#run-experiments-on-eagle","title":"Run experiments on Eagle","text":"<p>Follow the steps in the tutorial repo carefully.</p>"},{"location":"Documentation/MachineLearning/ReinforcementLearning/#run-multi-core-experiments","title":"Run multi-core experiments","text":"<p>The previous example is designed to run on a single CPU core. However, as explained above, RL training is highly benefited from running multiple concurrent OpenAI Gym rollouts. A single node on Eagle has 36 CPU cores, therefore use any number of those in order to speed up your agent training. </p> <p>For all 36 cores, adjust the <code>--num-cpus</code> hyperparameter to reflect to all CPUs on the node: <pre><code>python simple_trainer.py --num-cpus 35\n</code></pre> Again, RLlib by default utilizes a single CPU core, therefore by putting <code>--num-cpus</code> equal to 35 means that all 36 cores are requested.</p> <p>Such is not the case with the <code>num_gpus</code> key, where zero means no GPU allocation is permitted. This is because GPUs are used for policy training and not running the OpenAI Gym environment instances, thus they are not mandatory (although having a GPU node can assist the agent training by reducing training time).</p>"},{"location":"Documentation/MachineLearning/ReinforcementLearning/#run-experiments-on-multiple-nodes","title":"Run experiments on multiple nodes","text":"<p>Let's focus now on cases where the problem under consideration is highly complex and requires vast amounts of training data for training the policy network in a reasonable amount of time. It could be then, that you will require more than one nodes to run your experiments. In this case, it is better to use a slurm script file that will include all the necessary commands for agent train using multiple CPUs and multiple nodes.</p>"},{"location":"Documentation/MachineLearning/ReinforcementLearning/#example-cartpole-v0","title":"Example: CartPole-v0","text":"<p>As explained above, CartPole is a rather simple environment and solving it using multiple cores on a single node feels like an overkill, let alone multiple nodes! However, it is a good example for giving you an experience on running RL experiments using RLlib.</p> <p>For multiple nodes it is more convenient to use a slurm script instead of an interactive node. Slurm files are submitted as <code>sbatch &lt;name_of_your_batch_script&gt;</code>, and the results are exported in an <code>slurm-&lt;job_id&gt;.out</code> file. The <code>.out</code> file can be interactively accessed during training using the <code>tail -f slurm-&lt;job_id&gt;.out</code> command. Otherwise, after training, open it using a standard text editor (e.g. <code>nano</code>). Next, the basic parts of the slurm script file are given. The repo also provides the complete script.</p> <p>The slurm file begins with defining some basic <code>SBATCH</code> options, including the desired training time, number of nodes, tasks per node, etc.</p> <p><pre><code>#!/bin/bash --login\n\n#SBATCH --job-name=cartpole-multiple-nodes\n#SBATCH --time=00:10:00\n#SBATCH --nodes=3\n#SBATCH --tasks-per-node=1\n#SBATCH --cpus-per-task=36\n#SBATCH --account=A&lt;account&gt;\nenv\n</code></pre> Allocating multiple nodes means creating a Ray cluster. A Ray cluster consists of a head node and a set of worker nodes. The head node needs to be started first, and the worker nodes are given the address of the head node to form the cluster. </p> <p>The agent training will run for 20 minutes (<code>SBATCH --time=00:20:00</code>), and on three Eagle CPU nodes (<code>SBATCH --nodes=3</code>). Every node will execute a single task (<code>SBATCH --tasks-per-node=1</code>), which will be executed on all 36 cores (<code>SBATCH --cpus-per-task=36</code>). Then, define the project account. Other options are also available, such as whether to prioritize the experiment (<code>--qos=high</code>).</p> <p>Use the commands to activate the Anaconda environment. Do not forget to <code>unset LD_PRELOAD</code>. <pre><code>module purge\nconda activate /scratch/$USER/conda-envs/env_example\nunset LD_PRELOAD\n</code></pre> Set up the Redis server that will allow all the nodes you requested to communicate with each other. For that, set a Redis password: <pre><code>ip_prefix=$(srun --nodes=1 --ntasks=1 -w $node1 hostname --ip-address)\nport=6379\nip_head=$ip_prefix:$port\nredis_password=$(uuidgen)\n</code></pre> Submit the jobs one at a time at the workers, starting with the head node and moving on to the rest of them. <pre><code>srun --nodes=1 --ntasks=1 -w $node1 ray start --block --head \\\n--node-ip-address=\"$ip_prefix\" --port=$port --redis-password=$redis_password &amp;\nsleep 10\n\necho \"starting workers\"\nfor ((  i=1; i&lt;=$worker_num; i++ ))\ndo\n  node2=${nodes_array[$i]}\n  echo \"i=${i}, node2=${node2}\"\n  srun --nodes=1 --ntasks=1 -w $node2 ray start --block --address \"$ip_head\" --redis-password=$redis_password &amp;\n  sleep 5\ndone\n</code></pre> Set the Python script to run. Since this experiment will run on a cluster, Ray will be initialized as: <pre><code>ray.init(_redis_password=args.redis_password, address=os.environ[\"ip_head\"])\nnum_cpus = args.num_cpus - 1\n</code></pre> The <code>--redis-password</code> option must be active, along with the total number of CPUs: <pre><code>python -u simple_trainer.py --redis-password $redis_password --num-cpus $total_cpus\n</code></pre> The experiment is ready to begin, simply run: <pre><code>sbatch &lt;your_slurm_file&gt;\n</code></pre> If the trainer script is on a different directory, make sure to <code>cd</code> to this directory in the slurm script before executing it. <pre><code>### Example where the trainer is on scratch:\ncd /scratch/$USER/path_to_specific_directory\npython -u simple_trainer.py --redis-password $redis_password --num-cpus $total_cpus\n</code></pre></p>"},{"location":"Documentation/MachineLearning/ReinforcementLearning/#experimenting-using-gpus","title":"Experimenting using GPUs","text":"<p>It is now time to learn running experiments using GPU nodes on Eagle that can boost training times considerably. GPU nodes however is better to be utilized only in cases of environments with very large observation and/or action spaces. CartPole will be used again for establishing a template.</p>"},{"location":"Documentation/MachineLearning/ReinforcementLearning/#allocate-gpu-node","title":"Allocate GPU node","text":"<p>The following instructions are the same for both regular and Optimized TF versions of the Anaconda environments</p> <p>Running experiments with combined CPU and GPU nodes is not so straightforward as running them using only CPU nodes (either single or multiple nodes). Particularly, heterogenous jobs using slurm have to be submitted.</p> <p>Begin at first by specifying some basic options, similarly to previous section: <pre><code>#!/bin/bash  --login\n\n#SBATCH --account=A&lt;account&gt;\n#SBATCH --job-name=cartpole-gpus\n#SBATCH --time=00:10:00\n</code></pre> The slurm script will clearly define the various jobs. These jobs include the CPU nodes that will carry the environment rollouts, and the GPU node for policy learning. Eagle has 44 GPU nodes and each node has 2 GPUs. Either request one GPU per node (<code>--gres=gpu:1</code>), or both of them (<code>--gres=gpu:2</code>). For the purposes of this tutorial, one GPU core on a single node is utilized.</p> <p>In total, slurm nodes can be categorized as: </p> <ul> <li>A head node, and multiple rollout nodes (as before)</li> <li>A policy training node (GPU)</li> </ul> <p>Include the <code>hetjob</code> header for both the rollout nodes and the policy training node. Three CPU nodes are requested to be used for rollouts and a single GPU node is requested for policy learning: <pre><code># Ray head node\n#SBATCH --nodes=1\n#SBATCH --tasks-per-node=1\n\n# Rollout nodes - Nodes with multiple runs of OpenAI Gym \n#SBATCH hetjob\n#SBATCH --nodes=3\n#SBATCH --tasks-per-node=1\n#SBATCH --cpus-per-task=36\n\n# Policy training node - This is the GPU node\n#SBATCH hetjob\n#SBATCH --nodes=1\n#SBATCH --tasks-per-node=1\n#SBATCH --partition=debug\n#SBATCH --gres=gpu:1\n</code></pre> Of course, any number of CPU/GPU nodes can be requested, depending on problem complexity. </p> <p>As an example, a single node and perhaps just a single CPU core may be requested. Now, it is more reasonable to request GPUs for an OpenAI Gym environment that utilizes high-dimensional observation and/or action spaces. Hence, the first priority would be to start with multiple CPU nodes, and request GPUs only if they are needed.</p> <p>For the three types of nodes (head, rollouts, training), define three separate groups: <pre><code>head_node=$(scontrol show hostnames $SLURM_JOB_NODELIST_HET_GROUP_0)\nrollout_nodes=$(scontrol show hostnames $SLURM_JOB_NODELIST_HET_GROUP_1)\nrollout_nodes_array=( $rollout_nodes )\nlearner_node=$(scontrol show hostnames $SLURM_JOB_NODELIST_HET_GROUP_2)\necho \"head node    : \"$head_node\necho \"rollout nodes: \"$rollout_nodes\necho \"learner node : \"$learner_node\n</code></pre> Each group of nodes requires its separate <code>srun</code> command so that they will run independently of each other. <pre><code>echo \"starting head node at $head_node\"\nsrun --pack-group=0 --nodes=1 --ntasks=1 -w $head_node ray start --block --head \\\n--node-ip-address=\"$ip_prefix\" --port=$port --redis-password=$redis_password &amp; # Starting the head\nsleep 10\n\necho \"starting rollout workers\"\nfor ((  i=0; i&lt;$rollout_node_num; i++ ))\ndo\n  rollout_node=${rollout_nodes_array[$i]}\n  echo \"i=${i}, rollout_node=${rollout_node}\"\n  srun --pack-group=1 --nodes=1 --ntasks=1 -w $rollout_node \\\n   ray start --block --address \"$ip_head\" --redis-password=$redis_password &amp; # Starting the workers\n  sleep 5\ndone\n\necho \"starting learning on GPU\"\nsrun --pack-group=2 --nodes=1 --gres=gpu:1 -w $learner_node ray start --block --address \"$ip_head\" --redis-password=$redis_password &amp;\n</code></pre> The slurm commands for the head and rollout nodes are identical to those from the previous section. A third command is also added for engaging the GPU node.</p> <p>Finally, call <pre><code>python -u simple_trainer.py --redis-password $redis_password --num-cpus $rollout_num_cpus --num-gpus 1\n</code></pre> to begin training. Add the <code>---num-gpus</code> argument to include the requested GPU node (or nodes in case of <code>--gres=gpu:2</code>) for policy training. There is no need to manually declare the GPU for policy training in the <code>simple_trainer.py</code>, RLlib will automatically recognize the available GPU and use it accordingly.</p> <p>The repo contains the complete slurm file versions for both <code>env_example_gpu</code> and <code>env_gpu_optimized_tf</code>, and they can be used as templates for future projects.</p>"},{"location":"Documentation/MachineLearning/ReinforcementLearning/#create-gym-environments-from-scratch","title":"Create Gym environments from scratch","text":"<p>So far, only benchmark Gym environments were used in order to demonstrate the processes for running experiments. It is time now to see how one can create their own Gym environment, carefully tailor-made to one's needs. OpenAI Gym functionality allows the creation of custom-made environments using the same structure as the benchmark ones. </p> <p>Custom-made environments can become extremely complex due to the mechanics involved and may require many subscripts that perform parts of the simulation. Nevertheless, the basis of all environments is simply a Python class that inherits the <code>gym.Env</code> class, where the user can implement the three main Gym functions and define any hyperpameters necessary:</p> <ul> <li><code>def __init__(self)</code>: Initializes the environment. It defines initial values for variables/hyperparameters and may contain other necessary information. It also defines the dimensionality of the problem. Dimensionality is expressed at the sizes of the observation and action spaces, which are given using the parameters <code>self.observation_space</code> and <code>self.action_space</code>, respectively. Depending on their nature, they can take discrete, continuous, or a combination of values. OpenAI provides detailed examples of each one of these types of spaces.</li> <li><code>def reset(self)</code>: When called, it resets the environment on a previous state (hence the name). This state can either be a user-defined initial state or it may be a random initial position. The latter can be found on environments that describe locomotion like <code>CartPole</code>, where the initial state can be any possible position of the pole on the cart.</li> <li><code>def step(self, action)</code>: The heart of the class. It defines the inner mechanics of the environment, hence it can be seen as some kind of simulator. Its main input is the sampled action, which when acted upon moves the environment into a new state and calculates the new reward. The new state and reward are two of the function's output and they are necessary for policy training since they are also inputs to the policy network. Other outputs include a boolean variable <code>done</code> that is True when the environment reaches its final state (if it exists), and False otherwise*, as well as a dictionary (<code>info</code>) with user-defined key-value objects that contain further information from the inner workings of the environment.</li> </ul> <p>* Many environments do not consider a final state, since it might not make sense (e.g. a traffic simulator for fleets of autonomous ridesharing vehicles that reposition themselves based on a certain criterion. In this case the reward will get better every time, but there is no notion of a final vehicle position).</p> <p>Directions of how to create and register a custom-made OpenAI Gym environment are given below.</p>"},{"location":"Documentation/MachineLearning/ReinforcementLearning/#create-an-environment-class","title":"Create an environment class","text":"<p>As stated above, the basis of any Gym environment is a Python class that inherits the <code>gym.Env</code> class. After importing the gym package, define the class as: <pre><code>import gym\n\nclass BasicEnv(gym.Env):(...)\n</code></pre> The example environment is very simple and is represented by two possible states (0, 1) and 5 possible actions (0-4). For the purposes of this tutorial, consider state 0 as the initial state, and state 1 as the final state.</p> <p>Define the dimensions of observation and action spaces in the <code>def __init__(self)</code> function: <pre><code>def __init__(self):\n    self.action_space = gym.spaces.Discrete(5) # --&gt; Actions take values in the 0-4 interval\n    self.observation_space = gym.spaces.Discrete(2) # --&gt; Two possible states [0,1]\n</code></pre> Both spaces take discrete values, therefore they are defined using Gym's <code>Discrete</code> function. Other possible functions are <code>Box</code> for continuous single- or multi-dimensional observations and states, <code>MultiDiscrete</code> for vectors of discrete values, etc. OpenAi provides detailed explanation for all different space forms.</p> <p>Next, define the <code>def reset(self)</code> function: <pre><code>def reset(self):\n    state = 0\n    return state\n</code></pre> In this example, the reset function simply returns the environment to the initial state.</p> <p>Finally, define the <code>def step(self, action)</code> function, which takes as input the sampled action. Here the step function takes the environment at state 1 and based on the action, returns a reward of 1 or -1: <pre><code>def step(self, action):\n    state = 1\n\n    if action == 2:\n        reward = 1\n    else:\n        reward = -1\n\n    done = True\n    info = {}\n\n    return state, reward, done, info\n</code></pre> That's it, the new Gym environment is ready! Make note that there is one more function usually found on Gym environments. This is the <code>def render(self)</code> function, and is called in random intervals throughout training returning a \"snapshot\" of the environment at that time. While this is helpful for evaluating the agent training process, it is not necessary for the actual training process. OpenAI documentation provides details for every one of these functions.</p> <p>You can find the full script of this environment in the repo.</p>"},{"location":"Documentation/MachineLearning/ReinforcementLearning/#run-experiments-on-rllib","title":"Run experiments on RLlib","text":"<p>Let's now train the agent with RLlib. The full trainer script is given at the repo.</p> <p>The trainer is almost identical to the one used before, with few additions that are necessary to register the new environment.</p> <p>At first, along with <code>ray</code> and <code>tune</code>, import: <pre><code>from ray.tune.registry import register_env\nfrom custom_env import BasicEnv\n</code></pre> The <code>register_env</code> function is used to register the new environment, which is imported from the <code>custom_env.py</code>.</p> <p>Function <code>register_env</code> takes two arguments:</p> <ul> <li>Training name of the environment, chosen by the developer.</li> <li>Actual name of the environment (<code>BasicEnv</code>) in a <code>lambda config:</code> function.</li> </ul> <p><pre><code>env_name = \"custom-env\"\nregister_env(env_name, lambda config: BasicEnv())\n</code></pre> Once again, RLlib provides detailed explanation of how <code>register_env</code> works.</p> <p>The <code>tune.run</code> function, instead of <code>args.name_env</code>, it uses the <code>env_name</code> defined above.</p> <p>That's all! Proceed with agent training using any of the slurm scripts provided by the repo.</p> <p>As a final note, creating custom-made OpenAI Gym environment is more like an art than science. The main issue is to really clarify what the environment represents and how it works, and then define this functionality in Python.</p>"},{"location":"Documentation/MachineLearning/ReinforcementLearning/#validating-results-using-tensorboard","title":"Validating results using Tensorboard","text":"<p>Another way of visualizing the performance of agent training is with Tensorboard. </p> <p>Navigate to the <code>ray_results</code> directory: <pre><code>cd ~/ray_results/\n</code></pre> Every RL experiment generates a subdirectory named from the OpenAI Gym environment used in the experiment. </p> <p>E.g., after running all the examples previously shown in this tutorial, <code>ray_results</code> will have a subdirectory named <code>CartPole-v0</code>. Within, every experiment using CartPole generates a new subdirectory.</p> <p>For the purpose of this tutorial, <code>cd</code> to the <code>CartPole-v0</code> subdirectory and activate one of the environments: <pre><code>module purge\nconda activate &lt;your_environment&gt;\n</code></pre> Initialize Tensorboard following the steps in this tutorial. Open the localhost url in a browser, and all plots for rewards, iterations and other metrics will be demonstrated as:</p> <p> </p> <p>The <code>tune/episode_reward_mean</code> plot is essentialy the same as the figure plotted from data in the <code>progress.csv</code> file. The difference in the x-axis scale has a simple explanation. The <code>episode_reward_mean</code> column on the <code>progress.csv</code> file shows the reward progress on every training iteration, while the <code>tune/episode_reward_mean</code> plot on Tensorboard shows reward progress on every training episode (a single RLlib training iteration consists of thousands of episodes).</p>"},{"location":"Documentation/MachineLearning/TensorBoard/","title":"Validating ML results using Tensorboard","text":"<p>Tensorboard provides visualization and tooling needed for machine learning, deep learning, and reinforcement learning experimentation:</p> <ul> <li>Tracking and visualizing metrics such as loss and accuracy.</li> <li>Visualizing the model graph (ops and layers).</li> <li>Viewing histograms of weights, biases, or other tensors as they change over time.</li> <li>Projecting embeddings to a lower dimensional space.</li> <li>Displaying images, text, and audio data.</li> <li>Profiling TensorFlow programs.</li> </ul> <p>For RL it is useful to visualize metrics such as:</p> <ul> <li>Mean, min, and max reward values.</li> <li>Episodes/iteration.</li> <li>Estimated Q-values.</li> <li>Algorithm-specific metrics (e.g. entropy for PPO).</li> </ul> <p>To visualize results from Tensorboard, first <code>cd</code> to the directory where your results reside. E.g., if you ran experiments using <code>ray</code>, then do the following: <pre><code>cd ~/ray_results/\n</code></pre></p> <p>There are three main methods for activating Tensorboard:</p> <ul> <li>If you included Tensorboard installation in an Anaconda environment, simply activate it:    <pre><code>module purge\nconda activate &lt;your_environment&gt;\n</code></pre></li> <li>You can also install Tensorboard in userspace using <code>pip install</code>:    <pre><code>pip install tensorboard --user\n</code></pre></li> <li>Or, install using container images:    <pre><code>ml singularity-container\nsingularity pull docker://tensorflow/tensorflow\nsingularity run tensorflow_latest.sif\n</code></pre></li> </ul> <p>Then, initialize Tensorboard using a pre-specified port number of your choosing (e.g. 6006, 8008): <pre><code>tensorboard --logdir=. --port 6006 --bind_all\n</code></pre> If everything works properly, terminal will show: <pre><code>Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\nTensorBoard 2.5.0 at http://localhost:6006/ (Press CTRL+C to quit)\n</code></pre> Open a new Terminal tab and create a tunnel: <pre><code>ssh -NfL 6006:localhost:6006 $USER@el1.hpc.nrel.gov\n</code></pre> Finally, open the above localhost url (<code>http://localhost:6006/</code>) in a browser, where all the aforementioned plots will be shown.</p>"},{"location":"Documentation/Software_Tools/conda/","title":"Conda","text":""},{"location":"Documentation/Software_Tools/conda/#why-conda","title":"Why Conda?","text":"<p>Conda is a package manager which allows you to easily create and switch betwen different software environments in different languages for different purposes.  With Conda, it's easy to:</p> <ul> <li> <p>Manage different (potentially conflicting) versions of the same software without complication</p> </li> <li> <p>Quickly stand up even complicated dependencies for stacks of software</p> </li> <li> <p>Share your specific programming environment with others for reproducible results </p> </li> </ul>"},{"location":"Documentation/Software_Tools/conda/#creating-environments-by-name","title":"Creating Environments by Name","text":"<p>To create a basic Conda environment, we'll start by running</p> <p><code>conda create --name mypy python</code></p> <p>where the <code>--name</code> option (or the shortened <code>-n</code>) means the environment will be specified by name and <code>myenv</code> will be the name of the created environment.  Any arguments following the environment name are the packages to be installed.</p> <p>To specify a specific version of a package, simply add the version number after the \"=\" sign</p> <p><code>conda create --name mypy37 python=3.7</code></p> <p>You can specify multiple packages for installation during environment creation</p> <p><code>conda create --name mynumpy python=3.7 numpy</code></p> <p>Conda ensures dependencies are satisfied when installing packages, so the version of the numpy package installed will be consistent with Python 3.7 (and any other packages specified).</p> Tip: It\u2019s recommended to install all the packages you want to include in an environment at the same time to help avoid dependency conflicts."},{"location":"Documentation/Software_Tools/conda/#environment-navigation","title":"Environment Navigation","text":"<p>To see a list of all existing environments (useful to confirm the successful creation of a new environment):</p> <p><code>conda env list</code></p> <p>To activate your new environment:</p> <p><code>conda activate mypy</code></p> <p>Your usual command prompt should now be prefixed with <code>(mypy)</code>, which helps keep track of which environment is currently activated.</p> <p>To see which packages are installed from within a currently active environment:</p> <p><code>conda list</code></p> <p>When finished with this programming session, deactivate your environment with:</p> <p><code>conda deactivate</code></p>"},{"location":"Documentation/Software_Tools/conda/#creating-environments-by-location","title":"Creating Environments by Location","text":"<p>Creating environments by location is especially helpful when working on the Eagle HPC, as the default location is your <code>/home/&lt;username&gt;/</code> directory, which is limited to 50 GB.  To create a Conda environment somewhere besides the default location, use the <code>--prefix</code> flag (or the shortened <code>-p</code>) instead of <code>--name</code> when creating.</p> <p><code>conda create --prefix /path/to/mypy python=3.7 numpy</code></p> <p>This re-creates the python+numpy environment from earlier, but with all downloaded packages stored in the specified location.</p> Warning:  Keep in mind that scratch on Eagle is temporary in that files are purged after 28 days of inactivity.  <p>Unfortunately, placing environments outside of the default /env folder means that it needs to be activated with the full path (<code>conda activate /path/to/mypy</code>) and will show the full path rather than the environment name at the command prompt. </p> <p>To fix the cumbersome command prompt, simply modify the <code>env_prompt</code> setting in your <code>.condarc</code> file:</p> <p><code>conda config --set env_prompt '({name}) '</code></p> <p>Note that <code>'({name})'</code> is not a placeholder for your desired environment name but text to be copied literally.  This will edit your <code>.condarc</code> file if you already have one or create a <code>.condarc</code> file if you do not. For more on modifying your <code>.condarc</code> file, check out the User Guide.  Once you've completed this step, the command prompt will show the shortened name (mypy, in the previous example).</p>"},{"location":"Documentation/Software_Tools/conda/#managing-conda-environments","title":"Managing Conda Environments","text":"<p>Over time, it may become necessary to add additional packages to your environments.  New packages can be installed in the currently active environment with:</p> <p><code>conda install pandas</code></p> <p>Conda will ensure that all dependencies are satisfied which may include upgrades to existing packages in this repository.  To install packages from other sources, specify the <code>channel</code> option:</p> <p><code>conda install --channel conda-forge fenics</code></p> <p>To add a pip-installable package to your environment:</p> <p><code>conda install pip pip &lt;pip_subcommand&gt;</code></p> A note on mixing Conda and Pip:  Issues may arise when using pip and conda together. When combining conda and pip, it is best to use an isolated conda environment. Only after conda has been used to install as many packages as possible should pip be used to install any remaining software. If modifications are needed to the environment, it is best to create a new environment rather than running conda after pip. When appropriate, conda and pip requirements should be stored in text files.  <p>For more information on this point, check out the User Guide</p> <p>We can use <code>conda list</code> to see which packages are currently installed, but for a more version-control-flavored approach:</p> <p><code>conda list --revisions</code></p> <p>which shows changes to the environment over time.  To revert back to a previous environemnt</p> <p><code>conda install --revision 1</code></p> <p>To remove packages from the currently activated environment:</p> <p><code>conda remove pkg1</code></p> <p>To completely remove an environment and all installed packages:</p> <p><code>conda remove --name mypy --all</code></p> <p>Conda environments can become large quickly due to the liberal creation of cached files.  To remove these files and free up space you can use</p> <p><code>conda clean --all</code></p> <p>or to simply preview the potential changes before doing any actual deletion</p> <p><code>conda clean --all --dry-run</code></p>"},{"location":"Documentation/Software_Tools/conda/#sharing-conda-environments","title":"Sharing Conda Environments","text":"<p>To create a file with the the exact \"recipe\" used to create the current environment:</p> <p><code>conda env export &gt; environment.yaml</code></p> <p>In practice, this recipe may be overly-specific to the point of creating problems on different hardware.  To save an abbreviated version of the recipe with only the packages you explicitly requested: </p> <p><code>conda env export --from-history &gt; environment.yaml</code></p> <p>To create a new environment with the recipe specified in the .yaml file:</p> <p><code>conda env create --name mypyeagle --file environment.yaml</code></p> <p>If a name or prefix isn't specified, the environment will be given the same name as the original environment the recipe was exported from (which may be desirable if you're moving to a different computer).</p>"},{"location":"Documentation/Software_Tools/conda/#speed-up-dependency-solving","title":"Speed up dependency solving","text":"<p>To speed up dependency solving, substitute the mamba command for conda.  Mamba is a dependency solver written in C++ designed to speed up the conda environment solve.</p> <p><code>mamba create --prefix /path/to/mypy python=3.7 numpy</code></p>"},{"location":"Documentation/Software_Tools/conda/#reduce-home-directory-usage","title":"Reduce home directory usage","text":"<p>By default, the conda module uses the home directory for package caches and named environments.  This results in a lot of the home directory quota used. Some ways to reduce home directory usage include:</p> <ul> <li> <p>Use the -p PATH_NAME switch when creating or updating your environment.  Make sure PATH_NAME isn't in the home directory.  Keep in mind files in /scratch are deleted after about a month of inactivity.</p> </li> <li> <p>Change the directory used for caching.  This location is set by the module file to ~/.conda-pkgs.  Calling export CONDA_PKGS_DIRS=PATH_NAME to somewhere to store downloads and cached files such as /scratch/$USER/.conda-pkgs will reduce home directory usage.  </p> </li> </ul>"},{"location":"Documentation/Software_Tools/conda/#eagle-considerations","title":"Eagle Considerations","text":"<p>Interacting with your Conda environments on Eagle should feel exactly the same as working on your desktop.  An example desktop-to-HPC workflow might go:</p> <ol> <li>Create the environment locally</li> <li>Verify that environment works on a minimal working example</li> <li>Export local environment file and copy to Eagle</li> <li>Duplicate local environment on Eagle</li> <li>Execute production-level runs on Eagle</li> </ol> <pre><code>#!/bin/bash \n#SBATCH --ntasks=4\n#SBATCH --nodes=1\n#SBATCH --time=5\n#SBATCH --account=&lt;project_handle&gt;\n\nmodule purge\nmodule load conda\nconda activate mypy\n\nsrun -n 8 python my_main.py\n</code></pre>"},{"location":"Documentation/Software_Tools/conda/#cheat-sheet-of-common-commands","title":"Cheat Sheet of Common Commands","text":"Task ... outside environment ... inside environment Create by name <code>conda create -n mypy pkg1 pkg2</code> N/A Create by path <code>conda create -p path/to/mypy pkg1 pkg2</code> N/A Create by file <code>conda env create -f environment.yml</code> N/A Show environments <code>conda env list</code> N/A Activate <code>conda activate mypy</code> N/A Deactivate N/A <code>conda deactivate</code> Install New Package <code>conda install -n mypy pkg1 pkg2</code> <code>conda install pkg1 pkg2</code> List All Packages <code>conda list -n mypy</code> <code>conda list</code> Revision Listing <code>conda list --revisions -n mypy</code> <code>conda list --revisions</code> Export Environment <code>conda env export -n mypy &gt; environment.yaml</code> <code>conda env export &gt; environment.yaml</code> Remove Package <code>conda remove -n mypy pkg1 pkg2</code> <code>conda remove pkg1 pkg2</code>"},{"location":"Documentation/Software_Tools/spack/","title":"Spack","text":""},{"location":"Documentation/Software_Tools/spack/#introduction","title":"Introduction","text":"<p>Spack is an HPC-centric package manager for acquiring, building, and managing HPC applications as well as all their dependencies, down to the compilers themselves. Like frameworks such as Anaconda, it is associated with a repository of both source-code and binary packages. Builds are fully configurable through a DSL at the command line as well as in YAML files. Maintaining many build-time permutations of packages is simple through an automatic and user-transparent hashing mechanism. The Spack system also automatically creates (customizable) environment modulefiles for each built package.</p>"},{"location":"Documentation/Software_Tools/spack/#installation","title":"Installation","text":"<p>Multiple installations of Spack can easily be kept, and each is separate from the others by virtue of the environment variable <code>SPACK_ROOT</code>.  All package, build, and modulefile content is kept inside the <code>SPACK_ROOT</code> path, so working with different package collections is as simple as setting <code>SPACK_ROOT</code> to the appropriate location.  The only exception to this orthogonality are <code>YAML</code> files in <code>$HOME/.spack/&lt;platform&gt;</code>. Installing a Spack instance is as easy as</p> <p><code>git clone https://github.com/spack/spack.git</code></p> <p>Once the initial Spack instance is set up, it is easy to create new ones from it through</p> <p><code>spack clone &lt;new_path&gt;</code></p> <p><code>SPACK_ROOT</code> will need to point to <code>&lt;new_path&gt;</code> in order to be consistent.</p> <p>Spack environment setup can be done by sourcing <code>$SPACK_ROOT/share/spack/setup-env.sh</code>, or by simply adding <code>$SPACK_ROOT/bin</code> to your PATH. </p> <p><code>source $SPACK_ROOT/share/spack/setup-env.sh</code> or  <code>export PATH=$SPACK_ROOT/bin:$PATH</code></p>"},{"location":"Documentation/Software_Tools/spack/#setting-up-compilers","title":"Setting Up Compilers","text":"<p>Spack is able to find certain compilers on its own, and will add them to your environment as it does.  In order to obtain the list of available compilers on Eagle the user can run <code>module avail</code>, the user can then load the compiler of interest using <code>module use &lt;compiler&gt;</code>. To see which compilers your Spack collections know about, type</p> <p><code>spack compilers</code></p> <p>To add an existing compiler installation to your collection, point Spack to its location through</p> <p><code>spack add compiler &lt;path to Spack-installed compiler directory with hash in name&gt;</code></p> <p>The command will add to <code>$HOME/.spack/linux/compilers.yaml</code>.  To configure more generally, move changes to one of the lower-precedence <code>compilers.yaml</code> files (paths described below in Configuration section). Spack has enough facility with standard compilers (e.g., GCC, Intel, PGI, Clang) that this should be all that\u2019s required to use the added compiler successfully.</p>"},{"location":"Documentation/Software_Tools/spack/#available-packages-in-repo","title":"Available Packages in Repo","text":"Command Description <code>spack list</code> all available packages by name. Dumps repo content, so if use local repo, this should dump local package load. <code>spack list &lt;pattern&gt;</code> all available packages that have <code>&lt;pattern&gt;</code> somewhere in their name. <code>&lt;pattern&gt;</code> is simple, not regex. <code>spack info &lt;package_name&gt;</code> available versions classified as safe, preferred, or variants, as well as dependencies. Variants are important for selecting certain build features, e.g., with/without Infiniband support. <code>spack versions &lt;package_name&gt;</code> see which versions are available"},{"location":"Documentation/Software_Tools/spack/#installed-packages","title":"Installed packages","text":"Command Description <code>spack find</code> list all locally installed packages <code>spack find --deps &lt;package&gt;</code> list dependencies of <code>&lt;package&gt;</code> <code>spack find --explicit</code> list packages that were explicitly requested via spack install <code>spack find --implicit</code> list packages that were installed as a dependency to an explicitly installed package <code>spack find --long</code> include partial hash in package listing. Useful to see distinct builds <code>spack find --paths</code> show installation paths <p>Finding how an installed package was built does not seem as straightforward as it should be.  Probably the best way is to examine <code>&lt;install_path&gt;/.spack/build.env</code>, where <code>&lt;install_path&gt;</code> is the Spack-created directory with the hash for the package being queried.  The environment variable <code>SPACK_SHORT_SPEC</code> in <code>build.env</code> contains the Spack command that can be used to recreate the package (including any implicitly defined variables, e.g., arch).  The 7-character short hash is also included, and should be excluded from any spack install command.</p> Symbols Description <code>@</code> package versions. Can use range operator \u201c:\u201d, e.g., X@1.2:1.4 . Range is inclusive and open-ended, e.g., \u201cX@1.4:\u201d matches any version of package X 1.4 or higher. <code>%</code> compiler spec. Can include versioning, e.g., X%gcc@4.8.5 <code>+,-,~</code> build options. +opt, -opt, \u201c~\u201d is equivalent to \u201c-\u201c <code>name=value</code> build options for non-Boolean flags. Special names are cflags, cxxflags, fflags, cppflags, ldflags, and ldlibs <code>target=value</code> for defined CPU architectures, e.g., target=haswell <code>os=value</code> for defined operating systems <code>^</code> dependency specification, using above specs as appropriate <code>^/&lt;hash&gt;</code> specify dependency where <code>&lt;hash&gt;</code> is of sufficient length to resolve uniquely"},{"location":"Documentation/Software_Tools/spack/#external-packages","title":"External Packages","text":"<p>Sometimes dependencies are expected to be resolved through a package that is installed as part of the host system, or otherwise outside of the Spack database.  One example is Slurm integration into MPI builds.  If you were to try to add a dependency on one of the listed Slurms in the Spack database, you might see, e.g.,</p> <pre><code>[$user@el2 ~]$ spack spec openmpi@3.1.3%gcc@7.3.0 ^slurm@19-05-3-2\nInput spec\n--------------------------------\nopenmpi@3.1.3%gcc@7.3.0\n    ^slurm@19-05-3-2\n\nConcretized\n--------------------------------\n==&gt; Error: The spec 'slurm' is configured as not buildable, and no matching external installs were found\n</code></pre> <p>Given that something like Slurm is integrated deeply into the runtime infrastructure of our local environment, we really want to point to the local installation.  The way to do that is with a <code>packages.yaml</code> file, which can reside in the standard Spack locations (see Configuration below).  See the Spack docs on external packages for more detail.  In the above example at time of writing, we would like to build OpenMPI against our installed <code>Slurm 19.05.2</code>.  So, you can create file <code>~/.spack/linux/packages.yaml</code> with the contents</p> <pre><code>packages:\nslurm:\npaths:\nslurm@18-08-0-3: /nopt/slurm/18.08.3\nslurm@19-05-0-2: /nopt/slurm/19.05.2\n</code></pre> <p>that will enable builds against both installed Slurm versions.  Then you should see</p> <pre><code>[$user@el2 ~]$ spack spec openmpi@3.1.3%gcc@7.3.0 ^slurm@19-05-0-2\nInput spec\n--------------------------------\nopenmpi@3.1.3%gcc@7.3.0\n    ^slurm@19-05-0-2\n\nConcretized\n--------------------------------\nopenmpi@3.1.3%gcc@7.3.0 cflags=\"-O2 -march=skylake-avx512 -mtune=skylake-avx512\" cxxflags=\"-O2 -march=skylake-avx512 -mtune=skylake-avx512\" fflags=\"-O2 -march=skylake-avx512 -mtune=skylake-avx512\" +cuda+cxx_exceptions fabrics=verbs ~java~legacylaunchers~memchecker+pmi schedulers=slurm ~sqlite3~thread_multiple+vt arch=linux-centos7-x86_64\n-\n    ^slurm@19-05-0-2%gcc@7.3.0 cflags=\"-O2 -march=skylake-avx512 -mtune=skylake-avx512\" cxxflags=\"-O2 -march=skylake-avx512 -mtune=skylake-avx512\" fflags=\"-O2 -march=skylake-avx512 -mtune=skylake-avx512\" ~gtk~hdf5~hwloc~mariadb+readline arch=linux-centos7-x86_64\n</code></pre> <p>where the Slurm dependency will be satisfied with the installed Slurm (cflags, cxxflags, and arch are coming from site-wide configuration in <code>/nopt/nrel/apps/base/2018-12-02/spack/etc/spack/compilers.yaml</code>; the variants string is likely coming from the configuration in the Spack database, and should be ignored).</p>"},{"location":"Documentation/Software_Tools/spack/#virtual-packages","title":"Virtual Packages","text":"<p>It is possible to specify some packages for which multiple options are available at a higher level.  For example, <code>mpi</code> is a virtual package specifier that can resolve to mpich, openmpi, Intel MPI, etc.  If a package's dependencies are spec'd in terms of a virtual package, Spack will choose a specific package at build time according to site preferences. Choices can be constrained by spec, e.g.,</p> <p><code>spack install X ^mpich@3</code></p> <p>would satisfy package X\u2019s mpi dependency with some version 3 of MPICH. You can see available providers of a virtual package with</p> <p><code>spack providers &lt;vpackage&gt;</code></p>"},{"location":"Documentation/Software_Tools/spack/#extensions","title":"Extensions","text":"<p>In many cases, frameworks have sub-package installations in standard locations within their own installations.  A familiar example of this is Python and its usual module location in <code>lib(64)/python&lt;version&gt;/site-packages</code>, and pointed to via the environment variable <code>PYTHONPATH</code>.</p> <p>To find available extensions</p> <p><code>spack extensions &lt;package&gt;</code></p> <p>Extensions are just packages, but they are not enabled for use out of the box. To do so (e.g., so that you could load the Python module after installing), you can either load the extension package\u2019s environment module, or</p> <p><code>spack use &lt;extension package&gt;</code></p> <p>This only lasts for the current session, and is not of general interest. A more persistent option is to activate the extension:</p> <p><code>spack activate &lt;extension package&gt;</code></p> <p>This takes care of dependencies as well. The inverse operation is deactivation.</p> Command Description <code>spack deactivate &lt;extension package&gt;</code> deactivates extension alone. Will not deactivate if dependents exist <code>spack deactivate --force &lt;extension package&gt;</code> deactivates regardless of dependents <code>spack deactivate --all &lt;extension package&gt;</code> deactivates extension and all dependencies <code>spack deactivate --all &lt;parent&gt;</code> deactivates all extensions of parent (e.g., <code>&lt;python&gt;</code>)"},{"location":"Documentation/Software_Tools/spack/#modules","title":"Modules","text":"<p>Spack can auto-create environment modulefiles for the packages that it builds, both in Tcl for \u201cenvironment modules\u201d per se, and in Lua for Lmod.  Auto-creation includes each dependency and option permutation, which can lead to excessive quantities of modulefiles.  Spack also uses the package hash as part of the modulefile name, which can be somewhat disconcerting to users.  These default behaviors can be treated in the active modules.yaml file, as well as practices used for support. Tcl modulefiles are created in <code>$SPACK_ROOT/share/spack/modules</code> by default, and the equivalent Lmod location is <code>$SPACK_ROOT/share/spack/lmod</code>.  Only Tcl modules are created by default.  You can modify the active modules.yaml file in the following ways to affect some example behaviors:</p>"},{"location":"Documentation/Software_Tools/spack/#to-turn-lmod-module-creation-on","title":"To turn Lmod module creation on:","text":"<pre><code>modules:\n    enable:\n        - tcl\n        - lmod \n</code></pre>"},{"location":"Documentation/Software_Tools/spack/#to-change-the-modulefile-naming-pattern","title":"To change the modulefile naming pattern:","text":"<pre><code>modules:\n    tcl:\n        naming_scheme: \u2018{name}/{version}/{compiler.name}-{compiler.version}\n</code></pre> <p>would achieve the Eagle naming scheme. </p>"},{"location":"Documentation/Software_Tools/spack/#to-remove-default-variable-settings-in-the-modulefile-eg-cpath","title":"To remove default variable settings in the modulefile, e.g., CPATH:","text":"<pre><code>modules:\n    tcl:\n        all:\n            filter:\n                environment_blacklist: [\u2018CPATH\u2019]\n</code></pre> <p>Note that this would affect Tcl modulefiles only; if Spack also creates Lmod files, those would still contain default CPATH modification behavior.</p>"},{"location":"Documentation/Software_Tools/spack/#to-prevent-certain-modulefiles-from-being-built-you-can-whitelist-and-blacklist","title":"To prevent certain modulefiles from being built, you can whitelist and blacklist:","text":"<pre><code>modules:\n    tcl:\n        whitelist: [\u2018gcc\u2019]\n        blacklist: [\u2018%gcc@4.8.5\u2019]\n</code></pre> <p>This would create modules for all versions of GCC built using the system compiler, but not for the system compiler itself. There are a great many further behaviors that can be changed, see https://spack.readthedocs.io/en/latest/module_file_support.html#modules for more.</p> <p>For general user support, it is not a bad idea to keep the modules that are publicly visible separate from the collection that Spack auto-generates. This involves some manual copying, but is generally not onerous as all rpaths are included in Spack-built binaries (i.e., you don\u2019t have to worry about satisfying library dependencies for Spack applications with an auto-built module, since library paths are hard-coded into the application binaries). This separation also frees one from accepting Spack\u2019s verbose coding formats within modulefiles, should you decide to maintain certain modulefiles another way.</p>"},{"location":"Documentation/Software_Tools/spack/#configuration","title":"Configuration","text":"<p>Spack uses hierarchical customization files.  Every package is a Python class, and inherits from the top-level class Package.  Depending on the degree of site customization, you may want to fork the Spack repo to create your own customized Spack package. There are 4 levels of configuration. In order of increasing precedence,</p> <ol> <li>Default: <code>$SPACK_ROOT/etc/spack/default</code></li> <li>System-wide: <code>/etc/spack</code></li> <li>Site-wide: <code>$SPACK_ROOT/etc/spack</code></li> <li>User-specific: <code>$HOME/.spack</code></li> </ol> <p>Spack configuration uses YAML files, a subset of JSON native to Python. There are 5 main configuration files.</p> <ol> <li> <p><code>compilers.yaml</code>. Customizations to the Spack-known compilers for all builds</p> <p>i.  Use full path to compilers</p> <p>ii. Additional rpaths beyond the Spack repo</p> <p>iii.    Additional modules necessary when invoking compilers</p> <p>iv. Mixing toolchains</p> <p>v.  Optimization flags</p> <p>vi. Environment modifications</p> </li> <li> <p><code>config.yaml</code>. Base functionality of Spack itself</p> <p>i.  install_tree: where to install packages</p> <p>ii. build_stage: where to do compiles. For performance, can specify a local SSD or a RAMFS.</p> <p>iii.    modules_roots: where to install modulefiles</p> </li> <li> <p><code>modules.yaml</code>. How to create modulefiles</p> <p>i.  whitelist/blacklist packages from having their own modulefiles created</p> <p>ii. adjust hierarchies</p> </li> <li> <p><code>packages.yaml</code>. Specific optimizations, such as multiple hardware targets.</p> <p>i.  dependencies, e.g., don\u2019t build OpenSSL (usually want sysadmins to handle updates, etc.)</p> <p>ii. mark specific packages as non-buildable, e.g., vendor MPIs</p> <p>iii.    preferences, e.g., BLAS -&gt; MKL, LAPACK -&gt; MKL</p> </li> <li> <p><code>repos.yaml</code></p> <p>i.  Directory-housed, not remote</p> <p>ii. Specify other package locations</p> <p>iii.    Can then spec build in other configs (e.g., binary, don\u2019t build)</p> <p>iv. Precedence in YAML file order, but follows Spack precedence order (user &gt; site &gt; system &gt; default)</p> </li> </ol>"},{"location":"Documentation/Software_Tools/spack/#variants-standard-adjustments-to-package-build","title":"Variants: standard adjustments to package build","text":"<p><code>spack edit \u2026</code>-- opens Python file for package, can easily write new variants</p>"},{"location":"Documentation/Software_Tools/spack/#providers","title":"Providers","text":"<p><code>spack providers</code> -- virtual packages, e.g., blas, mpi, etc. Standards, not implementations. Abstraction of an implementation (blas/mkl, mpi/mpich, etc.)</p>"},{"location":"Documentation/Software_Tools/spack/#mirrors","title":"Mirrors","text":"<ul> <li>mirrors.yaml: where packages are kept</li> <li>A repo is where build information is kept; a mirror is where code lives</li> </ul> <pre><code>MirrorTopLevel\n    package_a\n        package_a-version1.tar.gz\n        package_a-version2.tar.gz\n    package_b\n        \u22ee\n</code></pre> <p><code>spack mirror</code> to manage mirrors</p>"},{"location":"Documentation/Software_Tools/spack/#repos","title":"Repos","text":"<ul> <li>Can take precedence from, e.g., a site repo</li> <li>Can namespace</li> </ul> <pre><code>packages\n    repo.yaml\n    alpha\n        hotfix-patch-ABC.patch\n        package.py\n        package.pyc\n    beta\n    theta\n</code></pre>"},{"location":"Documentation/Software_Tools/Containers/","title":"Introduction to containers","text":""},{"location":"Documentation/Software_Tools/Containers/#what-are-containers","title":"What are containers?","text":"<p>Containers provide a method of packaging your code so that it can be run anywhere you have a container runtime. This enables you to create a container on your local laptop and then run it on Eagle or other computing resources. Containers provide an alternative way of isolating and packaging your code from solutions such as Conda environments. </p>"},{"location":"Documentation/Software_Tools/Containers/#docker-vs-singularity","title":"Docker vs. Singularity","text":"<p>The most common container runtime outside of HPC is Docker. Docker is not suited for the HPC environment on Eagle and is therefore not available on the system currently. Singularity is an alternative container tool which is provided. </p>"},{"location":"Documentation/Software_Tools/Containers/#compatibility","title":"Compatibility","text":"<p>Singularity is able to run most Docker images, but Docker is unable to run Singularity images. A key consideration when deciding to containerize an application is which container engine to build with. A suggested best practice is to build images with Docker when possible, as this provides more flexibility. Sometimes this is not possible though, and you may have to build with Singularity or maintain separate builds for each container engine. </p>"},{"location":"Documentation/Software_Tools/Containers/#container-advantages","title":"Container advantages","text":"<ul> <li>Portability: containers can be run on HPC, locally, and on cloud infrastructure used at NREL. </li> <li>Reproducibility: Containers are one option to ensure reproducible research by packaging all necessary software to reproduce an analysis. Containers are also easily versioned using a hash.</li> <li>Workflow integration: Workflow management systems such as Airflow, Nextflow, Luigi, and others provide built in integration with container engines. </li> </ul>"},{"location":"Documentation/Software_Tools/Containers/#hpc-hardware","title":"HPC hardware","text":"<p>Both Singularity and Docker provide the ability to use hardware based features of Eagle. A common usage for containers is packaging of GPU enabled tools such as TensorFlow. Singularity enables access to the GPU and driver on the host. Likewise the MPI installations available on Eagle can be accessed from correctly configured containers. </p>"},{"location":"Documentation/Software_Tools/Containers/#building","title":"Building","text":"<p>Containers are built from a container specification file, Dockerfiles for Docker or Singularity Definition File in Singularity. These files specify the steps necessary to create the desired package and the additional software packages to install and configure in this environment.  <pre><code>FROM ubuntu:20.04\n\nRUN apt-get -y update &amp;&amp; apt-get install -y python3 \n</code></pre></p> <p>The above Dockerfile illustrates the build steps to create a simple image. Images are normally built from a base image indicated by <code>FROM</code>, in this case Ubuntu. The ability to use a different base image provides a way to use packages which may work more easily on one Linux Distribution. For example the Linux distribution on Eagle is CentOS, building the above image would allow the user to install packages from Ubuntu repositories. </p> <p>The <code>RUN</code> portion of the above Dockerfile indicates the command to run, in this example it installs the Python 3 package. Additional commands such as <code>COPY</code>, <code>ENV</code>, and others enable the customization of your image to suit your compute environment requirements. </p>"},{"location":"Documentation/Software_Tools/Containers/registres/","title":"Container registries at NREL","text":""},{"location":"Documentation/Software_Tools/Containers/registres/#introduction","title":"Introduction","text":"<p>Container registries enable users to store container images. An overview of the steps to use each fo the main container registries available to NREL users is provided below. Registries can enable reproducibility by storing tagged versions of containers, and also facilitate transferring images easily between different computational resources. </p>"},{"location":"Documentation/Software_Tools/Containers/registres/#create-docker-images","title":"Create Docker images","text":"<p>Docker is not supported on NREL's HPC systems including Eagle. Instead Singularity is the container engine provided as a module. Singularity is able to pull Docker images and convert them to Singularity images. Although not always possible, we suggest creating Docker images when possible to ensure portability between compute resources and using Singularity to convert the image if it is to be run on an HPC system. </p>"},{"location":"Documentation/Software_Tools/Containers/registres/#accessibility","title":"Accessibility","text":"Registry Eagle Access AWS Access Docker Support Singularity Support Harbor Yes No Yes Yes AWS ECR Yes Yes Yes No* DockerHub Yes Yes Yes No* *for DockerHub and AWS ECR it may be possible to push images using ORAS, but this was not found to be a streamlined process in testing."},{"location":"Documentation/Software_Tools/Containers/registres/#aws-ecr","title":"AWS ECR","text":"<p>AWS ECR can be utilized by projects with a cloud allocation to host containers. ECR primarily can be used with Docker containers, although Singularity should also be possible. </p>"},{"location":"Documentation/Software_Tools/Containers/registres/#harbor","title":"Harbor","text":"<p>NREL's Harbor is a registry hosted by ITS that supports both Docker and Singularity containers. </p> <p>**NREL ITS is currently evaluating a replacement to internally hosted Harbor (likely moving to Enterprise DockerHub)</p>"},{"location":"Documentation/Software_Tools/Containers/registres/#docker","title":"Docker","text":""},{"location":"Documentation/Software_Tools/Containers/registres/#login","title":"Login","text":"<p>On your local machine to push a container to the registry.  <pre><code>docker login harbor.nrel.gov\n</code></pre></p>"},{"location":"Documentation/Software_Tools/Containers/registres/#prepare-image-for-push","title":"Prepare image for push","text":"<pre><code>docker tag SOURCE_IMAGE[:TAG] harbor.nrel.gov/REPO/IMAGE[:TAG]\n</code></pre> <pre><code>docker push harbor.nrel.gov/REPO/IMAGE[:TAG]\n</code></pre>"},{"location":"Documentation/Software_Tools/Containers/registres/#pull-docker-image-on-eagle","title":"Pull Docker image on Eagle","text":"<p>Pull and convert container to Singularity on Eagle.</p> <p>Note: <code>--nohttps</code> is not optimal but need to add certs for NREL otherwise there is a cert error.  <pre><code>singularity pull --nohttps --docker-login docker://harbor.nrel.gov/REPO/IMAGE[:TAG]\n</code></pre></p> <p>The container should now be downloaded and usable as usual</p>"},{"location":"Documentation/Software_Tools/Containers/registres/#singularity","title":"Singularity","text":""},{"location":"Documentation/Software_Tools/Containers/registres/#login-information","title":"Login information","text":"<p>Under your User Profile in Harbor obtain and export the following information <pre><code>export SINGULARITY_DOCKER_USERNAME=&lt;harbor username&gt;\nexport SINGULARITY_DOCKER_PASSWORD=&lt;harbor CLI secret&gt;\n</code></pre></p>"},{"location":"Documentation/Software_Tools/Containers/registres/#push-a-singularity-image","title":"Push a Singularity image","text":"<pre><code>singularity push &lt;image&gt;.sif oras://harbor.nrel.gov/&lt;PROJECT&gt;/&lt;IMAGE&gt;:&lt;TAG&gt;\n</code></pre>"},{"location":"Documentation/Software_Tools/Containers/registres/#pull-a-singularity-image","title":"Pull a Singularity image","text":"<pre><code>singularity pull oras://harbor.nrel.gov/&lt;PROJECT&gt;/&lt;IMAGE&gt;:&lt;TAG&gt;\n</code></pre>"},{"location":"Documentation/Software_Tools/Containers/registres/#dockerhub","title":"Dockerhub","text":"<p>Currently under testing, and not generally available</p>"},{"location":"Documentation/Software_Tools/Containers/registres/#credentials","title":"Credentials","text":"<p>To get the needed credentials for NREL Dockerhub, select your username in the top right -&gt; Account -&gt; Security -&gt; Create a new access token.</p> <p>The dialog box will describe how to use the security token with <code>docker login</code> to enable pulling and pushing containers. </p>"},{"location":"Documentation/Software_Tools/Containers/singularity/","title":"Singularity on Eagle","text":"<p>Singularity is installed on Eagle's compute nodes as a module named singularity-container.  Images can be copied to eagle and run, or can be generated from a recipe (definition file). </p> <p>Note: Input commands in the following examples are preceded by a <code>$</code>.</p>"},{"location":"Documentation/Software_Tools/Containers/singularity/#run-hello-world-ubuntu-image","title":"Run hello-world ubuntu image","text":"<p>Step 1: Log into compute node, checking it is running CentOS 7  <pre><code>$ ssh eagle.hpc.nrel.gov\n[$USER@el1 ~]$ srun -A MYALLOCATION -t 60 -N 1 --pty $SHELL\n[$USER@r1i3n18 ~]$ cat /etc/redhat-release \nCentOS Linux release 7.7.1908 (Core) </code></pre> Step 2: Load the singularity-container module <pre><code>[$USER@r1i3n18 ~]$ module purge\n[$USER@r1i3n18 ~]$ module load singularity-container\n</code></pre> Step 3: Retrieve hello-world image.  Be sure to use /scratch as images are typically large <pre><code>[$USER@r1i3n18 ~]$ cd /scratch/$USER\n[$USER@r1i3n18 $USER]$ mkdir -p singularity-images\n[$USER@r1i3n18 $USER]$ cd singularity-images\n[$USER@r1i3n18 singularity-images]$ singularity pull --name hello-world.simg shub://vsoch/hello-world\nProgress |===================================| 100.0% \nDone. Container is at: /lustre/eaglefs/scratch/$USER/singularity-images/hello-world.simg\n</code></pre> Step 4: Explore image details <pre><code>[$USER@r1i3n18 singularity-images]$ singularity inspect hello-world.simg # Shows labels\n{\n\"org.label-schema.usage.singularity.deffile.bootstrap\": \"docker\",\n    \"MAINTAINER\": \"vanessasaur\",\n    \"org.label-schema.usage.singularity.deffile\": \"Singularity\",\n    \"org.label-schema.schema-version\": \"1.0\",\n    \"WHATAMI\": \"dinosaur\",\n    \"org.label-schema.usage.singularity.deffile.from\": \"ubuntu:14.04\",\n    \"org.label-schema.build-date\": \"2017-10-15T12:52:56+00:00\",\n    \"org.label-schema.usage.singularity.version\": \"2.4-feature-squashbuild-secbuild.g780c84d\",\n    \"org.label-schema.build-size\": \"333MB\"\n}\n[$USER@r1i3n18 singularity-images]$ singularity inspect -r hello-world.simg # Shows the script run\n#!/bin/sh \n\nexec /bin/bash /rawr.sh\n</code></pre> Step 5: Run image default script <pre><code>[$USER@r1i3n18 singularity-images]$ singularity run hello-world.simg\nRaawwWWWWWRRRR!! Avocado.\n</code></pre> Step 6: Run in singularity bash shell <pre><code>[$USER@r1i3n18 singularity-images]$ cat /etc/redhat-release \nCentOS Linux release 7.7.1908 (Core)\n[$USER@r1i3n18 singularity-images]$ cat /etc/lsb-release \ncat: /etc/lsb-release: No such file or directory\n[$USER@r1i3n18 singularity-images]$ singularity shell hello-world.simg\nSingularity: Invoking an interactive shell within container...\n\nSingularity hello-world.simg:~&gt; cat /etc/lsb-release DISTRIB_ID=Ubuntu\nDISTRIB_RELEASE=14.04\nDISTRIB_CODENAME=trusty\nDISTRIB_DESCRIPTION=\"Ubuntu 14.04.5 LTS\"\nSingularity hello-world.simg:~&gt; cat /etc/redhat-release \ncat: /etc/redhat-release: No such file or directory\n</code></pre></p>"},{"location":"Documentation/Software_Tools/Containers/singularity/#create-a-centos-7-epel-image-with-mpi-support","title":"Create a CentOS 7 EPEL image with MPI support","text":"<p>This example shows how to create a CentOS 7 singularity image with openmpi installed.  It requires root/admin privileges to create the image so must be run on a user's computer with singularity installed.  After creation, the image can be copied to Eagle and run.</p> <p>Step 1: Create a new recipe based on singularityhub/centos:latest <pre><code>echo \"Bootstrap: shub\nFrom: singularityhub/centos:latest\n\" &gt; centos-mpi.recipe\n</code></pre> Step 2: Install development tools and enable epel repository after bootstrap is created <pre><code>echo \"%post\n  yum -y groupinstall \"Development Tools\"\n  yum -y install https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm\n\" &gt;&gt; centos-mpi.recipe\n</code></pre> Step 3: Download, compile and install openmpi 2.1 <pre><code>echo \"\ncurl -O https://download.open-mpi.org/release/open-mpi/v2.1/openmpi-2.1.2.tar.bz2\ntar jxf openmpi-2.1.2.tar.bz2\ncd openmpi-2.1.2\n./configure --prefix=/usr/local\nmake\nmake install\n\" &gt;&gt; centos-mpi.recipe\n</code></pre> Step 4: Compile and install example mpi application <pre><code>echo \"\nmpicc examples/ring_c.c -o ring\ncp ring /usr/bin/\n\" &gt;&gt; centos-mpi.recipe\n</code></pre> Step 5: Install a package found in EPEL, in this example R <pre><code>echo \"  yum -y install R\n\" &gt;&gt; centos-mpi.recipe\n</code></pre> Step 6: Set default script to run ring <pre><code>echo \"%runscript\n  /usr/bin/ring\n\" &gt;&gt; centos-mpi.recipe\n</code></pre> Step 7: Build image <pre><code>sudo $(type -p singularity) build centos-mpi.simg centos-mpi.recipe\n</code></pre> Step 8: Test image <pre><code>$ mpirun -np 20 singularity exec centos-mpi.simg /usr/bin/ring\n$ singularity run centos-epel-r.simg --version\nR version 3.4.4 (2018-03-15) -- \"Someone to Lean On\"\nCopyright (C) 2018 The R Foundation for Statistical Computing\nPlatform: x86_64-redhat-linux-gnu (64-bit)\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under the terms of the\nGNU General Public License versions 2 or 3.\nFor more information about these matters see\nhttp://www.gnu.org/licenses/.\n\n$ singularity exec centos-mpi.simg Rscript -e \"a &lt;- 42; A &lt;- a*2; print(A)\"\n[1] 84\n</code></pre></p>"},{"location":"Documentation/Software_Tools/FastX/fastx/","title":"FastX on Eagle DAV nodes","text":"<p>In addition to four standard ssh-only login nodes, Eagle is also equipped with several specialized Data Analysis and Visualization (DAV) login nodes, intended for HPC applications on Eagle that require a graphical user interface. It is not a general-purpose remote desktop, so we ask that you restrict your usage to only HPC or visualization software that requires Eagle.</p> <p>There are five internal DAV nodes available only to internal NREL users (or via the HPC VPN), and one node that is externally accessible. </p> <p>All DAV nodes have 36 CPU cores (Intel Xeon Gold 6150), 768GB RAM, one 32GB NVIDIA Quadro GV100 GPU, and offer a  Linux desktop (via FastX) with visualization capabilities, optional VirtualGL, and standard Linux terminal applications.</p> <p>DAV nodes are shared resources that support multiple simultaneous users. CPU and RAM usage is monitored by automated software, and  high usage may result in temporary throttling by Arbiter. Users who exceed 8 CPUs and 128GB RAM will receive an email  notice when limits have been exceeded, and another when usage returns to normal and restrictions are removed.</p>"},{"location":"Documentation/Software_Tools/FastX/fastx/#getting-started-with-fastx","title":"Getting Started with FastX","text":"<p>Information on how to log into a DAV node with a FastX remote desktop can be found in the FastX Documentation at https://www.nrel.gov/hpc/eagle-software-fastx.html. NREL users may use the web browser or the FastX desktop client. External users must use the FastX desktop client, or connect to the HPC VPN for the web client.</p>"},{"location":"Documentation/Software_Tools/FastX/fastx/#multiple-fastx-sessions","title":"Multiple FastX Sessions","text":"<p>FastX sessions may be closed without terminating the session and resumed at a later time. However, since there is a  license-based limit to the number of concurrent users, please fully log out/terminate your remote desktop session when you are done working and no longer need to leave processes running. Avoid having remote desktop sessions open on multiple nodes that you are not using, or your sessions may be terminated by system administrators to make licenses available for active users. </p>"},{"location":"Documentation/Software_Tools/FastX/fastx/#reattaching-fastx-sessions","title":"Reattaching FastX Sessions","text":"<p>Connections to the DAV nodes via eagle-dav.hpc.nrel.gov will connect you to a random node. To resume a session that you have suspended, take note of the node your session is running on (ed1, ed2, ed3, ed5, or ed6) before you close the FastX client or browser window, and you may directly access that node when you are ready to reconnect at <code>ed#.hpc.nrel.gov</code> in the FastX client or through your web browser at <code>https://ed#.hpc.nrel.gov</code>.</p>"},{"location":"Documentation/Software_Tools/FastX/fastx/#troubleshooting","title":"Troubleshooting","text":""},{"location":"Documentation/Software_Tools/FastX/fastx/#could-not-connect-to-session-bus-failed-to-connect-to-socket-tmpdbus-xxx-connection-refused","title":"Could not connect to session bus: Failed to connect to socket /tmp/dbus-XXX: Connection refused","text":"<p>This error is usually the result of a change to the default login environment, often by an alteration to <code>~/.bashrc</code> by  altering your $PATH, or by configuring Conda to launch into a (base) or other environment immediately upon login. </p> <p>For changes to your <code>$PATH</code>, be sure to prepend any changes with <code>$PATH</code> so that the default system paths are included before  any custom changes that you make. For example: <code>$PATH=$PATH:/home/username/bin</code> instead of <code>$PATH=/home/username/bin/:$PATH</code>.</p> <p>For conda users, the command <code>conda config --set auto_activate_base false</code> will prevent conda from launching into a base environment upon login.</p>"},{"location":"Documentation/Software_Tools/FastX/fastx/#how-to-get-help","title":"How to Get Help","text":"<p>Please contact the HPC Helpdesk at hpc-help@nrel.gov if you have any questions, technical issues, or receive a \"no free licenses\" error. </p>"},{"location":"Documentation/Software_Tools/Git/Git/","title":"Git","text":"<p>To begin, let's start by clearing up some common misconceptions about git. You probably have heard of git through the popular git-repository hosting web-service GitHub. GitHub (the hosting service) is to git (the open-source version control software) what the internet is to computers\u2014git is used locally to track incremental development and modifications to a collection of files, and GitHub gets those changes to serve as a synchronized, common access point. GitHub also has social aspects, like tracking who changed what and why. There are other git hosting services like GitLab which are similar to GitHub but offer slightly different features.</p> <p>The git workflow has some pretty colorful vocabulary, so let's define some of the terms to avoid confusion going forward: * Repository/repo: A git repository is an independent grouping of files to be tracked. A git repo has a \"root\" which is the directory that it sits in, and tracks further directory nesting from that. A single repo is often thought of as a complete project or application, though it's not uncommon to nest modules of an application as child repositories to isolate the development history of those submodules.</p> <ul> <li> <p>Commit: A commit, or \"revision\", is an individual change to a file (or set of files). It's like when you save a file, except with Git, every time you save it creates a unique ID (a.k.a. the \"SHA\" or \"hash\") that allows you to keep record of what changes were made when and by who. Commits usually contain a commit message which is a brief description of what changes were made.</p> </li> <li> <p>Fork: A fork is a personal copy of another user's repository that lives on your account. Forks allow you to freely make changes to a project without affecting the original. Forks remain attached to the original, allowing you to submit a pull request to the original's author to update with your changes. You can also keep your fork up to date by pulling in updates from the original.</p> </li> <li> <p>Pull: Pull refers to when you are fetching in changes and merging them. For instance, if someone has edited the remote file you're both working on, you'll want to pull in those changes to your local copy so that it's up to date.</p> </li> <li> <p>Pull request: Pull requests are proposed changes to a repository submitted by a user and accepted or rejected by a repository's collaborators. Like issues, pull requests each have their own discussion forum. </p> </li> <li> <p>Push: Pushing refers to sending your committed changes to a remote repository, such as a repository hosted on GitHub. For instance, if you change something locally, you'd want to then push those changes so that others may access them.</p> </li> </ul>"},{"location":"Documentation/Software_Tools/Jupyter/","title":"Introduction to Jupyter","text":""},{"location":"Documentation/Software_Tools/Jupyter/#what-is-jupyter","title":"What is Jupyter?","text":"<p>A web app for interactive Python in a browser </p> <ul> <li>\"Live coding\"</li> <li>Instant visualization</li> <li>Sharable</li> <li>Reproducible</li> <li>Customizable</li> <li>Now supports other languages besides Python (R, Julia..)   <ul> <li>https://github.com/jupyter/jupyter/wiki/Jupyter-kernels</li> <li>these slides were created using Markdown in Jupyter!</li> </ul> </li> </ul> <pre><code>import chart_studio.plotly as py\nimport plotly.figure_factory as ff\nimport pandas as pd\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nx = np.linspace(0, 5, 10)\ny = x ** 2\nn = np.array([0,1,2,3,4,5])\nxx = np.linspace(-0.75, 1., 100)\n\nfig, axes = plt.subplots(1, 4, figsize=(12,3))\n\naxes[0].scatter(xx, xx + 1.25*np.random.randn(len(xx)))\n#axes[0].scatter(xx, xx + 0.25*np.random.randn(len(xx)))\naxes[0].set_title(\"scatter\")\n\naxes[1].step(n, n**2.0, lw=2)\naxes[1].set_title(\"step\")\n\naxes[2].bar(n, n**2, align=\"center\", width=0.5, alpha=0.5)\naxes[2].set_title(\"bar\")\n\naxes[3].fill_between(x, x**2.5, x**3, color=\"green\", alpha=0.5);\naxes[3].set_title(\"fill_between\");\n</code></pre> <p></p>"},{"location":"Documentation/Software_Tools/Jupyter/#terminology-a-confusion-of-words","title":"Terminology - a Confusion of Words","text":""},{"location":"Documentation/Software_Tools/Jupyter/#jupyterhub","title":"Jupyterhub","text":"<pre><code>* Multi-user \"backend\" server\n* Controls launching the single-user Jupyter server\n* NREL's \"Europa\" runs Jupyterhub\n</code></pre> <p>(In general, don't worry about JupyterHub--unless you're a sysadmin)</p>"},{"location":"Documentation/Software_Tools/Jupyter/#jupyterjupyter-servernotebook-server","title":"Jupyter/Jupyter Server/Notebook server","text":"<pre><code>* The single-user server/web interface\n* Create/save/load .ipynb notebook files\n* What users generally interact with\n</code></pre>"},{"location":"Documentation/Software_Tools/Jupyter/#jupyter-notebook","title":"Jupyter Notebook","text":"<pre><code>* An individual .pynb file\n* Contains your Python code and visualizations\n* Sharable/downloadable\n</code></pre>"},{"location":"Documentation/Software_Tools/Jupyter/#jupyter-lab","title":"Jupyter lab","text":"<pre><code>* A \"nicer\" web interface for Jupyter - \"notebooks 2.0\"\n* Preferred by some\n* Lacking some features of \"classic\" notebooks\n</code></pre>"},{"location":"Documentation/Software_Tools/Jupyter/#kernel","title":"Kernel","text":"<pre><code>* The Python environment used by a notebook\n* More on kernels later\n</code></pre>"},{"location":"Documentation/Software_Tools/Jupyter/#using-europa","title":"Using Europa","text":"<p>We run a Jupyterhub server that is available. </p>"},{"location":"Documentation/Software_Tools/Jupyter/#europas-advantages","title":"Europa's Advantages:","text":"<pre><code>* Fast and easy access\n* Use regular Eagle credentials\n* Great for light to moderate processing/debugging/testing\n</code></pre>"},{"location":"Documentation/Software_Tools/Jupyter/#europas-disadvantages","title":"Europa's Disadvantages:","text":"<pre><code>* Limited resource: 8 cores/128GB RAM per user beefore automatic throttling\n* Compete with other users for CPU/RAM on a single machine\n* No custom environments (for now)\n</code></pre>"},{"location":"Documentation/Software_Tools/Jupyter/#simple-instructions","title":"Simple Instructions:","text":"<pre><code>- Visit Europa at (https://europa.hpc.nrel.gov/)\n\n- Log in using your HPC credentials\n\n- Opens a standard \"notebooks\" interface\n\n- Change url end /tree to /lab for Lab interface\n</code></pre>"},{"location":"Documentation/Software_Tools/Jupyter/#using-a-compute-node","title":"Using a Compute Node","text":""},{"location":"Documentation/Software_Tools/Jupyter/#advantages","title":"Advantages:","text":"<pre><code>* Custom environments\n* 36 cores and up to ~750GB RAM\n* No competing with other users for cores\n</code></pre>"},{"location":"Documentation/Software_Tools/Jupyter/#disadvantages","title":"Disadvantages:","text":"<pre><code>* Compete with other users for nodes\n* Costs AU\n</code></pre>"},{"location":"Documentation/Software_Tools/Jupyter/#is-more-than-one-node-possible","title":"...Is more than one node possible?","text":"<p>Yes... please see our advanced Jupyter documentation. </p>"},{"location":"Documentation/Software_Tools/Jupyter/#using-a-compute-node-hard-way","title":"Using a Compute Node - Hard Way","text":""},{"location":"Documentation/Software_Tools/Jupyter/#start-on-a-login-node","title":"Start on a login node:","text":"<p><code>ssh eagle.hpc.nrel.gov</code></p> <p><code>[user@el1:]$ srun -A &lt;account&gt; -t 02:00:00 --pty /bin/bash</code></p>"},{"location":"Documentation/Software_Tools/Jupyter/#when-the-job-starts-on-the-compute-node","title":"When the job starts on the compute node:","text":"<p><code>[user@r2i7n35]$ module load conda</code></p> <p><code>source activate myjupenv</code></p> <p><code>jupyter-notebook --no-browser --ip=$(hostname -s)</code></p> <p>note the node name (r2i7n35 in this example)</p> <p>and the url, e.g. <code>http://127.0.0.1:8888/?token=&lt;alphabet soup&gt;</code></p>"},{"location":"Documentation/Software_Tools/Jupyter/#in-a-terminal-on-your-computer","title":"In a terminal on your computer:","text":"<p><code>[user@laptop]$ ssh -N -L 8888:&lt;nodename&gt;:8888 username@eagle.hpc.nrel.gov</code></p> <p>copy full url from jupyter startup into your web browser. e.g.: </p> <p><code>http://127.0.0.1:8888/?token=&lt;alphabet soup&gt;</code></p>"},{"location":"Documentation/Software_Tools/Jupyter/#using-a-compute-node-easy-way","title":"Using a Compute Node - Easy Way","text":"<p>Automation makes life better!</p>"},{"location":"Documentation/Software_Tools/Jupyter/#auto-launching-with-an-sbatch-script","title":"Auto-launching with an sbatch script","text":"<p>Full directions included in the Jupyter repo.</p> <p>Download sbatch_jupyter.sh and auto_launch_jupyter.sh</p> <p>Edit sbatch_jupyter.sh to change:</p> <p><code>--account=*yourallocation*</code></p> <p><code>--time=*timelimit*</code></p> <p>Run auto_launch_jupyter.sh and follow directions</p> <p>That's it!</p>"},{"location":"Documentation/Software_Tools/Jupyter/#using-a-login-node","title":"Using a Login Node","text":"<p>Yes, you can run jupyter directly on a login node.</p> <p>Should you run jupyter directly on a login node?</p>"},{"location":"Documentation/Software_Tools/Jupyter/#reasons-to-not-run-jupyter-directly-on-a-login-node","title":"Reasons to Not Run Jupyter Directly on a Login Node","text":"<pre><code>* Heavy lifting should be done via Europa or compute nodes\n* Using a highly shared resource (login nodes)\n    * Competition for cycles\n    * arbiter2 will throttle moderate to heavy usage\n</code></pre>"},{"location":"Documentation/Software_Tools/Jupyter/#custom-conda-environments-and-jupyter-kernels","title":"Custom Conda Environments and Jupyter Kernels","text":""},{"location":"Documentation/Software_Tools/Jupyter/#creating-a-conda-environment","title":"Creating a conda environment:","text":"<p><code>conda create -n myjupyter -c conda-forge jupyter ipykernel</code></p> <p><code>source activate myjupyter</code></p> <p><code>conda install -c conda-forge scipy numpy matplotlib</code></p>"},{"location":"Documentation/Software_Tools/Jupyter/#add-custom-ipykernel","title":"Add custom ipykernel","text":"<p><code>python -m ipykernel install --user --name=myjupyter</code></p> <p>Restart your jupyter server</p> <p>New kernel will appear in drop-down as an option</p>"},{"location":"Documentation/Software_Tools/Jupyter/#remove-custom-ipykernel","title":"Remove custom ipykernel","text":"<p><code>jupyter kernelspec list</code></p> <p><code>jupyter kernelspec remove myoldjupyter</code></p>"},{"location":"Documentation/Software_Tools/Jupyter/#magic-commands","title":"Magic commands","text":"<p>Magic commands are \"meta commands\" that add extra functionality.</p> <p>Magic commands begin with % or %%.</p>"},{"location":"Documentation/Software_Tools/Jupyter/#a-few-useful-examples","title":"A Few Useful Examples","text":"<pre><code>* %lsmagic - list all magic commands\n* %run _file.py_ - run an external python script\n* %%time - placed at top of cell, prints execution time\n* %who - list all defined variables in notebook\n</code></pre> <pre><code>%lsmagic\n</code></pre> <pre><code>Available line magics:\n%alias  %alias_magic  %autoawait  %autocall  %automagic  %autosave  %bookmark  %cat  %cd  %clear  %colors  %conda  %config  %connect_info  %cp  %debug  %dhist  %dirs  %doctest_mode  %ed  %edit  %env  %gui  %hist  %history  %killbgscripts  %ldir  %less  %lf  %lk  %ll  %load  %load_ext  %loadpy  %logoff  %logon  %logstart  %logstate  %logstop  %ls  %lsmagic  %lx  %macro  %magic  %man  %matplotlib  %mkdir  %more  %mv  %notebook  %page  %pastebin  %pdb  %pdef  %pdoc  %pfile  %pinfo  %pinfo2  %pip  %popd  %pprint  %precision  %prun  %psearch  %psource  %pushd  %pwd  %pycat  %pylab  %qtconsole  %quickref  %recall  %rehashx  %reload_ext  %rep  %rerun  %reset  %reset_selective  %rm  %rmdir  %run  %save  %sc  %set_env  %store  %sx  %system  %tb  %time  %timeit  %unalias  %unload_ext  %who  %who_ls  %whos  %xdel  %xmode\n\nAvailable cell magics:\n%%!  %%HTML  %%SVG  %%bash  %%capture  %%debug  %%file  %%html  %%javascript  %%js  %%latex  %%markdown  %%perl  %%prun  %%pypy  %%python  %%python2  %%python3  %%ruby  %%script  %%sh  %%svg  %%sx  %%system  %%time  %%timeit  %%writefile\n\nAutomagic is ON, % prefix IS NOT needed for line magics.\n</code></pre>"},{"location":"Documentation/Software_Tools/Jupyter/#shell-commands","title":"Shell Commands","text":"<p>You can also run shell commands inside a cell. For example:</p> <p><code>!conda list</code> - see the packages installed in the environment you're using</p> <pre><code>!pwd\n!ls\n</code></pre> <pre><code>/home/tthatche/jup\nauto_launch_jupyter.sh    Jupyter Presentation.ipynb  slurm-6445885.out\ngeojsondemo.ipynb         old                         sshot1.png\nInteresting Graphs.ipynb  sbatch_jupyter.sh           sshot2.png\njup-logo.png              slurm\n</code></pre>"},{"location":"Documentation/Software_Tools/Jupyter/#interestinguseful-notebooks","title":"Interesting/Useful Notebooks","text":"<p>Awesome Jupyter</p> <p>Awesome Jupyterlab</p> <p>Plotting with matplotlib</p> <p>Python for Data Science</p> <p>Numerical Computing in Python</p> <p>The Sound of Hydrogen</p> <p>Plotting Pitfalls</p> <p>GeoJSON Extension</p>"},{"location":"Documentation/Software_Tools/Jupyter/#happy-notebooking","title":"Happy Notebooking!","text":""},{"location":"Documentation/Software_Tools/Jupyter/jupyterhub/","title":"JupyterHub","text":"<p>Prior to using Jupyterhub, you will have had to have logged into Eagle via the command line at least once.</p> <p>Given that, to start using Jupyterhub on Eagle, go to Europa in your local machine's browser, and log in with your Eagle username and password.  You should land in your home directory, and see everything there via the standard Jupyter file listing.</p> <p>From the \"New\" pulldown on the right hand side, you can start a notebook, open a terminal, or create a file or folder.  The default installation is Python version 3, and a variety of Conda modules are installed already. You can start a Python3 notebook right away, and access the Python modules that are already present. To see what's installed, from a notebook you can use the following command:</p> <pre><code>!conda list\n</code></pre> <p>Alternatively, you can start a Terminal, and use the usual conda commands from the shell.</p>"},{"location":"Documentation/Software_Tools/Jupyter/jupyterhub/#creating-a-custom-environment-to-access-from-the-notebook","title":"Creating a custom environment to access from the notebook","text":"<p>Start a Terminal session, and follow the instructions on the HPC website  to create an environment. Now, to make this environment visible from your future notebooks, run the following command:</p> <pre><code>source activate &lt;myenv&gt;\npython -m ipykernel install --user --name &lt;myenv&gt; --display-name \"How-you-want-your-custom-kernel-to-appear-in-the-notebook-pulldown (&lt;myenv&gt;)\"\n</code></pre> <p>where <code>&lt;myenv&gt;</code> is the argument to <code>-n</code> you used in your <code>conda create</code> command.</p> <p>After running this command, when you open a new notebook, you should see as an option your new environment, and once loaded be able to access all Python modules therein.</p>"},{"location":"Documentation/Software_Tools/Jupyter/jupyterhub/#using-jupyterhub-from-eagle","title":"Using Jupyterhub from Eagle","text":"<p>To use inside Eagle, the Jupyterhub server exists on the internal network @ https://europa-int/.</p>"},{"location":"Documentation/Software_Tools/Jupyter/jupyterhub/#customizing","title":"Customizing","text":"<p>JupyterHub provides the ability to use custom kernels including ones for other popular programming languages such as Julia and R. NREL's custom kernels documentation provides more information on how to setup JupyterHub with other languages. </p>"},{"location":"Documentation/Software_Tools/building-packages/","title":"Building packages on Eagle for individual or project use.","text":"<p>This training module will walk through how to build a reasonably complex package, OpenMPI, and deploy it for use by yourself or members of a project.</p> <ol> <li> <p>Acquire the package and set up for build</p> </li> <li> <p>Configure, build, and install the package</p> </li> <li> <p>Setting up your own environment module</p> </li> </ol>"},{"location":"Documentation/Software_Tools/building-packages/#why-build-your-own-application","title":"Why build your own application?","text":"<ul> <li>Sometimes, the package version that you need, or the capabilities you want, are only available as source code.</li> <li>Other times, a package has dependencies on other ones with application programming interfaces that change rapidly. A source code build might have code to adapt to the (older, newer) libraries you have available, whereas a binary distribution will likely not. In other cases, a binary distribution may be associated with a particular Linux distribution and version different from Eagle's. One example is a package for Linux version X+1 (with a shiny new libc). If you try to run this on Linux version X, you will almost certainly get errors associated with the GLIBC version required. If you build the application against your own, older libc version, those dependencies are not created.</li> <li>Performance; for example, if a more performant numerical library is available, you may be able to link against it. A pre-built binary may have been built against a more universally available but lower performance library. The same holds for optimizing compilers.</li> <li>Curiosity to know more about the tools you use.</li> <li>Pride of building one's tools oneself.</li> <li>For the sheer thrill of building packages.</li> </ul>"},{"location":"Documentation/Software_Tools/building-packages/acquire/","title":"Acquire","text":""},{"location":"Documentation/Software_Tools/building-packages/acquire/#getting-the-package","title":"Getting the package","text":"<ol> <li> <p>Change working directory to the location where you'll build the package. A convenient location is <code>/scratch/$USER</code>, which we'll use for this example. <code>cd /scratch/$USER</code></p> </li> <li> <p>OpenMPI can be found at https://www.open-mpi.org/software/ompi/. This will automatically redirect you to the latest version, but older releases can be seen in the left menu bar. For this, choose version 4.1.</p> </li> <li> <p>There are several packaging options. Here, we'll get the bzipped tarball <code>openmpi-4.1.0.tar.bz2</code>. You can either download it to a local machine (laptop) and then scp the file over to Eagle, or get it directly on Eagle with wget. <pre><code>wget https://download.open-mpi.org/release/open-mpi/v4.1/openmpi-4.1.0.tar.bz2\n</code></pre></p> <p>You should now have a compressed tarball in your scratch directory.</p> </li> <li> <p>List the contents of the tarball before unpacking. This is very useful to avoid inadvertently filling a directory with gobs of files and directories when the tarball has them at the top of the file structure), <pre><code>tar -tf openmpi-4.1.0.tar.bz2\n</code></pre></p> </li> <li> <p>Unpack it via <pre><code>tar -xjf openmpi-4.1.0.tar.bz2\n</code></pre> If you're curious to see what's in the file as it unpacks, add the <code>-v</code> option. </p> </li> <li> <p>You should now have an <code>openmpi-4.1.0</code> directory. <code>cd openmpi-4.1.0</code>, at which point you are in the top level of the package distribution.</p> </li> </ol> <p>You can now proceed to configuring, making, and installing.</p>"},{"location":"Documentation/Software_Tools/building-packages/config_make_install/","title":"Config Make Install","text":""},{"location":"Documentation/Software_Tools/building-packages/config_make_install/#configuring-your-build","title":"Configuring your build","text":"<ol> <li> <p>We will illustrate a package build that relies on the popular autotools system. Colloquially, this is the <code>configure; make; make install</code> process that is often encountered first by those new to package builds on Linux. Other build systems like CMake (which differ primarily in the configuration steps) won't be covered. If you need to build a package that relies on CMake, please contact hpc-help@nrel.gov for assistance.</p> </li> <li> <p>We'll use GCC version 8.4.0 for this illustration, so load the associated module first (i.e., <code>gcc/8.4.0</code>).</p> </li> <li> <p>Now that you've acquired and unpacked the package tarball and changed into the top-level directory of the package, you should see a script named \"configure\". In order to see all available options to an autotools configure script, use <code>./configure -h</code> (don't forget to include the <code>./</code> explicit path, otherwise the script will not be found in the default Linux search paths, or worse, a different script will be found).</p> <p>We will build with the following command:  <pre><code>./configure --prefix=/scratch/$USER/openmpi/4.1.0-gcc-8.4.0 --with-slurm --with-pmi=/nopt/slurm/current --with-gnu-ld --with-lustre --with-zlib --without-psm --without-psm2 --with-ucx --without-verbs --with-hwloc=external --with-hwloc-libdir=/nopt/nrel/apps/base/2020-05-12/spack/opt/spack/linux-centos7-x86_64/gcc-8.4.0/hwloc-1.11.11-mb5lwdajmllvrdtwltwe3r732aca76ny/lib --enable-cxx-exceptions --enable-mpi-cxx --enable-mpi-fortran --enable-static LDFLAGS=\"-L/nopt/nrel/apps/base/2020-05-12/spack/opt/spack/linux-centos7-x86_64/gcc-8.4.0/11.0.2-4x2ws7fkooqbrerbsnfbzs6wyr5xutdk/lib64 -L/nopt/nrel/apps/base/2020-05-12/spack/opt/spack/linux-centos7-x86_64/gcc-8.4.0/cuda-11.0.2-4x2ws7fkooqbrerbsnfbzs6wyr5xutdk/lib64 -Wl,-rpath=/nopt/nrel/apps/base/2020-05-12/spack/opt/spack/linux-centos7-x86_64/gcc-8.4.0/hwloc-1.11.11-mb5lwdajmllvrdtwltwe3r732aca76ny/lib -Wl,-rpath=/nopt/nrel/apps/base/2020-05-12/spack/opt/spack/linux-centos7-x86_64/gcc-8.4.0/cuda-11.0.2-4x2ws7fkooqbrerbsnfbzs6wyr5xutdk/lib64\" CPPFLAGS=-I/nopt/nrel/apps/base/2020-05-12/spack/opt/spack/linux-centos7-x86_64/gcc-8.4.0/hwloc-1.11.11-mb5lwdajmllvrdtwltwe3r732aca76ny/include\n</code></pre> These options are given for the following reasons.</p> <ul> <li><code>--prefix=</code> : This sets the location that \"make install\" will ultimately populate. If this isn't given, generally the default is to install into /usr or /usr/local, both of which require privileged access. We'll set up the environment using environment modules to point to this custom location.</li> <li><code>--with-slurm</code> : Enables the interface with the Slurm resource manager</li> <li><code>--with-pmi=</code> : Point to the Process Management Interface, the abstraction layer for MPI options</li> <li><code>--with-gnu-ld</code> : Letting the build system know that linking will be done with GNU's linker, rather than a commercial or alternative open one.</li> <li><code>--with-lustre</code> : Enable Lustre features</li> <li><code>--with-zlib</code> : Enable compression library</li> <li><code>--without-psm[2]</code> : Explicitly turn off interfaces to Intel's Performance Scaled Messaging for the now-defunct Omni-Path network</li> <li><code>--with-ucx=</code> : Point to UCX, an intermediate layer between the network drivers and MPI</li> <li><code>--without-verbs=</code> : For newer MPIs, communications go through UCX and/or libfabric, not directly to the Verbs layer</li> <li><code>--with-hwloc[-libdir]=</code> : Point to a separately built hardware localization library for process pinning</li> <li><code>--enable-cxx-exceptions</code>, <code>--enable-mpi-cxx</code> : Build the C++ interface for the libraries</li> <li><code>--enable-mpi-fortran</code> : Build the Fortran interface for the libraries</li> <li><code>--enable-static</code> : Build the .a archive files for static linking of applications</li> <li><code>LDFLAGS</code> : -L options point to non-standard library locations. -Wl,-rpath options embed paths into the binaries, so that having LD_LIBRARY_PATH set correctly is not necessary (i.e., no separate module for these components).</li> <li><code>CPPFLAGS</code> : Point to header files in non-standard locations.</li> </ul> <p>NOTE: The CUDA paths are not needed for CUDA function per se, but the resulting MPI errors out without setting them. There appears to be a lack of modularity that sets up a seemingly unneeded dependency.</p> <p>After lots of messages scroll by, you should be returned to a prompt following a summary of options. It's not a bad idea to glance through these, and make sure everything makes sense and is what you intended.</p> </li> <li> <p>Now that the build is configured, you can \"make\" it. For packages that are well integrated with automake, you can speed the build up by parallelizing it over multiple processes with the <code>-j #</code> option. If you're building this on a compute node, feel free to set this option to the total number of cores available. On the other hand, if you're using a login node, be a good citizen and leave cores available for other users (i.e., don't use more than 4; Arbiter should limit access at any rate regardless of this setting).</p> <pre><code>make -j 4\n</code></pre> </li> <li> <p>Try a <code>make check</code> and/or a <code>make test</code>. Not every package enables these tests, but if they do, it's a great idea to run these sanity checks to find if your build is perfect, maybe-good-enough, or totally wrong before building lots of other software on top of it.</p> </li> <li> <p>Assuming checks passed if present, it's now time for <code>make install</code>. Assuming that completes without errors, you can move onto creating an environment module to use your new MPI library.</p> </li> </ol>"},{"location":"Documentation/Software_Tools/building-packages/modules/","title":"Modules","text":""},{"location":"Documentation/Software_Tools/building-packages/modules/#setting-up-your-module","title":"Setting up your module","text":"<ol> <li> <p>Now that the package has been installed to your preferred location, we can set up an environment module.</p> <p>a. If this is your first package, then you probably need to create a place to collect modulefiles. For example, <code>mkdir -p /scratch/$USER/modules/default/modulefiles</code>.</p> <p>b. You can look at the systems module collection(s), e.g., <code>/nopt/nrel/apps/modules/default/modulefiles</code>, to see how modules are organized from a filesystem perspective. In short, each library, application, or framework has its own directory in the <code>modulefiles</code> directory, and the modulefile itself sits either in this directory, or one level lower to accomodate additional versioning. In this example, there is the MPI version (4.1.0), as well as the compiler type and version (GCC 8.4.0) to keep track of. So, we'll make a <code>/scratch/$USER/modules/default/modulefiles/openmpi/4.1.0</code> directory, and name the file by the compiler version used to build (gcc-8.4.0). You're free to modify this scheme to suit your own intentions.</p> <p>c. In the <code>openmpi/4.1.0/gcc840</code> directory you just made, or whatever directory name you chose, goes the actual modulefile. It's much easier to copy an example from the system collection than to write one de novo, so you can do</p> <pre><code>cp /nopt/nrel/apps/modules/default/modulefiles/openmpi/4.0.4/gcc-8.4.0.lua /scratch/$USER/modules/default/modulefiles/openmpi/4.1.0/.\n</code></pre> <p>The Lmod modules system uses the Lua language natively for module code. It is not necessary for you to know the language to modify our examples. Tcl modules will also work under Lmod, but don't offer quite as much flexibility.</p> <p>d. For this example, (a) the OpenMPI version we're building is 4.1.0 instead of 4.0.4, and (b) the location is in <code>/scratch/$USER</code>, rather than <code>/nopt/nrel/apps</code>. So, edit <code>/scratch/$USER/modules/default/modulefiles/openmpi/4.1.0/gcc-8.4.0.lua</code> to make the required changes. Most of these changes only need to be made at the top of the file; variable definitions take care of the rest.</p> <p>e. Now you need to make a one-time change in order to see modules that you put in this collection (<code>/scratch/$USER/modules/default/modulefiles</code>). In your <code>$HOME/.bash_profile</code>, add the following line near the top:</p> <pre><code>module use /scratch/$USER/modules/default/modulefiles\n</code></pre> <p>Obviously, if you've built packages before and enabled them this way, you don't have to do this again!</p> </li> <li> <p>Now logout, log back in, and you should see your personal modules collection with a brand new module.</p> <pre><code>[cchang@el1 ~]$ module avail\n\n---------------------------------- /scratch/cchang/modules/default/modulefiles -----------------------------------\nopenmpi/4.1.0/gcc-8.4.0\n</code></pre> <p>Notice that the \".lua\" extension does not appear--the converse is also true, if the extension is missing it will not appear via module commands! As a sanity check, it's a good idea to load the module, and check that an executable file you know exists there is in fact on your PATH:</p> <pre><code>[cchang@el1 ~]$ module load openmpi/4.1.0/gcc-8.4.0\n[cchang@el1 ~]$ which mpirun\n/scratch/cchang/openmpi/4.1.0-gcc-8.4.0/bin/mpirun\n</code></pre> </li> </ol>"},{"location":"Documentation/Systems/","title":"NREL Systems","text":"<p>NREL operates three on-premises systems for computational work. </p>"},{"location":"Documentation/Systems/#system-configurations","title":"System configurations","text":"Name Eagle Swift Vermilion OS CentOS Rocky Linux RedHat Login eagle.hpc.nrel.gov swift.hpc.nrel.gov vs.hpc.nrel.gov CPU Dual Intel Xeon Gold Skylake 6154 Dual AMD EPYC 7532 Rome CPU Dual AMD EPYC 7532 Rome CPU Interconnect InfiniBand EDR InfiniBand HDR 25GbE HPC scheduler Slurm Slurm Slurm Network Storage 17PB Lustre FS 3PB NFS 440 TB GPU Dual NVIDIA Tesla V100 None 17 nodes Single A100 Memory 96GB, 192GB, 768GB 256GB 256GB (base) Number of Nodes 2618 484 159 virtual"},{"location":"Documentation/Systems/Swift/","title":"About the Swift Cluster","text":"<p>Swift is an NREL supercomputing cluster housed in the ESIF data center for LDRD projects, as an alternative to the NREL flagship cluster Eagle.  Swift is an AMD-based cluster, featuring 484 nodes with two EPYC 7532 (32 core/64 thread) CPUs and 256GB RAM each.</p> <p>Please see the System Configurations page for more information about hardware, storage, and networking.</p>"},{"location":"Documentation/Systems/Swift/#accessing-swift","title":"Accessing Swift","text":"<p>Access to Swift requires an NREL HPC account and permission to join an existing allocation. Please see the System Access page for more information on accounts and allocations.</p>"},{"location":"Documentation/Systems/Swift/#for-nrel-employees","title":"For NREL Employees:","text":"<p>Swift can be reached from the NREL VPN via ssh to swift.hpc.nrel.gov, swift-login-1.hpc.nrel.gov, or swift-login-2.hpc.nrel.gov.</p>"},{"location":"Documentation/Systems/Swift/#for-external-collaborators","title":"For External Collaborators:","text":"<p>There are currently no external-facing login nodes for Swift. There are two options to connect:</p> <ol> <li>ssh hpcsh.nrel.gov and log in with your username, password, and OTP code. Once connected, ssh to the login nodes as above.</li> <li>Connect to the HPC VPN and ssh to the login nodes as above.</li> </ol>"},{"location":"Documentation/Systems/Swift/#get-help-with-swift","title":"Get Help With Swift","text":"<p>The Known Issues and Answers page has answers to common Swift questions. </p> <p>Please see the Help and Support Page for further information on how to seek assistance with Swift or your NREL HPC account. </p>"},{"location":"Documentation/Systems/Swift/applications/","title":"Swift applications","text":"<p>Some optimized versions of common applications are provided for the Swift cluster. Below is a list of how to utilize these applications and the optimizations for Swift. </p>"},{"location":"Documentation/Systems/Swift/applications/#modules","title":"Modules","text":"<p>Many are available as part of the Modules setup.</p>"},{"location":"Documentation/Systems/Swift/applications/#tensorflow","title":"TensorFlow","text":"<p>TensorFlow has been built for the AMD architecture on Swift. This was done by using the following two build flags. </p> <pre><code>-march=znver2\n-mtune=znver2\n</code></pre> <p>This version of TensorFlow can be installed from a wheel file:  <pre><code>pip install --upgrade --no-deps --force-reinstall /nopt/nrel/apps/wheels/tensorflow-2.4.2-cp38-cp38-linux_x86_64-cpu.whl\n</code></pre></p> <p>Currently, this wheel is not built with NVIDIA CUDA support for running on GPU. </p> <p>TensorFlow installed on Swift with Conda may be significantly slower than the optimized version </p>"},{"location":"Documentation/Systems/Swift/filesystems/","title":"Swift Filesystem Architecture Overview","text":"<p>Swift's central storage currently has a capacity of approximately 3PB, served over NFS (Network File System). It is a performant system with  multiple read and write cache layers and redundancies for data protection, but it is not a parallel filesystem, unlike Eagle's Lustre configuration.</p> <p>The underlying filesystem and volume management is via ZFS. Data is protected in ZFS RAID arrangements (raidz3) of 8 storage disks and 3 parity disks. </p> <p>Each Swift fileserver serves a single storage chassis (JBOD, \"just a bunch of disks\") consisting of multiple spinning disks plus SSD drives for read and write caches. </p> <p>Each fileserver is also connected to a second storage chassis to serve as a redundant backup in case the primary fileserver for that storage chassis fails, allowing continued access to the data on the storage chassis until the primary fileserver for that chassis is restored to service.</p>"},{"location":"Documentation/Systems/Swift/filesystems/#project-storage-projects","title":"Project Storage: /projects","text":"<p>Each active project is granted a subdirectory under <code>/projects/&lt;projectname&gt;</code>. This is where the bulk of data is expected to be, and where jobs should generally be run from. Storage quotas are based on the allocation award.</p> <p>Quota usage can be viewed at any time by issuing a <code>cd</code> command into the project directory, and using the <code>df -h</code> command to view total, used, and remaining available space for the mounted project directory.</p>"},{"location":"Documentation/Systems/Swift/filesystems/#nfs-automount-system","title":"NFS Automount System","text":"<p>Project directories are automatically mounted or unmounted via NFS on an \"as-needed\" basis. /projects directories that have not been accessed for a period of time will be umounted and not immediately visible via a command such as <code>ls /projects</code>, but will become immediately available if a file or path is accessed with an <code>ls</code>, <code>cd</code>, or other file access is made in that path. </p>"},{"location":"Documentation/Systems/Swift/filesystems/#home-directories-home","title":"Home Directories: /home","text":"<p>/home directories are mounted as <code>/home/&lt;username&gt;</code>. Home directories are hosted under the user's initial /project directory. Quotas in /home are included as a part of the quota of that project's storage allocation. </p>"},{"location":"Documentation/Systems/Swift/filesystems/#scratch-space-scratchusername-and-scratchusernamejobid","title":"Scratch Space: /scratch/username and /scratch/username/jobid","text":"<p>For users who also have Eagle allocations, please be aware that scratch space on Swift behaves differently, so adjustments to job scripts may be necessary. </p> <p>The scratch directory on each Swift compute node is a 1.8TB spinning disk, and is accessible only on that node. The default writable path for scratch use is <code>/scratch/&lt;username&gt;</code>. There is no global, network-accessible <code>/scratch</code> space. <code>/projects</code> and <code>/home</code> are both network-accessible, and may be used as /scratch-style working space instead.</p>"},{"location":"Documentation/Systems/Swift/filesystems/#temporary-space-tmpdir","title":"Temporary space: $TMPDIR","text":"<p>When a job starts, the environment variable <code>$TMPDIR</code> is set to <code>/scratch/&lt;username&gt;/&lt;jobid&gt;</code> for the duration of the job. This is temporary space only, and should be purged when your job is complete. Please be sure to use this path instead of /tmp for your tempfiles.</p> <p>There is no expectation of data longevity in scratch space, and it is subject to purging once the node is idle. If desired data is stored here during the job, please be sure to copy it to a /projects directory as part of the job script before the job finishes.</p>"},{"location":"Documentation/Systems/Swift/filesystems/#mass-storage-system","title":"Mass Storage System","text":"<p>There is no Mass Storage System for deep archive storage on Swift. However, Swift is expected to be a part of the upcoming Campaign Storage system (VAST storage) in the future, allowing those projects with allocations on Eagle to seamlessly transfer data between clusters, and into the Eagle MSS system.</p>"},{"location":"Documentation/Systems/Swift/filesystems/#backups-and-snapshots","title":"Backups and Snapshots","text":"<p>There are no backups or snapshots of data on Swift. Though the system is protected from hardware failure by multiple layers of redundancy, please keep regular backups of important data on Swift, and consider using a Version Control System (such as Git) for important code. </p>"},{"location":"Documentation/Systems/Swift/help/","title":"Swift Technical Support Contacts","text":"<p>Please direct all inquiries regarding Swift to the HPC Helpdesk at HPC-Help@nrel.gov, including:</p> <ul> <li>Technical issues or questions</li> <li>Software installation requests or assistance</li> <li>Account or allocation issues </li> <li>All other general inquiries regarding Swift</li> </ul>"},{"location":"Documentation/Systems/Swift/help/#microsoft-teams","title":"Microsoft Teams","text":"<p>More information on joining the Microsoft Teams Swift channel coming soon.</p>"},{"location":"Documentation/Systems/Swift/help/#additional-support","title":"Additional Support","text":"<p>Additional HPC help and contact information can be found on the NREL HPC Help main page.</p>"},{"location":"Documentation/Systems/Swift/known/","title":"Swift Known Issues and Solutions","text":""},{"location":"Documentation/Systems/Swift/known/#mpi","title":"MPI","text":"<ol> <li>IntelMPI appears to be working properly.</li> <li>OpenMPI appears to be working properly.</li> </ol>"},{"location":"Documentation/Systems/Swift/known/#hardware","title":"Hardware","text":"<ol> <li>There are no GPU nodes currently available on Swift.</li> </ol>"},{"location":"Documentation/Systems/Swift/known/#softwareenvironment","title":"Software/Environment","text":"<ol> <li>There is a very basic version of conda in the \"anaconda\" directory in each  /nopt/nrel/apps/YYMMDDa directory. However, there is a more complete environment pointed to by the module under /nopt/nrel/apps/modules. This is set up like Eagle. Please see Eagle's Python Documentation for more information.</li> <li>User accounts have a default set of keys <code>cluster</code> and <code>cluster.pub</code>. The <code>config</code> file will use these even if you generate a new keypair using <code>ssh-keygen</code>. If you are adding your keys to Github or elsewhere you should either use <code>cluster.pub</code> or will have to modify the <code>config</code> file.</li> </ol>"},{"location":"Documentation/Systems/Swift/known/#job-scheduling","title":"Job Scheduling","text":"<ol> <li>Use <code>--cpus-per-task</code> with srun/sbatch otherwise some applications may only utilize a single core. This behavior differs from Eagle.</li> <li>By default, nodes can be shared between users.  To get exclusive access to a node use the <code>--exclusive</code> flag in your sbatch script or on the sbatch command line.</li> <li>There are some example slurm scripts in the example directory.</li> </ol>"},{"location":"Documentation/Systems/Swift/known/#how-to-get-help","title":"How to Get Help","text":"<p>Please visit the Help and Support Page for assistance or to report an issue.</p>"},{"location":"Documentation/Systems/Swift/modules/","title":"Swift modules","text":"<p>This describes how to activate and use the modules available on Swift. </p> <p>There are currently a number of known issues on Swift pleace check Known issues for a complete list</p>"},{"location":"Documentation/Systems/Swift/modules/#source","title":"Source","text":"<p>Environments are provided with a number of commonly used modules including compilers, common build tools, specific AMD optimized libraries, and some analysis tools. When you first login there is a default set of modules available.  These can be seen by running the command:</p> <pre><code>module avail \n</code></pre> <p>Since Swift is a new machine we are experimenting with additional environments. The environments are in date stamped subdirectory under in the directory /nopt/nrel/apps.  Each environemnt directory has a file myenv.*.   If the myenv.*. is missing from a directory then that environment is a work in progress.   Sourcing myenv.* file will enable the environment and give you a new set of modules.  </p> <p>For example to enable the environment /nopt/nrel/apps/210728a source the provided environment file. </p> <pre><code>source /nopt/nrel/apps/210728a/myenv.2107290127\n</code></pre> <p>You will now have access to the modules provided. These can be listed using the following: </p> <pre><code>ml avail \n</code></pre> <p>If you want to build applications you can then module load compilers and the like; for example</p> <pre><code>ml gcc openmpi\n</code></pre> <p>will load gnu 9.4 and openmpi.</p> <p>Software is installed using a spack hierarchy. It is possible to add software to the hierarchy.  This should be only done by people responsible for installing software for all users.  It is also possible to do a spack install creating a new level of the hierarchy in your personal space.  These procedures are documented in https://github.nrel.gov/tkaiser2/spackit.git in the file Notes03.md under the sections Building on the hierarchy and Building outside the hierarchy.  If you want to try this please contact Tim Kaiser tkaiser2@nrel.gov to walk through the procedure.</p> <p>Most environments have an example directory.  You can copy this directory to you own space and compile and run the examples.  The files runintel and runopenmp are  simple batch scripts.  These also have \"module load\" lines that you need to run before building with either compiler set.</p>"},{"location":"Documentation/Systems/Swift/running/","title":"Running on Swift","text":"<p>Please see the Modules page for information about setting up your environment and loading modules. </p>"},{"location":"Documentation/Systems/Swift/running/#login-nodes","title":"Login nodes","text":"<pre><code>swift.hpc.nrel.gov\nswift-login-1.hpc.nrel.gov\n</code></pre> <p><code>swift.hpc.nrel.gov</code> is a round-robin alias that will connect you to any available login node.</p> <p>External (non-NREL) users will need to either use the HPC VPN, or connect to an intermediate system such as the SSH gateway host at hpcsh.nrel.gov first, then ssh from that host to one of the addresses above. Note that hpcsh.nrel.gov will require Password+OTP to log in, but the connection to Swift will only require a password, no OTP code.</p>"},{"location":"Documentation/Systems/Swift/running/#slurm-and-partitions","title":"Slurm and Partitions","text":"<p>The most up to date list of partitions can always be found by running the <code>sinfo</code> command on the cluster.</p> Partition Description long jobs up to ten days of walltime standard jobs up to two days of walltime parallel optimized for large parallel jobs, up to two days of walltime debug two nodes reserved for short tests, up to four hours of walltime <p>Each partition also has a matching <code>-standby</code> partition. Allocations which have consumed all awarded AUs for the year may only submit jobs to these partitions, and their default QoS will be set to <code>standby</code>. Jobs in standby partitions will be scheduled when there are otherwise idle cycles and no other non-standby jobs are available. </p> <p>Any allocation may submit a job to a standby QoS, even if there are unspent AUs.</p>"},{"location":"Documentation/Systems/Swift/running/#allocation-unit-au-charges","title":"Allocation Unit (AU) Charges","text":"<p>The equation for calculating the AU cost of a job on Swift is:</p> <p><code>AU cost = (Walltime in hours * Number of Nodes * QoS Factor * Charge Factor)</code></p> <p>The Walltime is the actual length of time that the job runs, in hours or fractions thereof.</p> <p>The Number of nodes can be whole nodes or fractions of a node. See below for more information.</p> <p>The Charge Factor for Swift is 5. </p> <p>The QoS Factor for normal priority jobs is 1. </p> <p>The QoS Factor for high-priority jobs is 2.</p> <p>The QoS Factor for standby priority jobs is 0. There is no AU cost for standby jobs.</p> <p>One node for one hour of walltime at normal priority costs 5 AU total.</p> <p>One node for one hour of walltime at high priority costs 10 AU total.</p>"},{"location":"Documentation/Systems/Swift/running/#fractional-nodes","title":"Fractional Nodes","text":"<p>Swift allows jobs to share nodes, meaning fractional allocations are possible. </p> <p>Standard compute nodes have 128 CPU cores and 256GB RAM.</p> <p>When a job only requests part of a node, usage is tracked on the basis of: </p> <p>1 core = 2GB RAM = 1/128th of a node</p> <p>Using all resources on a single node, whether CPU, RAM, or both, will max out at 128/128 per node = 1.</p> <p>For example, a job that requests 64 cores and 128GB RAM (one half of a node) would be: </p> <p>1 hour walltime * 0.5 nodes * 1 QoS Factor * 5 Charge Factor = 2.5 AU per node-hour.</p>"},{"location":"Documentation/Systems/Swift/running/#software-environments-and-example-files","title":"Software Environments and Example Files","text":"<p>Multiple software environments are available on Swift, with a number of commonly used modules including compilers, common build tools, specific AMD optimized libraries, and some analysis tools. The environments are in date stamped subdirectories, in the directory /nopt/nrel/apps. Each environment directory has a file myenv.*.  Sourcing that file will enable the environment.</p> <p>When you login you will have access to the default environments and the myenv file will have been sourced for you. You can see the directory containing the environment by running the <code>module avail</code> command.  </p> <p>In the directory for an environment you will see a subdirectory example. This contains a makefile for a simple hello world program written in both Fortran and C. The README.md file contains additional information, most of which is replicated here. It is suggested that you copy the example directory to your own /home for experimentation:</p> <pre><code>cp -r example ~/example\ncd ~/example\n</code></pre>"},{"location":"Documentation/Systems/Swift/running/#simple-batch-script","title":"Simple batch script","text":"<p>Here is a sample batch script for running the 'hello world' example program, runopenmpi. </p> <pre><code>#!/bin/bash\n#SBATCH --job-name=\"install\"\n#SBATCH --nodes=2\n#SBATCH --tasks-per-node=2\n#SBATCH --exclusive\n#SBATCH --partition=debug\n#SBATCH --time=00:01:00\n\n\ncat $0\n\n#These should be loaded before doing a make\nmodule load gcc  openmpi export OMP_NUM_THREADS=2\nsrun  -n 4 ./fhostone -F\nsrun  -n 4 ./phostone -F\n</code></pre> <p>To run this you must first ensure that slurm is in your path by running:</p> <pre><code>module load slurm\n</code></pre> <p>Then submit the sbatch script with: </p> <pre><code>sbatch --partition=test runopenmpi\n</code></pre>"},{"location":"Documentation/Systems/Swift/running/#building-the-hello-world-example","title":"Building the 'hello world' example","text":"<p>Obviously for the script given above to work you must first build the application. You need to:</p> <ol> <li>Load the modules</li> <li>make</li> </ol>"},{"location":"Documentation/Systems/Swift/running/#loading-the-modules","title":"Loading the modules.","text":"<p>We are going to use gnu compilers with OpenMPI.</p> <pre><code>ml gcc openmpi\n</code></pre>"},{"location":"Documentation/Systems/Swift/running/#run-make","title":"Run make","text":"<pre><code>make\n</code></pre>"},{"location":"Documentation/Systems/Swift/running/#full-procedure","title":"Full procedure","text":"<pre><code>[nrmc2l@swift-login-1 ~]$ cd ~\n[nrmc2l@swift-login-1 ~]$ mkdir example\n[nrmc2l@swift-login-1 ~]$ cd ~/example\n[nrmc2l@swift-login-1 ~]$ cp -r /nopt/nrel/apps/210928a/example/* .\n\n[nrmc2l@swift-login-1 ~ example]$ cat runopenmpi #!/bin/bash\n#SBATCH --job-name=\"install\"\n#SBATCH --nodes=2\n#SBATCH --tasks-per-node=2\n#SBATCH --exclusive\n#SBATCH --partition=debug\n#SBATCH --time=00:01:00\n\n\ncat $0\n\n#These should be loaded before doing a make:\nmodule load gcc  openmpi export OMP_NUM_THREADS=2\nsrun  -n 4 ./fhostone -F\nsrun  -n 4 ./phostone -F\n\n\n[nrmc2l@swift-login-1 ~ example]$ module load gcc  openmpi\n[nrmc2l@swift-login-1 ~ example]$ make\nmpif90 -fopenmp fhostone.f90 -o fhostone\nrm getit.mod  mympi.mod  numz.mod\nmpicc -fopenmp phostone.c -o phostone\n[nrmc2l@swift-login-1 ~ example]$ sbatch runopenmpi\nSubmitted batch job 187\n[nrmc2l@swift-login-1 ~ example]$ </code></pre>"},{"location":"Documentation/Systems/Swift/running/#results","title":"Results","text":"<pre><code>[nrmc2l@swift-login-1 example]$ cat *312985*\n#!/bin/bash\n#SBATCH --job-name=\"install\"\n#SBATCH --nodes=2\n#SBATCH --tasks-per-node=2\n#SBATCH --exclusive\n#SBATCH --partition=debug\n#SBATCH --time=00:01:00\n\n\ncat $0\n\n#These should be loaded before doing a make\nmodule load gcc  openmpi export OMP_NUM_THREADS=2\nsrun  -n 4 ./fhostone -F\nsrun  -n 4 ./phostone -F\n\nMPI Version:Open MPI v4.1.1, package: Open MPI nrmc2l@swift-login-1.swift.hpc.nrel.gov Distribution, ident: 4.1.1, repo rev: v4.1.1, Apr 24, 2021\ntask    thread             node name  first task    # on node  core\n0002      0000                 c1-31        0002         0000   018\n0000      0000                 c1-30        0000         0000   072\n0000      0001                 c1-30        0000         0000   095\n0001      0000                 c1-30        0000         0001   096\n0001      0001                 c1-30        0000         0001   099\n0002      0001                 c1-31        0002         0000   085\n0003      0000                 c1-31        0002         0001   063\n0003      0001                 c1-31        0002         0001   099\n0001      0000                 c1-30        0000         0001  0097\n0001      0001                 c1-30        0000         0001  0103\n0003      0000                 c1-31        0002         0001  0062\n0003      0001                 c1-31        0002         0001  0103\nMPI VERSION Open MPI v4.1.1, package: Open MPI nrmc2l@swift-login-1.swift.hpc.nrel.gov Distribution, ident: 4.1.1, repo rev: v4.1.1, Apr 24, 2021\ntask    thread             node name  first task    # on node  core\n0000      0000                 c1-30        0000         0000  0072\n0000      0001                 c1-30        0000         0000  0020\n0002      0000                 c1-31        0002         0000  0000\n0002      0001                 c1-31        0002         0000  0067\n[nrmc2l@swift-login-1 example]$ </code></pre>"},{"location":"Documentation/Systems/Swift/running/#building-with-intel-fortran-or-intel-c-and-openmpi","title":"Building with Intel Fortran or Intel C and OpenMPI","text":"<p>You can build parallel programs using OpenMPI and the Intel Fortran ifort and Intel C icc compilers.</p> <p>We have the example programs build with gnu compilers and OpenMP using the lines:</p> <pre><code>[nrmc2l@swift-login-1 ~ example]$ mpif90 -fopenmp fhostone.f90 -o fhostone\n[nrmc2l@swift-login-1 ~ example]$ mpicc -fopenmp phostone.c -o phostone\n</code></pre> <p>This gives us:</p> <p><pre><code>[nrmc2l@swift-login-1 ~ example]$ ls -l fhostone\n-rwxrwxr-x. 1 nrmc2l nrmc2l 36880 Jul 30 13:36 fhostone\n[nrmc2l@swift-login-1 ~ example]$ ls -l phostone\n-rwxrwxr-x. 1 nrmc2l nrmc2l 27536 Jul 30 13:36 phostone\n</code></pre> Note the size of the executable files.  </p> <p>If you want to use the Intel compilers, first load the appropriate modules:</p> <pre><code>module load intel-oneapi-mpi intel-oneapi-compilers gcc\n</code></pre> <p>Then we can set the variables OMPI_FC=ifort and OMPI_CC=icc, and recompile:</p> <pre><code>[nrmc2l@swift-login-1 ~ example]$ export OMPI_FC=ifort\n[nrmc2l@swift-login-1 ~ example]$ export OMPI_CC=icc\n[nrmc2l@swift-login-1 ~ example]$ mpif90 -fopenmp fhostone.f90 -o fhostone\n[nrmc2l@swift-login-1 ~ example]$ mpicc -fopenmp phostone.c -o phostone\n\n\n[nrmc2l@swift-login-1 ~ example]$ ls -lt fhostone\n-rwxrwxr-x. 1 nrmc2l nrmc2l 951448 Jul 30 13:37 fhostone\n[nrmc2l@swift-login-1 ~ example]$ ls -lt phostone\n-rwxrwxr-x. 1 nrmc2l nrmc2l 155856 Jul 30 13:37 phostone\n[nrmc2l@swift-login-1 ~ example]$ </code></pre> <p>Note the size of the executable files have changed. You can also see the difference by running the commands:</p> <pre><code>nm fhostone | grep intel | wc\nnm phostone | grep intel | wc\n</code></pre> <p>on the two versions of the program. It will show how many calls to Intel routines are in each, 51 and 36 compared to 0 for the gnu versions.</p>"},{"location":"Documentation/Systems/Swift/running/#building-and-running-with-intel-mpi","title":"Building and Running with Intel MPI","text":"<p>We can build with the Intel versions of MPI. We assume we will want to build with icc and ifort as the backend compilers. We load the modules:</p> <pre><code>ml gcc\nml intel-oneapi-compilers\nml intel-oneapi-mpi\n</code></pre> <p>Then, build and run the same example as above:</p> <pre><code>make clean\nmake PFC=mpiifort PCC=mpiicc </code></pre> <p>Giving us:</p> <pre><code>[nrmc2l@swift-login-1 example]$ ls -lt fhostone phostone\n-rwxrwxr-x. 1 nrmc2l hpcapps 155696 Aug  5 16:14 phostone\n-rwxrwxr-x. 1 nrmc2l hpcapps 947112 Aug  5 16:14 fhostone\n[nrmc2l@swift-login-1 example]$ </code></pre> <p>We need to make some changes to our batch script.  Replace the module load line with:</p> <pre><code>module load intel-oneapi-mpi intel-oneapi-compilers gcc\n</code></pre> <p>Launch with the srun command:</p> <pre><code>srun   ./a.out -F\n</code></pre> <p>Our IntelMPI batch script is:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=\"install\"\n#SBATCH --nodes=2\n#SBATCH --tasks-per-node=2\n#SBATCH --exclusive\n#SBATCH --partition=debug\n#SBATCH --time=00:01:00\n\n\ncat $0\n\n#These should be loaded before doing a make\nmodule load intel-oneapi-mpi intel-oneapi-compilers gcc\n\nexport OMP_NUM_THREADS=2\nsrun  -n 4 ./fhostone -F\nsrun  -n 4 ./phostone -F\n</code></pre> <p>Which produces the following output:</p> <pre><code>MPI Version:Intel(R) MPI Library 2021.3 for Linux* OS\n\ntask    thread             node name  first task    # on node  core\n0000      0000                 c1-32        0000         0000   127\n0000      0001                 c1-32        0000         0000   097\n0001      0000                 c1-32        0000         0001   062\n0001      0001                 c1-32        0000         0001   099\n\nMPI VERSION Intel(R) MPI Library 2021.3 for Linux* OS\n\ntask    thread             node name  first task    # on node  core\n0000      0000                 c1-32        0000         0000  0127\n0000      0001                 c1-32        0000         0000  0097\n0001      0000                 c1-32        0000         0001  0127\n0001      0001                 c1-32        0000         0001  0099\n</code></pre>"},{"location":"Documentation/Systems/Swift/running/#running-vasp","title":"Running VASP","text":"<p>The batch script given above can be modified to run VASP. To do so, load the VASP module, as well:</p> <pre><code>ml vasp\n</code></pre> <p>This will give you:</p> <pre><code>[nrmc2l@swift-login-1 ~ example]$ which vasp_gam\n/nopt/nrel/apps/210728a/level02/gcc-9.4.0/vasp-6.1.1/bin/vasp_gam\n[nrmc2l@swift-login-1 ~ example]$ which vasp_ncl\n/nopt/nrel/apps/210728a/level02/gcc-9.4.0/vasp-6.1.1/bin/vasp_ncl\n[nrmc2l@swift-login-1 ~ example]$ which vasp_std\n/nopt/nrel/apps/210728a/level02/gcc-9.4.0/vasp-6.1.1/bin/vasp_std\n[nrmc2l@swift-login-1 ~ example]$ </code></pre> <p>Note the directory might be different.</p> <p>Then you need to add calls in your script to set up / point do your data files. So your final script will look something like the following. Here we use data downloaded from NREL's benchmark repository:</p> <p><pre><code>#!/bin/bash\n#SBATCH --job-name=b2_4\n#SBATCH --nodes=1\n#SBATCH --time=4:00:00\n##SBATCH --error=std.err\n##SBATCH --output=std.out\n#SBATCH --partition=debug\n#SBATCH --exclusive\n\ncat $0\n\nhostname\n\nmodule purge\nml openmpi gcc vasp #### get input and set it up\n#### This is from an old benchmark test\n#### see https://github.nrel.gov/ESIF-Benchmarks/VASP/tree/master/bench2\n\nmkdir $SLURM_JOB_ID\ncp input/* $SLURM_JOB_ID\ncd $SLURM_JOB_ID\n\n\n\nsrun   -n 16 vasp_std &gt; vasp.$SLURM_JOB_ID\n</code></pre> This will run a version of Vasp built with openmpi and gfortran/gcc. You can run a version of Vasp built with the Intel toolchain replacing the ml line with the following module load:</p> <p><code>ml vaspintel intel-oneapi-mpi intel-oneapi-compilers intel-oneapi-mkl</code></p>"},{"location":"Documentation/Systems/Swift/running/#running-jupyter-jupyter-lab","title":"Running Jupyter / Jupyter-lab","text":"<p>Jupyter and Jupyter-lab are available by loading the module \"python\"</p> <pre><code>[nrmc2l@swift-login-1 ~]$ ml python\n[nrmc2l@swift-login-1 ~]$ which python\n/nopt/nrel/apps/210928a/level00/gcc-9.4.0/python-3.10.0/bin/python\n[nrmc2l@swift-login-1 ~]$ which jupyter\n/nopt/nrel/apps/210928a/level00/gcc-9.4.0/python-3.10.0/bin/jupyter\n[nrmc2l@swift-login-1 ~]$ which jupyter-lab\n/nopt/nrel/apps/210928a/level00/gcc-9.4.0/python-3.10.0/bin/jupyter-lab\n[nrmc2l@swift-login-1 ~]$ </code></pre> <p>It is recommended that you use the --no-browser option and connect to your notebook from your desktop using a ssh tunnel and web browser.</p> <p>On Swift enter the command below, and note the URLs in the output:  </p> <p><pre><code>[nrmc2l@swift-login-1 ~]$ jupyter-lab --no-browser\n[I 2022-03-30 07:54:25.937 ServerApp] jupyterlab | extension was successfully linked.\n[I 2022-03-30 07:54:26.224 ServerApp] nbclassic | extension was successfully linked.\n[I 2022-03-30 07:54:26.255 ServerApp] nbclassic | extension was successfully loaded.\n[I 2022-03-30 07:54:26.257 LabApp] JupyterLab extension loaded from /nopt/nrel/apps/210928a/level00/gcc-9.4.0/python-3.10.0/lib/python3.10/site-packages/jupyterlab\n[I 2022-03-30 07:54:26.257 LabApp] JupyterLab application directory is /nopt/nrel/apps/210928a/level00/gcc-9.4.0/python-3.10.0/share/jupyter/lab\n[I 2022-03-30 07:54:26.260 ServerApp] jupyterlab | extension was successfully loaded.\n[I 2022-03-30 07:54:26.261 ServerApp] Serving notebooks from local directory: /home/nrmc2l\n[I 2022-03-30 07:54:26.261 ServerApp] Jupyter Server 1.11.1 is running at:\n[I 2022-03-30 07:54:26.261 ServerApp] http://localhost:8888/lab?token=183d33c61bb136f8d04b83c70c4257a976060dd84afc9156\n[I 2022-03-30 07:54:26.261 ServerApp]  or http://127.0.0.1:8888/lab?token=183d33c61bb136f8d04b83c70c4257a976060dd84afc9156\n[I 2022-03-30 07:54:26.261 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n[C 2022-03-30 07:54:26.266 ServerApp] To access the server, open this file in a browser:\n        file:///home/nrmc2l/.local/share/jupyter/runtime/jpserver-2056000-open.html\n    Or copy and paste one of these URLs:\n        http://localhost:8888/lab?token=183d33c61bb136f8d04b83c70c4257a976060dd84afc9156\n     or http://127.0.0.1:8888/lab?token=183d33c61bb136f8d04b83c70c4257a976060dd84afc9156\n</code></pre> Note the 8888 in the URL it might be different. On your desktop in a new terminal window enter the command:</p> <pre><code>ssh -t -L 8888:localhost:8888 swift-login-1.hpc.nrel.gov\n</code></pre> <p>replacing 8888 with the number in the URL if it is different.</p> <p>Then in a web browser window, paste the URL to bring up a new notebook.</p>"},{"location":"Documentation/Systems/Swift/running/#running-jupyter-jupyter-lab-on-a-compute-node","title":"Running Jupyter / Jupyter-lab on a compute node","text":"<p>You can get an interactive session on a compute node with the salloc command, as in the following example:</p> <pre><code>[nrmc2l@swift-login-1 ~]$ salloc  --account=hpcapps   --exclusive    --time=01:00:00   --ntasks=16           --nodes=1 --partition=debug\n</code></pre> <p>but replacing hpcapps with your account. After you get a session on a node, <code>module load python</code> and run as shown above.</p> <pre><code>[nrmc2l@swift-login-1 ~]$ salloc  --account=hpcapps   --exclusive    --time=01:00:00   --ntasks=16           --nodes=1 --partition=debug\nsalloc: Pending job allocation 313001\nsalloc: job 313001 queued and waiting for resources\nsalloc: job 313001 has been allocated resources\nsalloc: Granted job allocation 313001\n[nrmc2l@c1-28 ~]$ [nrmc2l@c1-28 ~]$ module load python\n[nrmc2l@c1-28 ~]$ [nrmc2l@c1-28 ~]$ jupyter-lab --no-browser\n[I 2022-03-30 08:04:28.063 ServerApp] jupyterlab | extension was successfully linked.\n[I 2022-03-30 08:04:28.468 ServerApp] nbclassic | extension was successfully linked.\n[I 2022-03-30 08:04:28.508 ServerApp] nbclassic | extension was successfully loaded.\n[I 2022-03-30 08:04:28.509 LabApp] JupyterLab extension loaded from /nopt/nrel/apps/210928a/level00/gcc-9.4.0/python-3.10.0/lib/python3.10/site-packages/jupyterlab\n[I 2022-03-30 08:04:28.509 LabApp] JupyterLab application directory is /nopt/nrel/apps/210928a/level00/gcc-9.4.0/python-3.10.0/share/jupyter/lab\n[I 2022-03-30 08:04:28.513 ServerApp] jupyterlab | extension was successfully loaded.\n[I 2022-03-30 08:04:28.513 ServerApp] Serving notebooks from local directory: /home/nrmc2l\n[I 2022-03-30 08:04:28.514 ServerApp] Jupyter Server 1.11.1 is running at:\n[I 2022-03-30 08:04:28.514 ServerApp] http://localhost:8888/lab?token=cd101872959be54aea33082a8af350fc7e1484e47a9fdfbf\n[I 2022-03-30 08:04:28.514 ServerApp]  or http://127.0.0.1:8888/lab?token=cd101872959be54aea33082a8af350fc7e1484e47a9fdfbf\n[I 2022-03-30 08:04:28.514 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n[C 2022-03-30 08:04:28.519 ServerApp] To access the server, open this file in a browser:\n        file:///home/nrmc2l/.local/share/jupyter/runtime/jpserver-3375148-open.html\n    Or copy and paste one of these URLs:\n        http://localhost:8888/lab?token=cd101872959be54aea33082a8af350fc7e1484e47a9fdfbf\n     or http://127.0.0.1:8888/lab?token=cd101872959be54aea33082a8af350fc7e1484e47a9fdfbf\n</code></pre> <p>On your desktop run the command:</p> <pre><code>ssh -t -L 8888:localhost:8475 swift-login-1 ssh -L 8475:localhost:8888 c1-28\n</code></pre> <p>replacing 8888 with the value in the URL if needed and c1-28 with the name of the compute node on which you are running. Then again paste the URL in a web browser. You should get a notebook running on the compute node.</p>"},{"location":"Documentation/Systems/Swift/running/#running-julia","title":"Running Julia","text":"<p>Julia is also available via a module.  </p> <p><pre><code>[nrmc2l@swift-login-1:~ ] $ module spider julia\n...\n     Versions:\n        julia/1.6.2-ocsfign\n        julia/1.7.2-gdp7a25\n...\n[nrmc2l@swift-login-1:~ ] $ [nrmc2l@swift-login-1:~/examples/spack ] $ module load julia/1.7.2-gdp7a25 [nrmc2l@swift-login-1:~/examples/spack ] $ which julia\n/nopt/nrel/apps/210928a/level03/install/linux-rocky8-zen2/gcc-9.4.0/julia-1.7.2-gdp7a253nsglyzssybqknos2n5amkvqm/bin/julia\n[nrmc2l@swift-login-1:~/examples/spack ] $ </code></pre> Julia can be run in a Jupyter notebook as discussed above. However, before doing so you will need to run the following commands in each Julia version you are using:  </p> <pre><code>julia&gt; using Pkg\njulia&gt; Pkg.add(\"IJulia\")\n</code></pre> <p>Please see https://datatofish.com/add-julia-to-jupyter/ for more information.</p> <p>If you would like to install your own copy of Julia complete with Jupyter-lab, contact Tim Kaiser tkaiser2@nrel.gov for a script to do so.</p>"},{"location":"Documentation/Systems/Vermilion/","title":"Vermilion","text":"<p>Vermilion is an OpenHPC-based cluster running on Dual AMD EPYC 7532 Rome CPUs. The nodes run as virtual machines in a local virtual private cloud (OpenStack). Mellanox drivers and OFED are installed on all nodes.</p>"},{"location":"Documentation/Systems/Vermilion/#collaboration-help","title":"Collaboration / Help","text":"<p>You can get help with Vermilion via email at HPC-Help@nrel.gov or live chat.</p> <p>Live chat: Users with access to NREL's Teams chat can collaborate via the Vermilion Users Teams room.</p> <p>HPC-Help: For specific questions about work you're running on Vermilion, send email to HPC-Help@nrel.gov and specify vermilion on the subject line.</p>"},{"location":"Documentation/Systems/Vermilion/#connecting-to-vermilion","title":"Connecting to Vermilion","text":"<p>To access vermilion, log into the NREL network and connect via ssh:</p> <pre><code>ssh vs.hpc.nrel.gov\nssh vermilion.hpc.nrel.gov\n</code></pre> <p>There are currently two login nodes. They share the same home directory so work done on one will appear on the other. They are:</p> <pre><code>vs-login-1\nvs-login-2\n</code></pre> <p>You may connect directly to a login node, but they may be cycled in and out of the pool. If a node is unavailable, try connecting to another login node or the <code>vs.hpc.nrel.gov</code> round-robin option.</p>"},{"location":"Documentation/Systems/Vermilion/#building-code","title":"Building code","text":"<p>Don't build or run code on a login node. Login nodes have limited CPU and memory available. Use a compute or GPU node instead. Simply start an interactive job on an appropriately provisioned node and partition for your work and do your builds there. Similarly, build your projects under <code>/projects/your_project_name/</code> as home directories are limited to 5GB per user.</p>"},{"location":"Documentation/Systems/Vermilion/applications/","title":"Applications","text":"<p>The Vermilion HPC cluster marries traditional HPC deployments and modern cloud architectures, both using the OpenHPC infrastructure, and spack. https://spack.io . </p> <p>There are a few packages installed using the OpenHPC infrastructure.  These can be found  in /opt/ohpc/pub/.  These are not in your path by default.  Some can be loaded via the module load command.  Running the command module avail you will see which of the packages can be loaded under the heading /opt/ohpc/pub/modulefiles.  </p> <p>However, there ary many additional modules that can be made available.  Instructions for enabling additional modules, Information about partitions, and running on Vermilion can be found in the documents Modules and Running.</p> <p>The page Modules discuses how to activate and use the modules on Vermilion. Modules are not available by default and must be activated.  Please see the Modules page for more information about setting up your environment and loading modules. </p> <p>The page Running describes running on Vermilion in more detail including a description of the hardware, partitions, simple build and run scripts and launching Vasp.</p>"},{"location":"Documentation/Systems/Vermilion/modules/","title":"Modules","text":"<p>The page Running describes running on Vermilion in more detail including a description of the hardware, partitions, simple build and run scripts and launching Vasp.</p>"},{"location":"Documentation/Systems/Vermilion/modules/#vermilion-modules-and-applications","title":"Vermilion Modules and Applications","text":"<p>This page describes how to activate and use the modules available on Vermilion. Modules are not available by default on the machine.  This page discusses how to enable them.</p>"},{"location":"Documentation/Systems/Vermilion/modules/#selecting-a-user-environment","title":"Selecting a user Environment","text":"<p>Environments are provided with a number of commonly used modules including compilers, common build tools, optimized libraries, and some analysis tools. </p> <p>Since Vermilion is a new machine with an unusual architecture we are experimenting with environments. The environments are defined in date stamped subdirectories under the directory /nopt/nrel/apps.  Some of the environments in this directory are experimental and not intended for general use.   </p> <p>User environments have a file myenv.* in the date stamped directory.  These are for general use.  If a directory does not have a myenv.* file then it is experimental, old, or not yet complete.  </p> <p>The current user environments can be found by going to the directory /nopt/nrel/apps and looking for  myenv.* in sub directories.  For example</p> <pre><code>[joeuser2@vs-login-1 apps]$ ls -1 `pwd`/*/myenv*\n/nopt/nrel/apps/210729a/myenv.2107300124\n/nopt/nrel/apps/210901a/myenv.2109020548\n/nopt/nrel/apps/210929a/myenv.2110041605\n/nopt/nrel/apps/220525b/myenv.2110041605\n[joeuser2@vs-login-1 apps]$ \n</code></pre> <ul> <li>210729a<ul> <li>A bit dated but still should work.</li> </ul> </li> <li>210901a<ul> <li>A bit dated but still should work.</li> </ul> </li> <li>210929a<ul> <li>This is the recommended user environment.</li> </ul> </li> <li>220525b<ul> <li>Has some newer versions of compilers and other packages such as python 3.10.2 &amp; gcc 12.1.</li> </ul> </li> </ul> <p>Currently, none of these environments are loaded by default for users.  Users must source one of the  /nopt/nrel/apps/210929a/myenv.* files to enable an environment.  </p> <p>The recommended environment is enabled by running the source command:</p> <pre><code>source /nopt/nrel/apps/210929a/myenv.2110041605\n</code></pre> <p>NOTE:  You may want to add this line to your .bashrc file so modules are available at login.</p> <p>After sourcing this file you will have access to a set of modules. These can be listed using the following command:</p> <pre><code>module avail \n</code></pre> <p>If you want to build applications you can then \"module load\" compilers and the like; for example</p> <pre><code>[joeuser2@vs-login-1 apps]$ ml gcc\n[joeuser2@vs-login-1 apps]$ ml openmpi\n</code></pre> <p>will load gnu 9.4 and openmpi.  This will give you access to gcc, gfortran, mpicc, mpif90 and related commands.</p> <p>You can load the Intel compilers (icc,icpc, ifort, mpiicc, mpiifort...) with the following commands.  Note you should also load gcc when using the Intel compilers because the Intel compilers actually use some gcc libraries.)</p> <pre><code>[joeuser2@vs-login-1 apps]$ ml intel-oneapi-compilers\n[joeuser2@vs-login-1 apps]$ ml intel-oneapi-mpi\n[joeuser2@vs-login-1 apps]$ ml gcc\n[joeuser2@vs-login-1 apps]$ \n</code></pre> <p>The python in this environment is very up to date, version 3.10.0.  It also contains many important packages including: numpy, scypi, matplotlib, pandas, jupyter, and jupyter-lab.</p>"},{"location":"Documentation/Systems/Vermilion/modules/#examples","title":"Examples","text":"<p>The directory /nopt/nrel/apps/210929a/example contains some simple build and run scripts.  The directory /nopt/nrel/apps/210929a/example/vasp contains information about running Vasp.  These are discussed in more detail in the page Running.</p>"},{"location":"Documentation/Systems/Vermilion/running/","title":"Running on Vermilion","text":"<p>The page Modules discuses how to activate and use the modules on Vermilion. Modules are not available by default and must be activated.  Please see the Modules page for more information about setting up your environment and loading modules. </p>"},{"location":"Documentation/Systems/Vermilion/running/#running-on-vermilion","title":"Running on Vermilion","text":"<p>This page discusses the compute nodes, partitions and gives some examples of building and running applications including running Vasp.</p>"},{"location":"Documentation/Systems/Vermilion/running/#compute-hosts","title":"Compute hosts","text":"<p>Vermilion is a collection of physical nodes with each regular node containing Dual AMD EPYC 7532 Rome CPUs.  However, each node is virtualized.  That is it is split up into virtual nodes with each virtual node having a portion of the cores and memory of the physical node.  Similar virtual nodes are then assigned slurm partitions as shown below.  </p>"},{"location":"Documentation/Systems/Vermilion/running/#shared-file-systems","title":"Shared file systems","text":"<p>Vermilion's home directories are shared across all nodes.  There is also /scratch/$USER and /projects spaces seen across all nodes.</p>"},{"location":"Documentation/Systems/Vermilion/running/#partitions","title":"Partitions","text":"<p>Partitions are flexible and fluid on Vermilion.  A list of partitions can be found by running the <code>sinfo</code> command.  Here are the partitions as of 10/20/2022.</p> Partition Name Qty RAM Cores/node /var/scratch 1K-blocks gpu1 x NVIDIA Tesla A100 17 114 GB 30 6,240,805,336 lg 39 229 GB 60 1,031,070,000 std 60 114 GB 30 515,010,816 sm 28 61 GB 16 256,981,000 t 15 16 GB 4 61,665,000"},{"location":"Documentation/Systems/Vermilion/running/#operating-software","title":"Operating Software","text":"<p>The Vermilion HPC cluster runs fairly current versions of OpenHPC and SLURM on top of OpenStack.</p>"},{"location":"Documentation/Systems/Vermilion/running/#example","title":"Example","text":"<p>Environments are provided with a number of commonly used compilers, common build tools, specific optimized libraries, and some analysis tools. Environments must be enabled before modules can be seen.  This is discussed in detail on the page Modules</p> <p>You can use the \"standard\" environment by running the command:</p> <pre><code>source /nopt/nrel/apps/210929a/myenv.2110041605\n</code></pre> <p>The examples on this page uses the environment enabled by this command.   You may want to add this command to your .bashrc file so you have a useful environment when you login.  </p> <p>In the directory /nopt/nrel/apps/210929a you will see a subdirectory example.  This contains a makefile for a simple hello world program written in both Fortran and C and several run scripts. The README.md file contains additional information, some of which is replicated here. </p> <p>It is suggested you copy the directory to run the examples:</p> <pre><code>cp -r /nopt/nrel/apps/210929a/example ~/example\ncd ~/example\n</code></pre>"},{"location":"Documentation/Systems/Vermilion/running/#simple-batch-script","title":"Simple batch script","text":"<p>Here is a sample batch script, runopenmpi, for running the hello world examples .  NOTE: You must build the applications before running this script.   Please see Building hello world first below.</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=\"install\"\n#SBATCH --nodes=2\n#SBATCH --exclusive\n#SBATCH --partition=t\n#SBATCH --time=00:01:00\n\ncat $0\n\nsource /nopt/nrel/apps/210929a/myenv*\nml gcc   openmpi \n\nexport OMP_NUM_THREADS=2\nsrun --mpi=pmi2 -n 2 ./fhostone -F\nsrun --mpi=pmi2 -n 2 ./phostone -F\n</code></pre> <p>The submission command is:</p> <pre><code>sbatch --partition=sm --account=MY_HPC_ACCOUNT runopenmpi\n</code></pre> <p>where MY_HPC_ACCOUNT is your account. </p>"},{"location":"Documentation/Systems/Vermilion/running/#building-hello-world-first","title":"Building hello world first","text":"<p>For the script given above to work you must first build the application.  You need to:</p> <ol> <li>Load the environment</li> <li>Load the modules</li> <li>make</li> </ol>"},{"location":"Documentation/Systems/Vermilion/running/#loading-the-environment","title":"Loading the environment","text":"<p>Loading the environment is just a matter of sourcing the file</p> <pre><code>source /nopt/nrel/apps/210929a/myenv.2110041605\n</code></pre>"},{"location":"Documentation/Systems/Vermilion/running/#loading-the-modules","title":"Loading the modules.","text":"<p>We are going to use gnu compilers with OpenMPI.</p> <pre><code>module load gcc \nmodule load openmpi\n</code></pre>"},{"location":"Documentation/Systems/Vermilion/running/#run-make","title":"Run make","text":"<pre><code>make\n</code></pre>"},{"location":"Documentation/Systems/Vermilion/running/#full-procedure-screen-dump","title":"Full procedure screen dump","text":"<pre><code>[joeuser@vs-login-1 ~]$ cp -r /nopt/nrel/apps/210929a/example ~/example\n[joeuser@vs-login-1 ~]$ cd example/\n[joeuser@vs-login-1 example]$ source /nopt/nrel/apps/210929a/myenv.2110041605\n[joeuser@vs-login-1 example]$ module load gcc\n[joeuser@vs-login-1 example]$ module load openmpi\n[joeuser@vs-login-1 example]$ make\nmpif90 -Wno-argument-mismatch -g -fopenmp fhostone.f90  -o fhostone \nrm getit.mod  mympi.mod  numz.mod\nmpicc -g -fopenmp phostone.c -o phostone\n[joeuser@vs-login-1 example]$ cat runopenmpi \n#!/bin/bash\n#SBATCH --job-name=\"install\"\n#SBATCH --nodes=2\n#SBATCH --exclusive\n#SBATCH --partition=t\n#SBATCH --time=00:01:00\n\ncat $0\n\nsource /nopt/nrel/apps/210929a/myenv*\nml gcc   openmpi \n\nexport OMP_NUM_THREADS=2\nsrun --mpi=pmi2 -n 2 ./fhostone -F\nsrun --mpi=pmi2 -n 2 ./phostone -F\n\n\n[joeuser@vs-login-1 example]$ sbatch --account=MY_HPC_ACCOUNT runopenmpi \nSubmitted batch job 50031771\n[joeuser@vs-login-1 example]$ \n</code></pre>"},{"location":"Documentation/Systems/Vermilion/running/#results","title":"Results","text":"<pre><code>[joeuser@vs example]$ cat slurm-187.out\n[joeuser@vs-login-1 example]$ cat slurm-50031771.out\n#!/bin/bash\n#SBATCH --job-name=\"install\"\n#SBATCH --nodes=2\n#SBATCH --exclusive\n#SBATCH --partition=t\n#SBATCH --time=00:01:00\n\ncat $0\n\nsource /nopt/nrel/apps/210929a/myenv*\nml gcc   openmpi \n\nexport OMP_NUM_THREADS=2\nsrun --mpi=pmi2 -n 2 ./fhostone -F\nsrun --mpi=pmi2 -n 2 ./phostone -F\n\nSRUN --mpi=pmi2 -n 2 ./fhostone -F\n\nMPI Version:Open MPI v4.1.1, package: Open MPI joeuser@vs-sm-0001 Distribution, ident: 4.1.1, repo rev: v4.1.1, Apr 24, 2021\ntask    thread             node name  first task    # on node  core\n0000      0000    vs-t-0012.vs.hpc.n        0000         0000   002\n0000      0001    vs-t-0012.vs.hpc.n        0000         0000   003\n0001      0000    vs-t-0013.vs.hpc.n        0001         0000   003\n0001      0001    vs-t-0013.vs.hpc.n        0001         0000   002\nSRUN --mpi=pmi2 -n 2 ./phostone -F\n\nMPI VERSION Open MPI v4.1.1, package: Open MPI joeuser@vs-sm-0001 Distribution, ident: 4.1.1, repo rev: v4.1.1, Apr 24, 2021\ntask    thread             node name  first task    # on node  core\n0000      0000    vs-t-0012.vs.hpc.nrel.gov        0000         0000  0003\n0000      0001    vs-t-0012.vs.hpc.nrel.gov        0000         0000  0002\n0001      0000    vs-t-0013.vs.hpc.nrel.gov        0001         0000  0003\n0001      0001    vs-t-0013.vs.hpc.nrel.gov        0001         0000  0000\n[joeuser@vs-login-1 example]$ \n</code></pre> <p>Many programs can be built/run with OpenMPI and with icc/ifort as the backend compilers or built/run with the Intel version of MPI with either gcc/gfortran or icc/ifort as the backend compilers.  These options are discussed below.</p>"},{"location":"Documentation/Systems/Vermilion/running/#building-with-intel-fortran-or-intel-c-and-openmpi","title":"Building with Intel Fortran or Intel C and OpenMPI","text":"<p>You can build parallel programs using OpenMPI and the Intel Fortran ifort and Intel C icc compilers.</p> <p>If you want to use the Intel compilers you first do a module load.</p> <pre><code>ml intel-oneapi-compilers\n</code></pre> <p>Then we can set the variables OMPI_FC=ifort and OMPI_CC=icc.  Then recompile.</p> <pre><code>[joeuser@vs example]$ export OMPI_FC=ifort\n[joeuser@vs example]$ export OMPI_CC=icc\n[joeuser@vs example]$ mpif90 -fopenmp fhostone.f90 -o fhostone\n[joeuser@vs example]$ mpicc -fopenmp phostone.c -o phostone\n</code></pre> <p>If you do a ls -l on the executable files you will note the size of the files change with different compiler versions.  You can also see the difference by running the commands</p> <pre><code>nm fhostone | grep intel | wc\nnm phostone | grep intel | wc\n</code></pre> <p>on the two versions of the program.  It will show how many calls to Intel routines are in each, 51 and 36 compared to 0 for the gnu versions.</p>"},{"location":"Documentation/Systems/Vermilion/running/#building-and-running-with-intel-mpi","title":"Building and Running with Intel MPI","text":"<p>We can build with the Intel versions of MPI and with icc and ifort as the backend compilers.  We load the modules:</p> <pre><code>ml gcc\nml intel-oneapi-compilers\nml intel-oneapi-mpi\n</code></pre> <p>Then, building and running the same example as above:</p> <pre><code>make clean\nmake PFC=mpiifort PCC=mpiicc\n</code></pre> <p>The actual compile lines produced by make are:</p> <pre><code>mpiifort -g -fopenmp fhostone.f90  -o fhostone \nmpiicc   -g -fopenmp phostone.c    -o phostone\n</code></pre> <p>For running, we need to make some changes to our batch script.  Replace the load of openmpi with:</p> <pre><code>ml intel-oneapi-compilers\nml intel-oneapi-mpi\n</code></pre> <p>Launch with the srun command:</p> <pre><code>srun --mpi=pmi2  ./a.out -F\n</code></pre> <p>Our IntelMPI batch script is:</p> <pre><code>[joeuser@vs-login-1 example]$ cat runintel \n#!/bin/bash\n#SBATCH --job-name=\"install\"\n#SBATCH --nodes=2\n#SBATCH --exclusive\n#SBATCH --partition=lg\n#SBATCH --time=00:01:00\n\ncat $0\n\nsource /nopt/nrel/apps/210929a/myenv*\nml intel-oneapi-mpi intel-oneapi-compilers gcc\n\nexport OMP_NUM_THREADS=2\nsrun --mpi=pmi2 -n 2 ./fhostone -F\nsrun --mpi=pmi2 -n 2 ./phostone -F\n</code></pre> <p>With output</p> <pre><code>MPI Version:Intel(R) MPI Library 2021.3 for Linux* OS\n\ntask    thread             node name  first task    # on node  core\n0000      0000                 c1-32        0000         0000   127\n0000      0001                 c1-32        0000         0000   097\n0001      0000                 c1-32        0000         0001   062\n0001      0001                 c1-32        0000         0001   099\n\nMPI VERSION Intel(R) MPI Library 2021.3 for Linux* OS\n\ntask    thread             node name  first task    # on node  core\n0000      0000                 c1-32        0000         0000  0127\n0000      0001                 c1-32        0000         0000  0097\n0001      0000                 c1-32        0000         0001  0127\n0001      0001                 c1-32        0000         0001  0099\n</code></pre>"},{"location":"Documentation/Systems/Vermilion/running/#linking-intels-mkl-library","title":"Linking Intel's MKL library.","text":"<p>The environment defined by sourcing the file /nopt/nrel/apps/210929a/myenv.2110041605 enables loading of many other modules, including one for Intel's MKL  library. Then to build against MKL using the Intel compilers icc or ifort you normally just need to add the flag -mkl.</p> <p>There are examples in the directory /nopt/nrel/apps/210929a/example/mkl. There is a Readme.md file that explains in a bit more detail.</p> <p>Assuming you copied the example directory to you home directory the mkl examples will be in ~example/mkl</p> <p>The short version is that you can:</p> <p><pre><code>[joeuser@vs-login-1 mkl]$ cd ~/example/mkl\n[joeuser@vs-login-1 mkl]$ source /nopt/nrel/apps/210929a/myenv.2110041605\n[joeuser@vs-login-1 mkl]$ module purge\n[joeuser@vs-login-1 mkl]$ module load intel-oneapi-compilers\n[joeuser@vs-login-1 mkl]$ module load intel-oneapi-mkl\n[joeuser@vs-login-1 mkl]$ module load gcc\n\n[joeuser@vs-login-1 mkl]$ icc   -O3 -o mklc mkl.c   -mkl\n[joeuser@vs-login-1 mkl]$ ifort -O3 -o mklf mkl.f90 -mkl\n</code></pre> or to build and run the examples using make instead directly calling icc and ifort you can:</p> <pre><code>make run\n</code></pre>"},{"location":"Documentation/Systems/Vermilion/running/#running-vasp-on-vermilion","title":"Running VASP on Vermilion","text":"<p>The batch script given above can be modified to run VASP.</p> <p>There are actually several builds of Vasp on Vermilion, including builds of VASP 5 and VASP 6.  There are scripts for running them in the directory /nopt/nrel/apps/210929a/example/vasp.  Some of these version use different environments from the one discussed above.  The script example/vasp/runvasp_4 will runs the a GPU enabled version of Vasp on 4 Vermilion GPU nodes.  This version of vasp needs to be launched using mpirun instead of srun.  </p> <p>The run times and additional information can be found in the file /nopt/nrel/apps/210929a/example/vasp/versions.  The run on the GPU nodes is considerably faster than the CPU node runs.  </p> <p>The data set for these runs is from a standard NREL vasp benchmark. See https://github.nrel.gov/ESIF-Benchmarks/VASP/tree/master/bench2 This is a system of 519 atoms (Ag504C4H10S1).</p> <p>There is a NREL report that discuss running the this test case and also a smaller test case with with various setting of nodes, tasks-per-nodes and OMP_NUM_THREADS.  It can be found at: https://github.com/NREL/HPC/tree/master/applications/vasp/Performance%20Study%202</p>"},{"location":"Documentation/Systems/Vermilion/running/#running-multi-node-vasp-jobs-on-vermilion","title":"Running multi-node VASP jobs on Vermilion","text":"<p>VASP runs faster on 1 node than on 2 nodes. In some cases, VASP runtimes on 2 nodes have been observed to be double (or more) the run times on a single node. Many issues have been reported for running VASP on multiple nodes, especially when requesting all available cores in each node. In order for MPI to work reliably on Vermilion, it is necessary to specify the interconnect network that Vermilion should use to communicate between nodes. This is documented in each of the scripts below. Different solutions exists for Open MPI and Intel MPI. The documented recommendations for setting the interconnect network have been shown to work well for multi-node jobs on 2 nodes, but aren't guaranteed to produce succesful multi-node runs on 4 nodes. </p> <p>If many cores are needed for your VASP calcualtion, it is recommended to run VASP on a singe node in the lg partition (60 cores/node), which provides the largest numbers of cores per node. </p>"},{"location":"Documentation/Systems/Vermilion/running/#setting-up-vasp-sbatch-scripts","title":"Setting up VASP sbatch scripts","text":"<p>The following sections walk through building sbatch scripts for running VASP on Vermilion, including explanations of necessary tweaks to run multi-node jobs reliably. </p> <ul> <li>VASP 5 (Intel MPI)</li> <li>VASP 6 (Intel MPI)</li> <li>VASP 6 (Open MPI)</li> <li>VASP 6 on GPUs</li> </ul>"},{"location":"Documentation/Systems/Vermilion/running/#running-vasp-5-with-intelmpi-on-cpus","title":"Running VASP 5 with IntelMPI on CPUs","text":"<p>To load a build of VASP 5 that is compatible with Intel MPI (and other necessary modules):</p> <pre><code>module use  /nopt/nrel/apps/220525b/level01/modules/lmod/linux-rocky8-x86_64/gcc/12.1.0/\nml vasp/5.5.4\nml intel-oneapi-mkl\nml intel-oneapi-compilers\nml intel-oneapi-mpi\n</code></pre> <p>This will give you:</p> <pre><code>[myuser@vs example]$ which vasp_gam\n/nopt/nrel/apps/220525b/level01/install/opt/spack/linux-rocky8-zen2/gcc-12.1.0/vasp544/bin/vasp_gam\n[myuser@vs example]$ which vasp_ncl\n/nopt/nrel/apps/220525b/level01/install/opt/spack/linux-rocky8-zen2/gcc-12.1.0/vasp544/bin/vasp_ncl\n[myuser@vs example]$ which vasp_std\n/nopt/nrel/apps/220525b/level01/install/opt/spack/linux-rocky8-zen2/gcc-12.1.0/vasp544/bin/vasp_std\n</code></pre> <p>Note the directory might be different. </p> <p>In order to run on more than one node, we need to specify the network interconnect. To do so, use mpirun instead of srun. We want to use \"ens7\" as the interconnect. The mpirun command looks like this. </p> <pre><code>I_MPI_OFI_PROVIDER=tcp mpirun -iface ens7 -np 16 vasp_std\n</code></pre> <p>For VASP calculations on a single node, srun is sufficient. However, srun and mpirun produce similar run times. To run with srun for single node calculations, use the following line.</p> <pre><code>srun -n 16 vasp_std\n</code></pre> <p>Then you need to add calls in your script to set up and point to your data files.  So your final script will look something like the following. Here we download data from NREL's benchmark repository.</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=vasp\n#SBATCH --nodes=1\n#SBATCH --time=8:00:00\n##SBATCH --error=std.err\n##SBATCH --output=std.out\n#SBATCH --partition=sm\n#SBATCH --exclusive\n\ncat $0\n\nhostname\n\nsource /nopt/nrel/apps/210929a/myenv.2110041605\n\nmodule purge\nmodule use  /nopt/nrel/apps/220525b/level01/modules/lmod/linux-rocky8-x86_64/gcc/12.1.0/\nml vasp/5.5.4\nml intel-oneapi-mkl\nml intel-oneapi-compilers\nml intel-oneapi-mpi\n\n# some extra lines that have been shown to improve VASP reliability on Vermilion\nulimit -s unlimited\nexport UCX_TLS=tcp,self\nexport OMP_NUM_THREADS=1\n\n#### wget is needed to download data\nml wget\n\n#### get input and set it up\n#### This is from an old benchmark test\n#### see https://github.nrel.gov/ESIF-Benchmarks/VASP/tree/master/bench2\n\n\nmkdir input\n\nwget https://github.nrel.gov/raw/ESIF-Benchmarks/VASP/master/bench2/input/INCAR?token=AAAALJZRV4QFFTS7RC6LLGLBBV67M   -q -O INCAR\nwget https://github.nrel.gov/raw/ESIF-Benchmarks/VASP/master/bench2/input/POTCAR?token=AAAALJ6E7KHVTGWQMR4RKYTBBV7SC  -q -O POTCAR\nwget https://github.nrel.gov/raw/ESIF-Benchmarks/VASP/master/bench2/input/POSCAR?token=AAAALJ5WKM2QKC3D44SXIQTBBV7P2  -q -O POSCAR\nwget https://github.nrel.gov/raw/ESIF-Benchmarks/VASP/master/bench2/input/KPOINTS?token=AAAALJ5YTSCJFDHUUZMZY63BBV7NU -q -O KPOINTS\n\n# mpirun is recommended (necessary for multi-node calculations)\nI_MPI_OFI_PROVIDER=tcp mpirun -iface ens7 -np 16 vasp_std\n\n# srun can be used instead of mpirun for sinlge node calculations\n# srun -n 16 vasp_std\n</code></pre>"},{"location":"Documentation/Systems/Vermilion/running/#running-vasp-6-with-intelmpi-on-cpus","title":"Running VASP 6 with IntelMPI on CPUs","text":"<p>To load a build of VASP 6 that is compatible with Intel MPI (and other necessary modules):</p> <pre><code>source /nopt/nrel/apps/210929a/myenv.2110041605 \nml vaspintel\nml intel-oneapi-mkl\nml intel-oneapi-compilers\nml intel-oneapi-mpi\n</code></pre> <p>This will give you:</p> <pre><code>[myuser@vs example]$ which vasp_gam\n/nopt/nrel/apps/210929a/level01/linux-centos8-zen2/gcc-9.4.0/vaspintel-1.0-dwljo4wr6xcrgxqaq7pz35yqfxdxxsq4/bin/vasp_gam\n[myuser@vs example]$ which vasp_ncl\n/nopt/nrel/apps/210929a/level01/linux-centos8-zen2/gcc-9.4.0/vaspintel-1.0-dwljo4wr6xcrgxqaq7pz35yqfxdxxsq4/bin/vasp_ncl\n[myuser@vs example]$ which vasp_std\n/nopt/nrel/apps/210929a/level01/linux-centos8-zen2/gcc-9.4.0/vaspintel-1.0-dwljo4wr6xcrgxqaq7pz35yqfxdxxsq4/bin/vasp_std\n</code></pre> <p>Note the directory might be different. </p> <p>In order to run on more than one node, we need to specify the network interconnect. To do so, use mpirun instead of srun. We want to use \"ens7\" as the interconnect. The mpirun command looks like this. </p> <pre><code>I_MPI_OFI_PROVIDER=tcp mpirun -iface ens7 -np 16 vasp_std\n</code></pre> <p>For VASP calculations on a single node, srun is sufficient. However, srun and mpirun produce similar run times. To run with srun for single node calculations, use the following line.</p> <pre><code>srun -n 16 vasp_std\n</code></pre> <p>Then you need to add calls in your script to set up and point to your data files.  So your final script will look something like the following. Here we download data from NREL's benchmark repository.</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=vasp\n#SBATCH --nodes=1\n#SBATCH --time=8:00:00\n##SBATCH --error=std.err\n##SBATCH --output=std.out\n#SBATCH --partition=sm\n#SBATCH --exclusive\n\ncat $0\n\nhostname\n\nsource /nopt/nrel/apps/210929a/myenv.2110041605\n\nmodule purge\nsource /nopt/nrel/apps/210929a/myenv.2110041605 \nml intel-oneapi-mkl\nml intel-oneapi-compilers\nml intel-oneapi-mpi\nml vaspintel\n\n# some extra lines that have been shown to improve VASP reliability on Vermilion\nulimit -s unlimited\nexport UCX_TLS=tcp,self\nexport OMP_NUM_THREADS=1\n\n#### wget is needed to download data\nml wget\n\n#### get input and set it up\n#### This is from an old benchmark test\n#### see https://github.nrel.gov/ESIF-Benchmarks/VASP/tree/master/bench2\n\n\nmkdir input\n\nwget https://github.nrel.gov/raw/ESIF-Benchmarks/VASP/master/bench2/input/INCAR?token=AAAALJZRV4QFFTS7RC6LLGLBBV67M   -q -O INCAR\nwget https://github.nrel.gov/raw/ESIF-Benchmarks/VASP/master/bench2/input/POTCAR?token=AAAALJ6E7KHVTGWQMR4RKYTBBV7SC  -q -O POTCAR\nwget https://github.nrel.gov/raw/ESIF-Benchmarks/VASP/master/bench2/input/POSCAR?token=AAAALJ5WKM2QKC3D44SXIQTBBV7P2  -q -O POSCAR\nwget https://github.nrel.gov/raw/ESIF-Benchmarks/VASP/master/bench2/input/KPOINTS?token=AAAALJ5YTSCJFDHUUZMZY63BBV7NU -q -O KPOINTS\n\n# mpirun is recommended (necessary for multi-node calculations)\nI_MPI_OFI_PROVIDER=tcp mpirun -iface ens7 -np 16 vasp_std\n\n# srun can be used instead of mpirun for sinlge node calculations\n# srun -n 16 vasp_std\n</code></pre>"},{"location":"Documentation/Systems/Vermilion/running/#running-vasp-6-with-openmpi-on-cpus","title":"Running VASP 6 with OpenMPI on CPUs","text":"<p>To load a build of VASP 6 that is compatible with Open MPI:</p> <pre><code>source /nopt/nrel/apps/210929a/myenv.2110041605\nml vasp\n</code></pre> <p>This will give you:</p> <pre><code>[myuser@vs example]$ which vasp_gam\n/nopt/nrel/apps/123456a/level02/gcc-9.4.0/vasp-6.1.1/bin/vasp_gam\n[myuser@vs example]$ which vasp_ncl\n/nopt/nrel/apps/123456a/level02/gcc-9.4.0/vasp-6.1.1/bin/vasp_ncl\n[myuser@vs example]$ which vasp_std\n/nopt/nrel/apps/123456a/level02/gcc-9.4.0/vasp-6.1.1/bin/vasp_std\n</code></pre> <p>Note the directory might be different. </p> <p>In order to specify the network interconnect, we need to set the OMPI_MCA_param variable. We want to use \"ens7\" as the interconnect.</p> <pre><code>module use /nopt/nrel/apps/220525b/level01/modules/lmod/linux-rocky8-x86_64/gcc/12.1.0\nmodule load openmpi\nOMPI_MCA_param=\"btl_tcp_if_include ens7\"\n</code></pre> <p>Then you need to add calls in your script to set up and point to your data files.  So your final script will look something like the following. Here we download data from NREL's benchmark repository.</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=vasp\n#SBATCH --nodes=1\n#SBATCH --time=8:00:00\n##SBATCH --error=std.err\n##SBATCH --output=std.out\n#SBATCH --partition=sm\n#SBATCH --exclusive\n\ncat $0\n\nhostname\n\nsource /nopt/nrel/apps/210929a/myenv.2110041605\n\nmodule purge\nml gcc\nml vasp\n\n# some extra lines that have been shown to improve VASP reliability on Vermilion\nulimit -s unlimited\nexport UCX_TLS=tcp,self\nexport OMP_NUM_THREADS=1\n\n# lines to set \"ens7\" as the interconnect network\nmodule use /nopt/nrel/apps/220525b/level01/modules/lmod/linux-rocky8-x86_64/gcc/12.1.0\nmodule load openmpi\nOMPI_MCA_param=\"btl_tcp_if_include ens7\"\n\n#### wget is needed to download data\nml wget\n\n#### get input and set it up\n#### This is from an old benchmark test\n#### see https://github.nrel.gov/ESIF-Benchmarks/VASP/tree/master/bench2\n\n\nmkdir input\n\nwget https://github.nrel.gov/raw/ESIF-Benchmarks/VASP/master/bench2/input/INCAR?token=AAAALJZRV4QFFTS7RC6LLGLBBV67M   -q -O INCAR\nwget https://github.nrel.gov/raw/ESIF-Benchmarks/VASP/master/bench2/input/POTCAR?token=AAAALJ6E7KHVTGWQMR4RKYTBBV7SC  -q -O POTCAR\nwget https://github.nrel.gov/raw/ESIF-Benchmarks/VASP/master/bench2/input/POSCAR?token=AAAALJ5WKM2QKC3D44SXIQTBBV7P2  -q -O POSCAR\nwget https://github.nrel.gov/raw/ESIF-Benchmarks/VASP/master/bench2/input/KPOINTS?token=AAAALJ5YTSCJFDHUUZMZY63BBV7NU -q -O KPOINTS\n\nsrun --mpi=pmi2 -n 16 vasp_std\n</code></pre>"},{"location":"Documentation/Systems/Vermilion/running/#running-vasp-6-on-gpus","title":"Running VASP 6 on GPUs","text":"<p>VASP can also be run on Vermilion's GPUs. To do this we need to add a few #SBATCH lines at the top of the script to assign the job to run in the gpu partition and to set the gpu binding. The --gpu-bind flag requires 1 set of \"0,1\" for each node used. </p> <pre><code>#SBATCH --nodes=2\n#SBATCH --partition=gpu\n#SBATCH --gpu-bind=map_gpu:0,1,0,1\n</code></pre> <p>A gpu build of VASP can be accessed by adding the following path to your PATH variable.</p> <pre><code>export PATH=/projects/hpcapps/tkaiser2/vasp/6.3.1/nvhpc_acc:$PATH\n</code></pre> <p>This will give you:</p> <pre><code>[myuser@vs example]$ which vasp_gam\n/projects/hpcapps/tkaiser2/vasp/6.3.1/nvhpc_acc/vasp_gam\n[myuser@vs example]$ which vasp_ncl\n/projects/hpcapps/tkaiser2/vasp/6.3.1/nvhpc_acc/vasp_ncl\n[myuser@vs example]$ which vasp_std\n/projects/hpcapps/tkaiser2/vasp/6.3.1/nvhpc_acc/vasp_std\n</code></pre> <p>Instead of srun, use mpirun to run VASP on GPUs. Since Vermilion only has 1 GPU per node, it's important to make sure you are only requesting 1 task per node by setting -npernode 1. </p> <pre><code>mpirun -npernode 1 vasp_std &gt; vasp.$SLURM_JOB_ID\n</code></pre> <p>There's a few more modules needed to run VASP on GPUs, and two library variables need to be set. We can modify the VASP CPU script to include lines to load the modules, set library variables and make the changes outlined above. The final script will look something like this.</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=vasp\n#SBATCH --nodes=2\n#SBATCH --time=1:00:00\n##SBATCH --error=std.err\n##SBATCH --output=std.out\n#SBATCH --partition=gpu\n#SBATCH --gpu-bind=map_gpu:0,1,0,1\n#SBATCH --exclusive\n\ncat $0\n\nhostname\n\n#load necessary modules and set library paths\nmodule use  /nopt/nrel/apps/220421a/modules/lmod/linux-rocky8-x86_64/gcc/11.3.0/\nml nvhpc\nml gcc\nml fftw\nexport LD_LIBRARY_PATH=/nopt/nrel/apps//220421a/install/opt/spack/linux-rocky8-zen2/gcc-11.3.0/nvhpc-22.2-ruzrtpyewnnrif6s7w7rehvpk7jimdrd/Linux_x86_64/22.2/compilers/extras/qd/lib:$LD_LIBRARY_PATH\nexport LD_LIBRARY_PATH=/nopt/nrel/apps//220421a/install/opt/spack/linux-rocky8-zen2/gcc-11.3.0/gcc-11.3.0-c3u46uvtuljfuqimb4bgywoz6oynridg/lib64:$LD_LIBRARY_PATH\n\n#add a path to the gpu build of VASP to your script\nexport PATH=/projects/hpcapps/tkaiser2/vasp/6.3.1/nvhpc_acc:$PATH\n\n#### wget is needed to download data\nml wget\n\n#### get input and set it up\n#### This is from an old benchmark test\n#### see https://github.nrel.gov/ESIF-Benchmarks/VASP/tree/master/bench2\n\n\nmkdir input\n\nwget https://github.nrel.gov/raw/ESIF-Benchmarks/VASP/master/bench2/input/INCAR?token=AAAALJZRV4QFFTS7RC6LLGLBBV67M   -q -O INCAR\nwget https://github.nrel.gov/raw/ESIF-Benchmarks/VASP/master/bench2/input/POTCAR?token=AAAALJ6E7KHVTGWQMR4RKYTBBV7SC  -q -O POTCAR\nwget https://github.nrel.gov/raw/ESIF-Benchmarks/VASP/master/bench2/input/POSCAR?token=AAAALJ5WKM2QKC3D44SXIQTBBV7P2  -q -O POSCAR\nwget https://github.nrel.gov/raw/ESIF-Benchmarks/VASP/master/bench2/input/KPOINTS?token=AAAALJ5YTSCJFDHUUZMZY63BBV7NU -q -O KPOINTS\n\nmpirun -npernode 1 vasp_std &gt; vasp.$SLURM_JOB_ID\n</code></pre>"},{"location":"Documentation/languages/bash/bash-starter/","title":"An Introduction to Bash Scripting","text":"<p>Bash (Bourne Again Shell) is one of the most widely available and used command line shell applications. Along with basic shell functionality, it offers a wide variety of features which, if utilized thoughtfully, can create powerful automated execution sequences that run software, manipulate text and files, parallelize otherwise single-process software, or anything else you may want to do from the command line. </p> <p>Shell scripts are also one of the most common ways our HPC community submits jobs, and running a large parallel workload often requires some initialization of the software environment before meaningful computations can begin. This typically involves tasks such as declaring environment\u00a0variables, preparing input files or staging directories for data, loading modules and libraries that the software needs to run, preparing inputs, manipulating datasets, and so on. Bash can even be used to launch several single-core jobs, effectively taking on the role of an ad hoc batch executor, as well. </p> <p>This article provides a brief introduction to bash, as well as a list of tips, tricks, and good practices when it comes to writing effective bash scripts that can apply widely in both HPC and non-HPC environments. We will also provide links to some additional resources to help further your bash scripting skills.</p>"},{"location":"Documentation/languages/bash/bash-starter/#executinginvoking-scripts","title":"Executing/Invoking Scripts","text":"<p>All of bash commands work at the command prompt \"live\", i.e. interpreted line-by-line as you type commands and press enter. A bash \"script\" may be regarded as a list of bash commands that have been saved to a file for convenience, usually with some basic formatting, and possibly comments, for legibility.</p> <p>All bash scripts must begin with a special character combination, called the \"shebang\" or <code>#!</code> character, followed by the name of an interpreter:</p> <p><code>#!/bin/bash</code></p> <p>This declares that the contents of the file that follow are to be interpreted as commands, using <code>/bin/bash</code> as the interpreter. This includes commands, control structures, and comments.</p> <p>Plenty of other interpreters exist. For example, Python scripts begin with: <code>#!/usr/bin/python</code> or <code>/usr/bin/env python</code>, perl scripts: <code>#!/usr/bin/perl</code>, and so on.</p>"},{"location":"Documentation/languages/bash/bash-starter/#bash-scripting-syntax","title":"Bash Scripting Syntax","text":"<p>If you read a bash script, you may be tempted to default to your usual understanding of how code generally works. For example, with most languages, typically there is a binary or kernel which digests the code you write (compilers/gcc for C, the python interpreter/shell, Java Virtual Machine for Java, and so on.) The binary/kernel/interpreter then interprets the text into some sort of data structure which enforces the priority of certain commands over others, and finally generates some execution of operations based on that data structure.  </p> <p>Bash isn't too far off from this model, and in some respects functions as any other interpreted language: you enter a command (or a control structure) and it is executed. </p> <p>However, as a shell that also serves as your major interface to the underlying operating system, it does have some properties and features that may blur the lines between what you think of as 'interpreted' versus 'compiled'.</p> <p>For instance, many aspects of the bash \"language\" are actually just the names of pre-compiled binaries which do the heavy lifting. Much the same way you can run <code>python</code>\u00a0or <code>ssh</code>\u00a0in a command line, under the hood normal bash operations such as <code>if</code>, <code>echo</code>, and <code>exit</code>\u00a0are actually just programs that expect a certain cadence for the arguments you give it. A block such as:</p> <p><pre><code>if true; then echo \"true was true\"; fi\n</code></pre> This is really just a sequence of executing many compiled applications or shell built-ins with arguments; the names of these commands were just chosen to read as a typical programming grammar. </p> <p>A good example is the program <code>[</code> which is just an oddly-named command you can invoke. Try running <code>which [</code> at a command prompt. The results may surprise you: <code>/usr/bin/[</code> is actually a compiled program on disk, not a \"built-in\" function!</p> <p>This is why you need to have a space between the brackets\u00a0and your conditional, because the conditional itself is passed as an argument to the command <code>[</code>. In languages like C it's common to write the syntax as <code>if (conditional)\u00a0{ ...; }</code>. However, in bash, if you try to run <code>if\u00a0[true]</code>\u00a0you will likely get an error saying there isn't a command called <code>[true]</code>\u00a0that you can run. This is also why you often see stray semicolons that seem somewhat arbitrary, as semicolons separate the execution of two binaries. Take this snippet for example: <pre><code>echo \"First message.\" ; echo \"Second message.\"\n</code></pre> This is equivalent to: <pre><code>echo \"First message.\"\necho \"Second message.\"\n</code></pre> In the first snippet, if the semicolon was not present, the second <code>echo</code>\u00a0would be interpreted as an argument to the first echo and would end up outputting:\u00a0<code>First message. echo Second message.</code></p> <p>Bash interprets <code>;</code> and <code>\\n</code> (newline) as separators. If you need to pass these characters into a function (for example, common in <code>find</code>'s <code>-exec</code> flag) you need to escape them with a <code>\\</code>. This is useful for placing arguments on separate lines to improve readability like this example: <pre><code>chromium-browser \\\n--start-fullscreen \\\n--new-window \\\n--incognito \\\n'https://google.com'\n</code></pre></p> <p>Similarly, normal if-then-else control flow that you would expect of any programming/scripting language has the same caveats. Consider this snippet: <pre><code>if\u00a0true\nthen\necho \"true is true\"\nelse\necho \"false is true?\"\nfi\n</code></pre> If we break down what's essentially happening here (omitting some of the technical details):</p> <ul> <li><code>if</code>\u00a0invokes the command <code>true</code> which always exits with a successful exit code (<code>0</code>)</li> <li><code>if</code> interprets a success exit code (<code>0</code>) as a truism and runs the <code>then</code>.</li> <li>the <code>then</code> command will execute anything it's given until <code>else</code>, <code>elif</code>, or <code>fi</code></li> <li>the <code>else</code> command is the same as <code>then</code> but will only execute if <code>if</code>\u00a0returned an erroneous exit code.</li> <li>the <code>fi</code> command\u00a0indicates that no more conditional branches exist relative to the logical expression given to the original <code>if</code>.</li> </ul> <p>All this to say, this is why you often see if-then-else blocks written succinctly as <code>if\u00a0[ &lt;CONDITIONAL&gt; ]; then &lt;COMMANDS&gt;; fi</code>\u00a0with seemingly arbitrary semicolons and spaces. It is exactly why things work this way that bash is able to execute arbitrary executables (some of which you may end up writing) and not require something like Python's subprocess module.</p> <p>This is just to give you an understanding for\u00a0why some of the syntax you will encounter is the way it is. Everything in bash is either a command or an argument to a command.</p>"},{"location":"Documentation/languages/bash/bash-starter/#parentheses-braces-and-brackets","title":"Parentheses, Braces, and Brackets","text":"<p>Bash utilizes many flavors of symbolic enclosures. A complete guide is beyond the scope of this document, but you may see the following:</p> <ul> <li><code>( )</code> - Single parentheses: run enclosed commands in a subshell<ul> <li><code>a='bad';(a='good'; mkdir $a); echo $a</code>  result: directory \"good\" is made, echoes \"bad\" to screen</li> </ul> </li> <li><code>$( )</code> - Single parentheses with dollar sign: subshell output to string(command substitution) (preferred method)<ul> <li><code>echo \"my name is $( whoami )\"</code> result: prints your username</li> </ul> </li> <li><code>&lt;( )</code> - Parentheses with angle bracket: process substitution<ul> <li><code>sort -n -k 5 &lt;( ls -l ./dir1) &lt;(ls -l ./dir2)</code> result: sorts ls -l results of two directories by column 5 (size)</li> </ul> </li> <li><code>[ ]</code> - Single Brackets: truth testing with filename expansion or word splitting<ul> <li><code>if [ -e myfile.txt ]; then echo \"yay\"; else echo \"boo\"; fi</code> result: if myfile.txt exists, celebrate</li> </ul> </li> <li><code>{ }</code> - Single Braces/curly brackets: expansion of a range</li> <li><code>${ }</code> - Single braces with dollar sign: expansion with interpolation</li> <li><code>` `</code> - Backticks: command/process substitution</li> <li><code>(( ))</code> - Double parentheses: integer arithmetic </li> <li><code>$(( ))</code> - Double parentheses with dollar sign: integer arithmatic to string</li> <li><code>[[ ]]</code> - Double brackets: truth testing with regex </li> </ul>"},{"location":"Documentation/languages/bash/bash-starter/#additional-notes-on-single-parentheses","title":"Additional Notes on <code>( )</code> (Single Parentheses)","text":"<p>There are 3 features in Bash which are denoted by a pair of parentheses, which are Bash subshells, Bash array declarations, and Bash function declarations. See the table below for when each feature is enacted:</p> Syntax Bash Feature Command/line begins with <code>(</code> Run the contained expression(s) in a subshell. This will pass everything until a closing <code>)</code> to a child-fork of Bash that inherits the environment from the invoking Bash instance, and exits with the exit code of the last command the subshell exitted with. See the section on subshells for more info. A valid Bash identifier is set equal to a parnethetically enclosed list of items(.e.g. <code>arr=(\"a\" \"b\" \"c\")</code> ) Creates a Bash array with elements enclosed by the parentheses. The default indexing of the elements is numerically incremental from 0 in the given order, but this order can be overridden or string-based keys can be used. See the section on arrays for more info. A valid Bash identifier is followed by <code>()</code> and contains some function(s) enclosed by <code>{ }</code>(i.e. <code>func() { echo \"test\"; }</code> ) Declare a function which can be re/used throughout a Bash script. See the either of \"<code>{ }</code>\" or functions for more info."},{"location":"Documentation/languages/bash/bash-starter/#examples-of-enclosure-usage","title":"Examples of Enclosure Usage","text":"<p>Note that whitespace is required, prohibited, or ignored in certain situations. See this block for specific examples of how to use whitespace in the various contexts of parantheses. <pre><code>### Subshells\n(echo hi)   # OK\n( echo hi)  # OK\n(echo hi )  # OK\n( echo hi ) # OK\n\n### Arrays\narr=(\"a\" \"b\" \"c\")   # Array of 3 strings\narr =(\"a\" \"b\" \"c\")    # ERROR\narr= (\"a\" \"b\" \"c\")    # ERROR\narr = (\"a\" \"b\" \"c\")   # ERROR\narr=(\"a\"\"b\"\"c\")     # Array of one element that is \"abc\"\narr=(\"a\",\"b\",\"c\")   # Array of one element that is \"a,b,c\"\narr=(\"a\", \"b\", \"c\") # ${arr[0]} == \"a,\"\n\n### Functions \nfunc(){echo hi;} # ERROR\nfunc(){ echo hi;}     # OK\nfunc (){ echo hi;}    # OK\nfunc () { echo hi;}   # OK\nfunc () { echo hi; }  # OK\n</code></pre></p> Command Behavior <code>(ls -1 | head -n 1)</code> Run the command in a subshell. This will return the exit code of the last process that was ran. <code>test_var=(ls -1)</code> Create a bash array with the elements <code>ls</code> and <code>-1</code>, meaning <code>${test_var[1]}</code> will evaluate to <code>-1</code>. <code>test_var=$(ls -1)</code> Evaluate <code>ls -1</code> and capture the output as a string. <code>test_var=(`ls -1`)</code> or <code>test_var=($(ls -1))</code> Evaluate <code>ls -1</code> and capture the output as an array."},{"location":"Documentation/languages/bash/bash-starter/#bracket-usage","title":"Bracket Usage:","text":"<p>Correct:</p> <ul> <li> <p><code>[ cmd ]</code> - There must be spaces or terminating characters (<code>\\n</code> or <code>;</code>) surrounding any brackets. </p> </li> <li> <p>Like many common bash commands, \"[\" is actually a standalone executable, usually located at <code>/usr/bin/[</code>, so it requires spaces to invoke correctly. </p> </li> </ul> <p>Erroneous:</p> <ul> <li><code>[cmd]</code>   - tries to find a command called <code>[cmd]</code> which likely doesn't exist</li> <li><code>[cmd ]</code>  - tries to find a command called <code>[cmd</code> and pass <code>]</code> as an argument to it</li> <li><code>[ cmd]</code>  - tries to pass <code>cmd]</code> as an argument to <code>[</code> which expects an argument of <code>]</code> that isn't technically provided.</li> </ul> <p>There are many other examples of using enclosures in bash scripting beyond the scope of this introduction. Please see the resources section for more information.</p>"},{"location":"Documentation/languages/bash/bash-starter/#variables","title":"Variables","text":"<p>Variable assignment in bash is simply to assign a value to a string of characters. All subsequent references to that variable must be prefixed by <code>$</code>:</p> <pre><code>$ MYSTRING=\"a string\"\n$ echo $MYSTRING\na string\n$ MYNUMBER=\"42\"\n$ echo $MYNUMBER\n42\n</code></pre>"},{"location":"Documentation/languages/bash/bash-starter/#exporting-variables","title":"Exporting Variables","text":"<p>When you declare a variable in bash, that variable is only available in the shell in which it is declared; if you spawn a sub-shell, the variable will not be accessible. Using the <code>export</code> command, you can essentially declare the variable to be inheritable.</p> <pre><code># without exporting:\n$ TESTVAR=100  \n$ echo $TESTVAR\n100     # returns a result\n$ bash  # spawn a sub-shell\n$ echo $TESTVAR\n# no result\n$ exit  # exit the subshell\n# with exporting: \n$ export TESTVAR=100\n$ echo $TESTVAR\n100     # returns a result \n$ bash  # spawn a sub-shell\n$ echo $TESTVAR  100     # value is passed into the subshell\n$ exit  # exit the subshell\n$\n</code></pre>"},{"location":"Documentation/languages/bash/bash-starter/#sourcing-variables","title":"Sourcing Variables","text":"<p>\"Source\" (shortcut: <code>.</code>) is a built-in bash command that takes a bash script as an argument. Bash will execute the contents of that file in the current shell, instead of spawning a sub-shell. This will load any variables, function declarations, and so on into your current shell. </p> <p>A common example of using the <code>source</code> command is when making changes to your <code>~/.bashrc</code>, which is usually only parsed once upon login. Rather than logging out and logging back in every time you wish to make a change, you can simply run <code>source ~/.bashrc</code> or <code>. ~/.bashrc</code> and the changes will take effect immediately.</p>"},{"location":"Documentation/languages/bash/bash-starter/#declaring-variables","title":"Declaring Variables","text":"<p>Variable typing in bash is implicit, and the need to declare a type is rare, but the <code>declare</code> command can be used when necessary: <pre><code>$ declare -i MYNUMBER # set type as an integer\n$ echo $MYNUMBER\n0\n$ declare -l MYWORD=\"LOWERCASE\" # set type as lowercase \n$ echo $MYWORD\nlowercase\n$\n</code></pre> see <code>help declare</code> at the command line for more information on types that can be declared.</p>"},{"location":"Documentation/languages/bash/bash-starter/#further-resources","title":"Further Resources","text":"<p>NREL HPC Github - User-contributed bash script and examples that you can use on HPC systems.</p> <p>BASH cheat sheet - A concise and extensive list of example commands, built-ins, control structures, and other useful bash scripting material.</p>"},{"location":"Documentation/languages/fortran/f90/","title":"Advanced Fortran 90","text":"<p>This document is derived from an HTML page written at the San Diego Supercomper Center many years ago. Its purpose is to Introduce Fortran 90 concepts to Fortran 77 programers.  It does this by presenting an example program and introducing concepts as various routines of the program are presented.  The original web page has been used over the years and has been translated into several languages. </p>"},{"location":"Documentation/languages/fortran/f90/#format-for-our-presentation","title":"Format for our presentation","text":"<ul> <li>We will \"develop\" an application<ul> <li>Incorporate f90 features</li> <li>Show source code</li> <li>Explain what and why as we do it</li> </ul> </li> <li>Application is a genetic algorithm<ul> <li>Easy to understand and program</li> <li>Offers rich opportunities for enhancement</li> </ul> </li> <li>We also provide an summary of F90 syntax, key words, operators, constants, and functions</li> </ul>"},{"location":"Documentation/languages/fortran/f90/#what-was-in-mind-of-the-language-writers-what-were-they-thinking","title":"What was in mind of the language writers? What were they thinking?","text":"<ul> <li>Enable portable codes<ul> <li>Same precision</li> <li>Include many common extensions</li> </ul> </li> <li>More reliable programs</li> <li>Getting away from underlying hardware</li> <li>Move toward parallel programming</li> <li>Run old programs</li> <li>Ease of programming<ul> <li>Writing</li> <li>Maintaining</li> <li>Understanding</li> <li>Reading</li> </ul> </li> <li>Recover C and C++ users</li> </ul>"},{"location":"Documentation/languages/fortran/f90/#why-fortran","title":"Why Fortran?","text":"<p>Famous Quote: \"I don't know what the technical characteristics of  the standard language for scientific and engineering computation in the year 2000 will be... but I know it will be called Fortran.\" John Backus.* </p> <p>### Note: He claimed that he never said this.</p> <ul> <li>Language of choice for Scientific programming</li> <li>Large installed user base.</li> <li>Fortran 90 has most of the features of C . . . and then some</li> <li>The compilers produce better programs</li> </ul>"},{"location":"Documentation/languages/fortran/f90/#justification-of-topics","title":"Justification of topics","text":"<ul> <li>Enhance performance</li> <li>Enhance portability</li> <li>Enhance reliability</li> <li>Enhance maintainability</li> </ul>"},{"location":"Documentation/languages/fortran/f90/#classification-of-topics","title":"Classification of topics","text":"<ul> <li>New useful features</li> <li>Old tricks</li> <li>Power features</li> <li>Overview of F90</li> </ul>"},{"location":"Documentation/languages/fortran/f90/#listing_of_topics_covered","title":"Listing_of_topics_covered","text":"<ol> <li>Listing of topics covered</li> <li>What is a Genetic Algorithm</li> <li>Simple algorithm for a GA</li> <li>Our example problem</li> <li>Start of real Fortran 90 discussion</li> <li>Comparing a FORTRAN 77 routine to a Fortran 90 routine</li> <li>Obsolescent features</li> <li>New source Form and related things</li> <li>New data declaration method</li> <li>Kind facility</li> <li>Modules</li> <li>Module functions and subroutines</li> <li>Allocatable arrays (the basics)</li> <li>Passing arrays to subroutines</li> <li>Interface for passing arrays</li> <li>Optional arguments and intent</li> <li>Derived data types</li> <li>Using defined types</li> <li>User defined operators</li> <li>Recursive functions introduction</li> <li>Fortran 90 recursive functions</li> <li>Pointers</li> <li>Function and subroutine overloading</li> <li>Fortran Minval and Minloc routines</li> <li>Pointer assignment</li> <li>More pointer usage, association and nullify</li> <li>Pointer usage to reference an array</li> <li>Data assignment with structures</li> <li>Using the user defined operator</li> <li>Passing arrays with a given arbitrary lower bounds</li> <li>Using pointers to access sections of arrays</li> <li>Allocating an array inside a subroutine</li> <li>Our fitness function</li> <li>Linked lists</li> <li>Linked list usage</li> <li>Our map representation</li> <li>Date and time functions</li> <li>Non advancing and character IO</li> <li>Internal IO</li> <li>Inquire function</li> <li>Namelist</li> <li>Vector valued functions</li> <li>Complete source for recent discussions</li> <li>Some array specific intrinsic functions</li> <li>The rest of our GA</li> <li>Compiler Information</li> <li>Summary</li> <li>Overview of F90</li> <li>Fortran 95</li> <li>References</li> </ol>"},{"location":"Documentation/languages/fortran/f90/#what-is-a-genetic-algorithm","title":"What is a Genetic Algorithm","text":"<ul> <li>A \"suboptimization\" system<ul> <li>Find good, but maybe not optimal, solutions to difficult problems</li> <li>Often used on NP-Hard or combinatorial optimization problems</li> </ul> </li> <li>Requirements<ul> <li>Solution(s) to the problem represented as a string</li> <li>A fitness function<ul> <li>Takes as input the solution string</li> <li>Output the desirability of the solution</li> </ul> </li> <li>A method of combining solution strings to generate new solutions</li> </ul> </li> <li>Find solutions to problems by Darwinian evolution<ul> <li>Potential solutions ar though of as living entities in a population</li> <li>The strings are the genetic codes for the individuals</li> <li>Fittest individuals are allowed to survive to reproduce</li> </ul> </li> </ul>"},{"location":"Documentation/languages/fortran/f90/#simple-algorithm-for-a-ga","title":"Simple algorithm for a GA","text":"<ul> <li>Generate a initial population, a collection of strings</li> <li>do for some time<ul> <li>evaluate each individual (string) of the population using the fitness function</li> <li>sort the population with fittest coming to the top</li> <li>allow the fittest individuals to \"sexually\" reproduce replacing the old     population</li> <li>allow for mutation</li> </ul> </li> <li>end do</li> </ul>"},{"location":"Documentation/languages/fortran/f90/#our-example-problem","title":"Our example problem","text":"<ul> <li>Instance:Given a map of the N  states or countries and a fixed number of colors</li> <li>Find a coloring of the map, if it exists, such that no two states that share a boarder have the same color</li> <li>Notes         - In general, for a fixed number of colors and an arbitrary map the only                 known way to find if there is a valid coloring is a brute force search                 with the number of combinations = (NUMBER_OF_COLORS)**(NSTATES)<ul> <li>The strings of our population are integer vectors represent the coloring</li> <li>Our fitness function returns the number of boarder violations</li> <li>The GA searches for a mapping with few, hopefully 0 violations</li> <li>This problem is related to several important NP_HARD problems in computer science<ul> <li>Processor scheduling</li> <li>Communication and grid allocation for parallel computing</li> <li>Routing</li> </ul> </li> </ul> </li> </ul>"},{"location":"Documentation/languages/fortran/f90/#start-of-real-fortran-90-discussion","title":"Start of real Fortran 90 discussion","text":""},{"location":"Documentation/languages/fortran/f90/#comparing-a-fortran-77-routine-to-a-fortran-90-routine","title":"Comparing a FORTRAN 77 routine to a Fortran 90 routine","text":"<ul> <li>The routine is one of the random number generators from:  Numerical Recipes, The Art of Scientific Computing. Press, Teukolsky, Vetterling and Flannery.  Cambridge University Press 1986.</li> <li>Changes<ul> <li>correct bugs</li> <li>increase functionality</li> <li>aid portability</li> </ul> </li> </ul>"},{"location":"Documentation/languages/fortran/f90/#original","title":"Original","text":"<pre><code>    function ran1(idum)\nreal ran1\ninteger idum\nreal r(97)\nparameter ( m1=259200,ia1=7141,ic1=54773)\nparameter ( m2=134456,ia2=8121,ic2=28411)\nparameter ( m3=243000,ia3=4561,ic3=51349)\ninteger j\ninteger iff,ix1,ix2,ix3\ndata iff /0/\nif (idum.lt.0.or.iff.eq.0)then\nrm1=1.0/m1\nrm2=1.0/m2\niff=1\nix1=mod(ic1-idum,m1)\nix1=mod(ia1*ix1+ic1,m1)\nix2=mod(ix1,m2)\nix1=mod(ia1*ix1+ic1,m1)\nix3=mod(ix1,m3)\ndo 11 j=1,97\nix1=mod(ia1*ix1+ic1,m1)\nix2=mod(ia2*ix2+ic2,m2)\nr(j)=(real(ix1)+real(ix2)*rm2)*rm1\n11           continue\nidum=1\nendif\nix1=mod(ia1*ix1+ic1,m1)\nix2=mod(ia2*ix2+ic2,m2)\nix3=mod(ia3*ix3+ic3,m3)\nj=1+(97*ix3)/m3\nif(j.gt.97.or.j.lt.1)then\n            write(*,*)' error in ran1 j=',j\nstop\n        endif\nran1=r(j)\nr(j)=(real(ix1)+real(ix2)*rm2)*rm1\nreturn\n     end </code></pre>"},{"location":"Documentation/languages/fortran/f90/#fortran-90","title":"Fortran 90","text":"<pre><code>module ran_mod\ncontains\n     function ran1(idum)\nuse numz\nimplicit none  !note after use statement\nreal (b8) ran1\ninteger , intent(inout), optional ::  idum\nreal (b8) r(97),rm1,rm2\ninteger , parameter :: m1=259200,ia1=7141,ic1=54773\ninteger , parameter :: m2=134456,ia2=8121,ic2=28411\ninteger , parameter :: m3=243000,ia3=4561,ic3=51349\ninteger j\ninteger iff,ix1,ix2,ix3\ndata iff /0/\nsave ! corrects a bug in the original routine\nif(present(idum))then\n          if (idum.lt.0.or.iff.eq.0)then\nrm1=1.0_b8 m1\nrm2=1.0_b8 m2\niff=1\nix1=mod(ic1-idum,m1)\nix1=mod(ia1*ix1+ic1,m1)\nix2=mod(ix1,m2)\nix1=mod(ia1*ix1+ic1,m1)\nix3=mod(ix1,m3)\ndo j=1,97\nix1=mod(ia1*ix1+ic1,m1)\nix2=mod(ia2*ix2+ic2,m2)\nr(j)=(real(ix1,b8)+real(ix2,b8)*rm2)*rm1\nenddo\nidum=1\nendif\n        endif\nix1=mod(ia1*ix1+ic1,m1)\nix2=mod(ia2*ix2+ic2,m2)\nix3=mod(ia3*ix3+ic3,m3)\nj=1+(97*ix3)/m3\nif(j.gt.97.or.j.lt.1)then\n            write(*,*)' error in ran1 j=',j\nstop\n        endif\nran1=r(j)\nr(j)=(real(ix1,b8)+real(ix2,b8)*rm2)*rm1\nreturn\n     end function ran1\n</code></pre>"},{"location":"Documentation/languages/fortran/f90/#comments","title":"Comments","text":"<ol> <li>Modules are a way of encapsulating functions an data.  More below.</li> <li>The use numz line is similar to an include file.  In this case it defines our real data type.</li> <li>real (b8)  is a new way to specify percision for data types in a portable way.</li> <li>integer , intent(inout), optional ::  idum we are saying idum is an optional input parameter</li> <li>integer , parameter :: just a different syntax</li> <li>The save statement is needed for program correctness</li> <li>present(idum) is a function to determine if ran1 was called with the optional parameter</li> </ol>"},{"location":"Documentation/languages/fortran/f90/#obsolescent-features","title":"Obsolescent features","text":"<p>The following are available in Fortran 90. On the other hand, the concept of \"obsolescence\" is introduced. This means that some constructs may be removed in the future. - Arithmetic IF-statement - Control variables in a DO-loop which are floating point or double-precision floating-point - Terminating several DO-loops on the same statement - Terminating the DO-loop in some other way than with CONTINUE or END DO - Alternate return - Jump to END IF from an outer block - PAUSE - ASSIGN and assigned GOTO and assigned FORMAT , that is the whole \"statement number variable\" concept. - Hollerith editing in FORMAT.</p>"},{"location":"Documentation/languages/fortran/f90/#new-source-form-and-related-things","title":"New source form and related things","text":""},{"location":"Documentation/languages/fortran/f90/#summary","title":"Summary","text":"<ul> <li>! now indicates the start of a comment</li> <li>&amp; indicates the next line is a continuation</li> <li>Lines can be longer than 72 characters</li> <li>Statements can start in any column</li> <li>Use ; to put multiple statements on one line</li> <li>New forms for the do loop</li> <li>Many functions are generic</li> <li>32 character names</li> <li>Many new array assignment techniques</li> </ul>"},{"location":"Documentation/languages/fortran/f90/#features","title":"Features","text":"<ul> <li>Flexibility can aid in program readability</li> <li>Readability decreases errors</li> <li>Got ya!<ul> <li>Can no longer use C to start a comment</li> <li>Character in column 5 no longer is continue</li> <li>Tab is not a valid character (may produce a warning)</li> <li>Characters past 72 now count</li> </ul> </li> </ul> <pre><code>program darwin\nreal a(10), b(10), c(10), d(10), e(10), x, y\ninteger odd(5),even(5)\n! this line is continued by using \"&amp;\"\nwrite(*,*)\"starting \",&amp;  \"darwin\" ! this line in a continued from above\n! multiple statement per line --rarely a good idea\nx=1; y=2; write(*,*)x,y  do i=1,10    ! statement lable is not required for do\ne(i)=i\nenddo\nodd= (/ 1,3,5,7,9 /)  ! array assignment\neven=(/ 2,4,6,8,10 /) ! array assignment\na=1          ! array assignment, every element of a = 1\nb=2\nc=a+b+e      ! element by element assignment\nc(odd)=c(even)-1  ! can use arrays of indices on both sides\nd=sin(c)     ! element by element application of intrinsics\nwrite(*,*)d\nwrite(*,*)abs(d)  ! many intrinsic functions are generic\na_do_loop : do i=1,10\nwrite(*,*)i,c(i),d(i)\nenddo a_do_loop\ndo\n        if(c(10) .lt. 0.0 ) exit\nc(10)=c(10)-1\nenddo\n     write(*,*)c(10)\ndo while (c(9) .gt. 0)\nc(9)=c(9)-1\nenddo\n     write(*,*)c(9)\nend program\n</code></pre>"},{"location":"Documentation/languages/fortran/f90/#new-data-declaration-method","title":"New data declaration method","text":"<ul> <li> <p>Motivation</p> <ul> <li>Variables can now have attributes such as         - Parameter             - Save             - Dimension</li> <li>Attributes are assigned in the variable declaration statement</li> </ul> </li> <li> <p>One variable can have several attributes</p> </li> <li>Requires Fortran 90 to have a new statement form</li> </ul> <p><pre><code>integer,parameter :: in2 = 14\nreal, parameter :: pi = 3.141592653589793239\nreal, save, dimension(10) :: cpu_times,wall_times\n!****    the old way of doing the same    ****!\n!****    real cpu_times(10),wall_times(10) ****!\n!****    save cpu_times, wall_times        ****!\n</code></pre> - Other Attributes     -  allocatable     -  public     -  private     -  target     -  pointer     -  intent     -  optional</p>"},{"location":"Documentation/languages/fortran/f90/#kind-facility","title":"Kind facility","text":"<ul> <li>Motivation<ul> <li>Assume we have a program that we want to run on two different machines</li> <li>We want the same representation of reals on both machines (same number     of significant digits)</li> <li>Problem: different machines have different representations for reals</li> </ul> </li> </ul>"},{"location":"Documentation/languages/fortran/f90/#digits-of-precision-for-some-old-machines-and-data-type","title":"Digits of precision for some (old) machines and data type","text":"Machine Real Double Precision IBM (SP) 6 15 Cray (T90) 15 33 Cray (T3E) 15 15"},{"location":"Documentation/languages/fortran/f90/#or","title":"* or *","text":"<ul> <li>We may want to run with at least 6 digits today and at least 14 digits tomorrow</li> <li>Use the Select_Real_Kind(P) function to create a data type with P digits of precision</li> </ul> <pre><code>program darwin\n! e has at least 4 significant digits\nreal(selected_real_kind(4))e\n! b8 will be used to define reals with 14 digits\ninteger, parameter:: b8 = selected_real_kind(14)\nreal(b8), parameter :: pi = 3.141592653589793239_b8 ! note usage of _b8\n! with  a constant\n! to force precision\ne= 2.71828182845904523536\nwrite(*,*)\"starting \",&amp;  ! this line is continued by using \"&amp;\"\n\"darwin\"       ! this line in a continued from above\nwrite(*,*)\"pi has \",precision(pi),\" digits precision \",pi\nwrite(*,*)\"e has   \",precision(e),\" digits precision \",e\nend program\n</code></pre>"},{"location":"Documentation/languages/fortran/f90/#example-output","title":"Example output","text":"<pre><code>  sp001  % darwin\n starting darwin\n pi has  15  digits precision  3.14159265358979312\n e has    6  digits precision  2.718281746\nsp001 %\n</code></pre> <ul> <li>Can convert to/from given precision for all variables created using \"b8\" by changing definition of \"b8\"</li> <li>Use the Select_Real_Kind(P,R) function to create a data type with P digits of precision and exponent range of R</li> </ul>"},{"location":"Documentation/languages/fortran/f90/#modules","title":"Modules","text":"<ul> <li> <p>Motivation:</p> <ul> <li>Common block usage is prone to error</li> <li>Provide most of capability of common blocks but safer</li> <li>Provide capabilities beyond common blocks</li> </ul> </li> <li> <p>Modules can contain:</p> <ul> <li>Data definitions</li> <li>Data to be shared much like using a labeled common</li> <li>Functions and subroutines</li> <li>Interfaces (more on this later)</li> </ul> </li> <li> <p>You \"include\" a module with a \"use\" statement</p> </li> </ul> <pre><code>module numz\ninteger,parameter:: b8 = selected_real_kind(14)\nreal(b8),parameter :: pi = 3.141592653589793239_b8\nintegergene_size\nend module\n program darwin\nuse numz\nimplicit none    ! now part of the standard, put it after the use statements\nwrite(*,*)\"pi has \",precision(pi),\"\ndigits precision \",pi\ncall set_size()\nwrite(*,*)\"gene_size=\",gene_size\nend program\nsubroutine set_size\nuse numz\ngene_size=10\nend subroutine\n</code></pre>"},{"location":"Documentation/languages/fortran/f90/#an-example-run","title":"An example run","text":"<pre><code>  pi has  15  digits precision  3.14159265358979312\n  gene_size=10\n</code></pre>"},{"location":"Documentation/languages/fortran/f90/#module-functions-and-subroutines","title":"Module functions and subroutines","text":"<ul> <li> <p>Motivation:</p> <ul> <li>Encapsulate related functions and subroutines</li> <li>Can \"USE\" these functions in a program or subroutine</li> <li>Can be provided as a library</li> <li>Only routines that contain the use statement can see the routines</li> </ul> </li> <li> <p>Example is a random number package: <pre><code>module ran_mod\n! module contains three functions\n! ran1 returns a uniform random number between 0-1\n! spread returns random number between min - max\n! normal returns a normal distribution\ncontains\n    function ran1()  !returns random number between 0 - 1\nuse numz\nimplicit none\nreal(b8) ran1,x\ncall random_number(x) ! built in fortran 90 random number function\nran1=x\nend function ran1\nfunction spread(min,max)  !returns random # between min/max\nuse numz\nimplicit none\nreal(b8) spread\nreal(b8) min,max\n        spread=(max - min) * ran1() + min\nend function spread\nfunction normal(mean,sigma) !returns a normal distribution\nuse numz\nimplicit none\nreal(b8) normal,tmp\nreal(b8) mean,sigma\ninteger flag\nreal(b8) fac,gsave,rsq,r1,r2\nsave flag,gsave\ndata flag /0/\nif (flag.eq.0) then\nrsq=2.0_b8\ndo while(rsq.ge.1.0_b8.or.rsq.eq.0.0_b8) ! new from for do\nr1=2.0_b8*ran1()-1.0_b8\nr2=2.0_b8*ran1()-1.0_b8\nrsq=r1*r1+r2*r2\nenddo\nfac=sqrt(-2.0_b8*log(rsq)/rsq)\ngsave=r1*fac\ntmp=r2*fac\nflag=1\nelse\ntmp=gsave\nflag=0\nendif\nnormal=tmp*sigma+mean\nreturn\n    end function normal end module ran_mod\n</code></pre></p> </li> </ul> <p>Exersize 1:  Write a program that returns 10 uniform random numbers.</p>"},{"location":"Documentation/languages/fortran/f90/#allocatable-arrays-the-basics","title":"Allocatable arrays (the basics)","text":"<ul> <li> <p>Motivation:</p> <ul> <li>At compile time we may not know the size an array needs to be</li> <li>We may want to change problem size without recompiling</li> </ul> </li> <li> <p>Allocatable arrays allow us to set the size at run time</p> </li> <li>We set the size of the array using the allocate statement</li> <li>We may want to change the lower bound for an array</li> <li>A simple example:</li> </ul> <pre><code>module numz\ninteger, parameter:: b8 = selected_real_kind(14)\ninteger gene_size,num_genes\ninteger,allocatable :: a_gene(:),many_genes(:,:)\nend module\nprogram darwin\nuse numz\nimplicit none\ninteger ierr\ncall set_size()\nallocate(a_gene(gene_size),stat=ierr) !stat= allows for an error code return\nif(ierr /= 0)write(*,*)\"allocation error\"  ! /= is .ne.\nallocate(many_genes(gene_size,num_genes),stat=ierr)  !2d array\nif(ierr /= 0)write(*,*)\"allocation error\"\nwrite(*,*)lbound(a_gene),ubound(a_gene) ! get lower and upper bound\n! for the array\nwrite(*,*)size(many_genes),size(many_genes,1) !get total size and size\n!along 1st dimension\ndeallocate(many_genes) ! free the space for the array and matrix\ndeallocate(a_gene)\nallocate(a_gene(0:gene_size)) ! now allocate starting at 0 instead of 1\nwrite(*,*)allocated(many_genes),allocated(a_gene) ! shows if allocated\nwrite(*,*)lbound(a_gene),ubound(a_gene)\nend program\n  subroutine set_size\nuse numz\nwrite(*,*)'enter gene size:'\nread(*,*)gene_size\nwrite(*,*)'enter number of genes:'\nread(*,*)num_genes\nend subroutine set_size\n</code></pre>"},{"location":"Documentation/languages/fortran/f90/#example-run","title":"Example run","text":"<pre><code>    enter gene size:\n10\n enter number of genes:\n20\n           1          10\n         200          10\n F T\n           0          10\n</code></pre>"},{"location":"Documentation/languages/fortran/f90/#passing-arrays-to-subroutines","title":"Passing arrays to subroutines","text":"<ul> <li>There are several ways to specify arrays for subroutines<ul> <li>Explicit shape<ul> <li>integer, dimension(8,8)::an_explicit_shape_array</li> </ul> </li> <li>Assumed size<ul> <li>integer, dimension(i,*)::an_assumed_size_array</li> </ul> </li> <li>Assumed Shape<ul> <li>integer, dimension(:,:)::an_assumed_shape_array</li> </ul> </li> </ul> </li> </ul>"},{"location":"Documentation/languages/fortran/f90/#example","title":"Example","text":"<pre><code>subroutine arrays(an_explicit_shape_array,&amp;\ni                      ,&amp; !note we pass all bounds except the last\nan_assumed_size_array  ,&amp;\nan_assumed_shape_array)\n! Explicit shape\ninteger, dimension(8,8)::an_explicit_shape_array\n! Assumed size\ninteger, dimension(i,*)::an_assumed_size_array\n! Assumed Shape\ninteger, dimension(:,:)::an_assumed_shape_array\nwrite(*,*)sum(an_explicit_shape_array)\nwrite(*,*)lbound(an_assumed_size_array) ! why does sum not work here?\nwrite(*,*)sum(an_assumed_shape_array)\nend subroutine\n</code></pre>"},{"location":"Documentation/languages/fortran/f90/#interface-for-passing-arrays","title":"Interface for passing arrays","text":"<ul> <li>!!!!Warning!!!!  When passing assumed shape arrays as arguments you must provide an interface</li> <li>Similar to C prototypes but much more versatile</li> <li>The interface is a copy of the invocation line and the argument definitions</li> <li>Modules are a good place for interfaces</li> <li>If a procedure is part of a \"contains\" section in a module an interface     is not required</li> <li>!!!!Warning!!!! The compiler may not tell you that you need an interface <pre><code>module numz\ninteger, parameter:: b8 = selected_real_kind(14)\ninteger,allocatable :: a_gene(:),many_genes(:,:)\nend module module face\ninterface fitness\nfunction fitness(vector)\nuse numz\nimplicit none\nreal(b8) fitness\ninteger, dimension(:) ::  vector\nend function fitness\nend interface\nend module program darwin\nuse numz\nuse face\nimplicit none\ninteger i\ninteger vect(10) ! just a regular array\nallocate(a_gene(10));allocate(many_genes(3,10))\na_gene=1  !sets every element of a_gene to 1\nwrite(*,*)fitness(a_gene)\nvect=8\nwrite(*,*)fitness(vect) ! also works with regular arrays\nmany_genes=3  !sets every element to 3\nmany_genes(1,:)=a_gene  !sets column 1 to a_gene\nmany_genes(2,:)=2*many_genes(1,:)\ndo i=1,3\nwrite(*,*)fitness(many_genes(i,:))\nenddo\n    write(*,*)fitness(many_genes(:,1))  !go along other dimension\n!!!!write(*,*)fitness(many_genes)!!!!does not work\nend program\nfunction fitness(vector)\nuse numz\nimplicit none\nreal(b8) fitness\ninteger, dimension(:)::  vector ! must match interface\nfitness=sum(vector)\nend function\n</code></pre></li> </ul> <p>Exersize 2:  Run this program using the \"does not work line\". Why?  Using intrinsic functions make it work?</p> <p>Exersize 3:  Prove that f90 does not \"pass by address\".</p>"},{"location":"Documentation/languages/fortran/f90/#optional-arguments-and-intent","title":"Optional arguments and intent","text":"<ul> <li>Motivation:<ul> <li>We may have a function or subroutine that we may not want to always pass     all arguments</li> <li>Initialization</li> </ul> </li> <li>Two examples<ul> <li>Seeding the intrinsic random number generator requires keyword arguments</li> <li>To define an optional argument in our own function we use the optional     attribute</li> </ul> </li> </ul> <pre><code>integer :: my_seed\n</code></pre>"},{"location":"Documentation/languages/fortran/f90/#becomes","title":"becomes","text":"<pre><code>integer, optional :: my_seed\n</code></pre> <p>Used like this:</p> <pre><code>! ran1 returns a uniform random number between 0-1\n! the seed is optional and used to reset the generator\ncontains\n   function ran1(my_seed)\nuse numz\nimplicit none\nreal(b8) ran1,r\ninteger, optional ,intent(in) :: my_seed  ! optional argument not changed in the routine\ninteger,allocatable :: seed(:)\ninteger the_size,j\nif(present(my_seed))then            ! use the seed if present\ncall random_seed(size=the_size) ! how big is the intrisic seed?\nallocate(seed(the_size))        ! allocate space for seed\ndo j=1,the_size                 ! create the seed\nseed(j)=abs(my_seed)+(j-1)   ! abs is generic\nenddo\n          call random_seed(put=seed)      ! assign the seed\ndeallocate(seed)                ! deallocate space\nendif\n      call random_number(r)\nran1=r\nend function ran1\nend module program darwin\nuse numz\nuse ran_mod          ! interface required if we have\n! optional or intent arguments\nreal(b8) x,y\nx=ran1(my_seed=12345) ! we can specify the name of the argument\ny=ran1()\nwrite(*,*)x,y\nx=ran1(12345)         ! with only one optional argument we don't need to\ny=ran1()\nwrite(*,*)x,y\nend program\n</code></pre> <ul> <li>Intent is a hint to the compiler to enable optimization<ul> <li>intent(in)<ul> <li>We will not change this value in our subroutine</li> </ul> </li> <li>intent(out)<ul> <li>We will define this value in our routine</li> </ul> </li> <li>intent(inout)<ul> <li>The normal situation</li> </ul> </li> </ul> </li> </ul>"},{"location":"Documentation/languages/fortran/f90/#derived-data-types","title":"Derived data types","text":"<ul> <li> <p>Motivation:</p> <ul> <li>Derived data types can be used to group different types of data together     (integers, reals, character, complex)</li> <li>Can not be done in F77 although people have \"faked\" it</li> </ul> </li> <li> <p>Example</p> <ul> <li>In our GA we define a collection of genes as a 2d array</li> <li>We call the fitness function for every member of the collection</li> <li>We want to sort the collection of genes based on result of fitness function</li> <li>Define a data type that holds the fitness value and an index into the 2d     array</li> <li>Create an array of this data type, 1 for each member of the collection</li> <li>Call fitness function with the result being placed into the new data type     along with a pointer into the array</li> </ul> </li> <li>Again modules are a good place for data type definitions</li> </ul> <pre><code>module galapagos\nuse numz\ntype thefit !the name of the type\nsequence  ! sequence forces the data elements\n! to be next to each other in memory\n! where might this be useful?\nreal(b8) val   ! our result from the fitness function\ninteger index  ! the index into our collection of genes\nend type thefit\nend module\n</code></pre>"},{"location":"Documentation/languages/fortran/f90/#using-defined-types","title":"Using defined types","text":"<ul> <li>Use the % to reference various components of the derived data type <pre><code>program darwin\nuse numz\nuse galapagos ! the module that contains the type definition\nuse face      ! contains various interfaces\nimplicit none\n! define an allocatable array of the data type\n! than contains an index and a real value\ntype (thefit),allocatable ,target  :: results(:)\n! create a single instance of the data type\ntype (thefit) best\ninteger,allocatable :: genes(:,:) ! our genes for the genetic algorithm\ninteger j\ninteger num_genes,gene_size\nnum_genes=10\ngene_size=10\nallocate(results(num_genes))         ! allocate the data type\n! to hold fitness and index\nallocate(genes(num_genes,gene_size)) ! allocate our collection of genes\ncall init_genes(genes)               ! starting data\nwrite(*,'(\"input\")' ) ! we can put format in write statement\ndo j=1,num_genes\nresults(j)%index =j\nresults(j)%val =fitness(genes(j,:)) ! just a dummy routine for now\nwrite(*,\"(f10.8,i4)\")results(j)%val,results(j)%index\nenddo\nend program\n</code></pre></li> </ul>"},{"location":"Documentation/languages/fortran/f90/#user-defined-operators","title":"User defined operators","text":"<ul> <li> <p>Motivation</p> <ul> <li>With derived data types we may want (need) to define operations</li> <li>(Assignment is predefined)</li> </ul> </li> <li> <p>Example:</p> <ul> <li>.lt. .gt. ==  not defined for our data types         -  We want to find the minimum of our fitness values so we need &lt; operator         -  In our sort routine we want to do &lt;, &gt;, ==         -  In C++ terms the operators are overloaded</li> <li>We are free to define new operators</li> </ul> </li> <li> <p>Two step process to define operators</p> <ul> <li>Define a special interface</li> <li>Define the function that performs the operation <pre><code>module sort_mod\n!defining the interfaces\ninterface operator (.lt.)  ! overloads standard .lt.\nmodule procedure theless ! the function that does it\nend interface   interface operator (.gt.)   ! overloads standard .gt.\nmodule procedure thegreat ! the function that does it\nend interface   interface operator (.ge.)  ! overloads standard .ge.\nmodule procedure thetest ! the function that does it\nend interface   interface operator (.converged.)  ! new operator\nmodule procedure index_test     ! the function that does it\nend interface\n  contains      ! our module will contain\n! the required functions\nfunction theless(a,b) ! overloads .lt. for the type (thefit)\nuse galapagos\nimplicit none\n    type(thefit), intent (in) :: a,b\nlogical theless           ! what we return\nif(a%val .lt. b%val)then     ! this is where we do the test\ntheless=.true.\nelse\ntheless=.false.\nendif\n    return\n  end function theless   function thegreat(a,b) ! overloads .gt. for the type (thefit)\nuse galapagos\nimplicit none\n    type(thefit), intent (in) :: a,b\nlogical thegreat\nif(a%val .gt. b%val)then\nthegreat=.true.\nelse\nthegreat=.false.\nendif\n    return\n  end function thegreat\nfunction thetest(a,b)   ! overloads .gt.= for the type (thefit)\nuse galapagos\nimplicit none\n    type(thefit), intent (in) :: a,b\nlogical thetest\nif(a%val &gt;= b%val)then\nthetest=.true.\nelse\nthetest=.false.\nendif\n    return\nend function thetest\nfunction index_test(a,b) ! defines a new operation for the type (thefit)\nuse galapagos\nimplicit none\n    type(thefit), intent (in) :: a,b\nlogical index_test\nif(a%index .gt. b%index)then   ! check the index value for a difference\nindex_test=.true.\nelse\nindex_test=.false.\nendif\n    return\nend function index_test\n</code></pre></li> </ul> </li> </ul>"},{"location":"Documentation/languages/fortran/f90/#recursive-functions-introduction","title":"Recursive functions introduction","text":"<ul> <li> <p>Notes</p> <ul> <li>Recursive function is one that calls itself</li> <li>Anything that can be done with a do loop can be done using a recursive     function</li> </ul> </li> <li> <p>Motivation</p> <ul> <li>Sometimes it is easier to think recursively</li> <li>Divide an conquer algorithms are recursive by nature         -  Fast FFTs             -  Searching             -  Sorting</li> </ul> </li> </ul>"},{"location":"Documentation/languages/fortran/f90/#algorithm-of-searching-for-minimum-of-an-array","title":"Algorithm of searching for minimum of an array","text":"<pre><code>    function findmin(array)\nis size of array 1?\nmin in the array is first element\nelse\nfind minimum in left half of array using findmin function\nfind minimum in right half of array using findmin function\nglobal minimum is min of left and right half\nend function\n</code></pre>"},{"location":"Documentation/languages/fortran/f90/#fortran-90-recursive-functions","title":"Fortran 90 recursive functions","text":"<ul> <li>Recursive functions should have an interface</li> <li>The result and recursive keywords are required as part of the function definition</li> <li>Example is a function finds the minimum value for an array</li> </ul> <pre><code>recursive function realmin(ain) result (themin)\n! recursive and result are required for recursive functions\nuse numz\nimplicit none\nreal(b8) themin,t1,t2\ninteger n,right\nreal(b8) ,dimension(:) :: ain\nn=size(ain)\nif(n == 1)then\nthemin=ain(1) ! if the size is 1 return value\nreturn\n    else\nright=n/2\nt1=realmin(ain(1:right))   ! find min in left half\nt2=realmin(ain(right+1:n)) ! find min in right half\nthemin=min(t1,t2)          ! find min of the two sides\nendif\nend function\n</code></pre> <ul> <li>Example 2 is the same except the input data is our derived data type</li> </ul> <pre><code>!this routine works with the data structure thefit not reals\nrecursive function typemin(ain) result (themin)\nuse numz\nuse sort_mod\nuse galapagos\nimplicit none\nreal(b8) themin,t1,t2\ninteger n,right\ntype (thefit) ,dimension(:) :: ain ! this line is different\nn=size(ain)\nif(n == 1)then\nthemin=ain(1)%val  ! this line is different\nreturn\n else\nright=n/2\nt1=typemin(ain(1:right))\nt2=typemin(ain(right+1:n))\nthemin=min(t1,t2)\nendif\nend function\n</code></pre>"},{"location":"Documentation/languages/fortran/f90/#pointers","title":"Pointers","text":"<ul> <li> <p>Motivation</p> <ul> <li>Can increase performance</li> <li>Can improve readability</li> <li>Required for some derived data types (linked lists and trees)</li> <li>Useful for allocating \"arrays\" within subroutines</li> <li>Useful for referencing sections of arrays</li> </ul> </li> <li> <p>Notes</p> <ul> <li>Pointers can be thought of as an alias to another variable</li> <li>In some cases can be used in place of an array</li> <li>To assign a pointer use =&gt; instead of just =</li> <li>Unlike C and C++, pointer arithmetic is not allowed</li> </ul> </li> <li> <p>First pointer example</p> <ul> <li>Similar to the last findmin routine</li> <li>Return a pointer to the minimum</li> </ul> </li> </ul> <pre><code>recursive function pntmin(ain) result (themin) ! return a pointer\nuse numz\nuse galapagos\nuse sort_mod ! contains the .lt. operator for thefit type\nimplicit none\n type (thefit),pointer:: themin,t1,t2\ninteger n,right\ntype (thefit) ,dimension(:),target :: ain\nn=size(ain)\nif(n == 1)then\nthemin=&gt;ain(1) !this is how we do pointer assignment\nreturn\n else\nright=n/2\nt1=&gt;pntmin(ain(1:right))\nt2=&gt;pntmin(ain(right+1:n))\nif(t1 .lt. t2)then; themin=&gt;t1; else; themin=&gt;t2; endif\n endif\nend function\n</code></pre> <p>Exercise 4:  Carefully write a recursive N! program.</p>"},{"location":"Documentation/languages/fortran/f90/#function-and-subroutine-overloading","title":"Function and subroutine overloading","text":"<ul> <li> <p>Motivation</p> <ul> <li>Allows us to call functions or subroutine with the same name with different     argument types</li> <li>Increases readability</li> </ul> </li> <li> <p>Notes:</p> <ul> <li>Similar in concept to operator overloading</li> <li>Requires an interface</li> <li>Syntax for subroutines is same as for functions</li> <li>Many intrinsic functions have this capability         -  abs (reals,complex,integer)             -  sin,cos,tan,exp(reals, complex)             -  array functions(reals, complex,integer)</li> <li>Example         -  Recall we had two functions that did the same thing but with different argument types</li> </ul> </li> </ul> <p><pre><code>         recursive function realmin(ain) result (themin)\n         real(b8) ,dimension(:) :: ain         recursive function typemin(ain) result (themin)\n         type (thefit) ,dimension(:) :: ain\n</code></pre> - We can define a generic interface for these two functions and call     them using the same name</p> <pre><code>! note we have two functions within the same interface\n! this is how we indicate function overloading\n! both functions are called \"findmin\" in the main program\ninterface findmin\n! the first is called with an array of reals as input\nrecursive function realmin(ain) result (themin)\nuse numz\nreal(b8) themin\nreal(b8) ,dimension(:) :: ain\nend function ! the second is called with a array of data structures as input\nrecursive function typemin(ain) result (themin)\nuse numz\nuse galapagos\nreal(b8) themin\ntype (thefit) ,dimension(:) :: ain\nend function\n    end interface\n</code></pre>"},{"location":"Documentation/languages/fortran/f90/#example-usage","title":"Example usage","text":"<pre><code>program darwin\nuse numz\nuse ran_mod\nuse galapagos ! the module that contains the type definition\nuse face      ! contains various interfaces\nuse sort_mod  ! more about this later it\n! contains our sorting routine\n! and a few other tricks\nimplicit none\n! create an allocatable array of the data type\n! than contains an index and a real value\ntype (thefit),allocatable ,target :: results(:)\n! create a single instance of the data type\ntype (thefit) best\n! pointers to our type\ntype (thefit) ,pointer :: worst,tmp\ninteger,allocatable :: genes(:,:) ! our genes for the ga\ninteger j\ninteger num_genes,gene_size\nreal(b8) x\nreal(b8),allocatable :: z(:)\nreal(b8),pointer :: xyz(:) ! we'll talk about this next\nnum_genes=10\ngene_size=10\nallocate(results(num_genes))         ! allocate the data type to\nallocate(genes(num_genes,gene_size)) ! hold our collection of genes\ncall init_genes(genes)               ! starting data\nwrite(*,'(\"input\")')\ndo j=1,num_genes\nresults(j)%index=j\nresults(j)%val=fitness(genes(j,:)) ! just a dummy routine\nwrite(*,\"(f10.8,i4)\")results(j)%val,results(j)%index\nenddo     allocate(z(size(results)))\nz=results(:)%val ! copy our results to a real array ! use a recursive subroutine operating on the real array\nwrite(*,*)\"the lowest fitness: \",findmin(z)\n! use a recursive subroutine operating on the data structure\nwrite(*,*)\"the lowest fitness: \",findmin(results)\nend program\n</code></pre>"},{"location":"Documentation/languages/fortran/f90/#fortran-minval-and-minloc-routines","title":"Fortran Minval and Minloc routines","text":"<ul> <li>Fortran has routines for finding minimum and maximum values in arrays and     the locations<ul> <li>minval</li> <li>maxval</li> <li>minloc (returns an array)</li> <li>maxloc (returns an array)</li> </ul> </li> </ul> <pre><code>! we show two other methods of getting the minimum fitness\n! use the built in f90 routines  on a real array\nwrite(*,*)\"the lowest fitness: \",minval(z),minloc(z)\n</code></pre>"},{"location":"Documentation/languages/fortran/f90/#pointer-assignment","title":"Pointer assignment","text":"<ul> <li>This is how we use the pointer function defined above</li> <li>worst is a pointer to our data type</li> <li>note the use of =&gt; <pre><code>! use a recursive subroutine operating on the data\n! structure and returning a pointer to the result\n    worst=&gt;pntmin(results) ! note pointer assignment\n! what will this line write?\n write(*,*)\"the lowest fitness: \",worst\n</code></pre></li> </ul>"},{"location":"Documentation/languages/fortran/f90/#more-pointer-usage-association-and-nullify","title":"More pointer usage, association and nullify","text":"<ul> <li> <p>Motivation</p> <ul> <li>Need to find if pointers point to anything</li> <li>Need to find if two pointers point to the same thing</li> <li>Need to deallocate and nullify when they are no longer used</li> </ul> </li> <li> <p>Usage</p> <ul> <li>We can use associated() to tell if a pointer has been set</li> <li>We can use associated() to compare pointers</li> <li>We use nullify to zero a pointer</li> </ul> </li> </ul> <pre><code>! This code will print \"true\" when we find a match,\n! that is the pointers point to the same object\ndo j=1,num_genes\ntmp=&gt;results(j)\nwrite(*,\"(f10.8,i4,l3)\")results(j)%val,   &amp;\nresults(j)%index, &amp;\nassociated(tmp,worst)\nenddo\n    nullify(tmp)\n</code></pre> <ul> <li>Notes:<ul> <li>If a pointer is nullified the object to which it points is not deallocated.</li> <li>In general, pointers as well as allocatable arrays become undefined on leaving a subroutine</li> <li>This can cause a memory leak</li> </ul> </li> </ul>"},{"location":"Documentation/languages/fortran/f90/#pointer-usage-to-reference-an-array-without-copying","title":"Pointer usage to reference an array without copying","text":"<ul> <li>Motivation<ul> <li>Our sort routine calls a recursive sorting routine</li> <li>It is messy and inefficient to pass the array to the recursive routine</li> </ul> </li> <li>Solution<ul> <li>We define a \"global\" pointer in a module</li> <li>We point the pointer to our input array</li> </ul> </li> </ul> <pre><code>module Merge_mod_types\nuse galapagos\ntype(thefit),allocatable :: work(:) ! a \"global\" work array\ntype(thefit), pointer:: a_pntr(:)   ! this will be the pointer to our input array\nend module Merge_mod_types\nsubroutine Sort(ain, n)\nuse Merge_mod_types\nimplicit none\ninteger n\ntype(thefit), target:: ain(n)\nallocate(work(n))\nnullify(a_pntr)\na_pntr=&gt;ain  ! we assign the pointer to our array\n! in RecMergeSort we reference it just like an array\ncall RecMergeSort(1,n) ! very similar to the findmin functions\ndeallocate(work)\nreturn\nend subroutine Sort\n</code></pre> <ul> <li>In our main program sort is called like this: <pre><code>! our sort routine is also recursive but\n! also shows a new usage for pointers\ncall sort(results,num_genes)\ndo j=1,num_genes\nwrite(*,\"(f10.8,i4)\")results(j)%val,   &amp;\nresults(j)%index\nenddo\n</code></pre></li> </ul>"},{"location":"Documentation/languages/fortran/f90/#data-assignment-with-structures","title":"Data assignment with structures","text":"<pre><code>! we can copy a whole structure\n! with a single assignment\nbest=results(1)\nwrite(*,*)\"best result \",best\n</code></pre>"},{"location":"Documentation/languages/fortran/f90/#using-the-user-defined-operator","title":"Using the user defined operator","text":"<pre><code>! using the user defined operator to see if best is worst\n! recall that the operator .converged. checks to see if %index matches\nworst=&gt;pntmin(results)\nwrite(*,*)\"worst result \",worst\nwrite(*,*)\"converged=\",(best .converged. worst)\n</code></pre>"},{"location":"Documentation/languages/fortran/f90/#passing-arrays-with-a-given-arbitrary-lower-bounds","title":"Passing arrays with a given arbitrary lower bounds","text":"<ul> <li> <p>Motivation</p> <ul> <li> <p>Default lower bound within a subroutine is 1</p> </li> <li> <p>May want to use a different lower bound</p> </li> </ul> </li> </ul> <pre><code>    if(allocated(z))deallocate(z)\nallocate(z(-10:10)) ! a 21 element array\ndo j=-10,10\nz(j)=j\nenddo ! pass z and its lower bound\n! in this routine we give the array a specific lower\n! bound and show how to use a pointer to reference\n! different parts of an array using different indices\ncall boink1(z,lbound(z,1)) ! why not just lbound(z) instead of lbound(z,1)?\n! lbound(z) returns a rank 1 array\nsubroutine boink1(a,n)\nuse numz\nimplicit none\ninteger,intent(in) :: n\nreal(b8),dimension(n:):: a ! this is how we set lower bounds in a subroutine\nwrite(*,*)lbound(a),ubound(a)\nend subroutine\n</code></pre>"},{"location":"Documentation/languages/fortran/f90/#warning-because-we-are-using-an-assumed-shape-array-we-need-an-interface","title":"Warning:  because we are using an assumed shape array we need an interface","text":""},{"location":"Documentation/languages/fortran/f90/#using-pointers-to-access-sections-of-arrays","title":"Using pointers to access sections of arrays","text":"<ul> <li>Motivation<ul> <li>Can increase efficiency</li> <li>Can increase readability</li> </ul> </li> </ul> <pre><code>call boink2(z,lbound(z,1))\n\nsubroutine boink2(a,n)\nuse numz\nimplicit none\ninteger,intent(in) :: n\nreal(b8),dimension(n:),target:: a\nreal(b8),dimension(:),pointer::b\nb=&gt;a(n:) ! b(1) \"points\" to a(-10)\nwrite(*,*)\"a(-10) =\",a(-10),\"b(1) =\",b(1)\nb=&gt;a(0:) ! b(1) \"points\" to a(0)\nwrite(*,*)\"a(-6) =\",a(-6),\"b(-5) =\",b(-5)\nend subroutine\n</code></pre>"},{"location":"Documentation/languages/fortran/f90/#allocating-an-array-inside-a-subroutine-and-passing-it-back","title":"Allocating an array inside a subroutine and passing it back","text":"<ul> <li>Motivation<ul> <li>Size of arrays are calculated in the subroutine</li> </ul> </li> </ul> <pre><code>module numz\ninteger, parameter:: b8 = selected_real_kind(14)\nend module\nprogram bla\nuse numz\nreal(b8), dimension(:) ,pointer :: xyz\ninterface boink\nsubroutine boink(a)\nuse numz\nimplicit none\nreal(b8), dimension(:), pointer :: a\nend subroutine\n   end interface\n   nullify(xyz) ! nullify sets a pointer to null\nwrite(*,'(l5)')associated(xyz) ! is a pointer null, should be\ncall boink(xyz)\nwrite(*,'(l5)',advance=\"no\")associated(xyz)\nif(associated(xyz))write(*,'(i5)')size(xyz)\nend program\nsubroutine boink(a)\nuse numz\nimplicit none\nreal(b8),dimension(:),pointer:: a\nif(associated(a))deallocate(a)\nallocate(a(10))\nend subroutine\n</code></pre>"},{"location":"Documentation/languages/fortran/f90/#an-example-run_1","title":"An example run","text":"<pre><code>     F\n     T\n10\n</code></pre>"},{"location":"Documentation/languages/fortran/f90/#our-fitness-function","title":"Our fitness function","text":"<p>Given a fixed number of colors, M, and a description of a map of a collection of  N states.</p> <p>Find a coloring of the map such that no two states that share a boarder have the same coloring.</p>"},{"location":"Documentation/languages/fortran/f90/#example-input-is-a-sorted-list-of-22-western-states","title":"Example input is a sorted list of 22 western states","text":"<pre><code>22\nar ok tx la mo xx\naz ca nm ut nv xx\nca az nv or xx\nco nm ut wy ne ks xx\nia mo ne sd mn xx\nid wa or nv ut wy mt xx\nks ne co ok mo xx\nla tx ar xx\nmn ia sd nd xx\nmo ar ok ks ne ia xx\nmt wy id nd xx\nnd mt sd wy xx\nne sd wy co ks mo ia xx\nnm az co ok tx mn xx\nnv ca or id ut az xx\nok ks nm tx ar mo xx\nor ca wa id xx\nsd nd wy ne ia mn xx\ntx ok nm la ar xx\nut nv az co wy id xx\nwa id or mt xx\nwy co mt id ut nd sd ne xx\n</code></pre> <p>Our fitness function takes a potential coloring, that is, an integer vector of length N and a returns the number of boarders that have states of the same coloring</p> <ul> <li>How do we represent the map in memory?<ul> <li>One way would be to use an array but it would be very sparse</li> <li>Linked lists are often a better way</li> </ul> </li> </ul>"},{"location":"Documentation/languages/fortran/f90/#linked-lists","title":"Linked lists","text":"<ul> <li> <p>Motivation</p> <ul> <li>We have a collection of states and for each state a list of adjoining states. (Do not count a boarder twice.)</li> <li>Problem is that you do not know the length of the list until runtime.</li> <li> <p>List of adjoining states will be different lengths for different states</p> </li> <li> <p>Solution         -   Linked list are a good way to handle such situations</p> </li> <li>Linked lists use a derived data type with at least two components<ul> <li>Data  </li> <li>Pointer to next element</li> </ul> </li> </ul> </li> </ul> <pre><code>module list_stuff\ntype llist\ninteger index ! data\ntype(llist),pointer::next ! pointer to the\n! next element\nend type llist\nend module\n</code></pre>"},{"location":"Documentation/languages/fortran/f90/#linked-list-usage","title":"Linked list usage","text":"<p>One way to fill a linked list is to use a recursive function <code>`fortran</code> recursive subroutine insert (item, root) use list_stuff implicit none type(llist), pointer :: root integer item if (.not. associated(root)) then allocate(root) nullify(root%next) root%index = item else call insert(item,root%next) endif end subroutine <pre><code>- - -\n- - -\n\n## Our map representation\n- An array of the derived data type states\n            -   State is name of a state\n    -   Linked list containing boarders\n\n```fortran\n    type states\n        character(len=2)name\n        type(llist),pointer:: list\n    end type states\n</code></pre> - Notes:     -  We have an array of linked lists             -  This data structure is often used to represent sparse arrays                 -  We could have a linked list of linked lists     -  State name is not really required</p>"},{"location":"Documentation/languages/fortran/f90/#date-and-time-functions","title":"Date and time functions","text":"<ul> <li> <p>Motivation</p> <ul> <li> <p>May want to know the date and time of your program</p> </li> <li> <p>Two functions</p> </li> </ul> </li> </ul> <pre><code>! all arguments are optional\ncall date_and_time(date=c_date, &amp;  ! character(len=8) ccyymmdd\ntime=c_time, &amp;  ! character(len=10) hhmmss.sss\nzone=c_zone, &amp;  ! character(len=10) +/-hhmm (time zone)\nvalues=ivalues) ! integer ivalues(8) all of the above\ncall system_clock(count=ic,           &amp; ! count of system clock (clicks)\ncount_rate=icr,     &amp; ! clicks / second\ncount_max=max_c)      ! max value for count\n</code></pre>"},{"location":"Documentation/languages/fortran/f90/#non-advancing-and-character-io","title":"Non advancing and character IO","text":"<ul> <li> <p>Motivation</p> <ul> <li> <p>We read the states using the two character identification</p> </li> <li> <p>One line per state and do not know how many boarder states per line</p> </li> </ul> </li> <li> <p>Note: Our list of states is presorted <pre><code>character(len=2) a ! we have a character variable of length 2\nread(12,*)nstates ! read the number of states\nallocate(map(nstates)) ! and allocate our map\ndo i=1,nstates\nread(12,\"(a2)\",advance=\"no\")map(i)%name ! read the name\n!write(*,*)\"state:\",map(i)%name\nnullify(map(i)%list) ! \"zero out\" our list\ndo\n        read(12,\"(1x,a2)\",advance=\"no\")a ! read list of states\n! without going to the\n! next line\nif(lge(a,\"xx\") .and. lle(a,\"xx\"))then ! if state == xx\nbackspace(12) ! go to the next line\nread(12,\"(1x,a2)\",end=1)a ! go to the next line\nexit\n        endif\n1 continue\n        if(llt(a,map(i)%name))then ! we only add a state to\n! our list if its name\n! is before ours thus we\n! only count boarders 1 time\n! what we want put into our linked list is an index\n! into our map where we find the bordering state\n! thus we do the search here\n! any ideas on a better way of doing this search?\nfound=-1\ndo j=1,i-1\nif(lge(a,map(j)%name) .and. lle(a,map(j)%name))then\n!write(*,*)a\nfound=j\nexit\n            endif\n        enddo\n        if(found == -1)then\n            write(*,*)\"error\"\nstop\n        endif\n! found the index of the boarding state insert it into our list\n! note we do the insert into the linked list for a particular state\ncall insert(found,map(i)%list)\nendif\n    enddo\nenddo\n</code></pre></p> </li> </ul>"},{"location":"Documentation/languages/fortran/f90/#internal-io","title":"Internal IO","text":"<ul> <li> <p>Motivation</p> <ul> <li> <p>May need to create strings on the fly</p> </li> <li> <p>May need to convert from strings to reals and integers</p> </li> <li> <p>Similar to sprintf and sscanf</p> </li> </ul> </li> <li> <p>How it works</p> <ul> <li> <p>Create a string</p> </li> <li> <p>Do a normal write except write to the string instead of file number</p> </li> </ul> </li> <li> <p>Example 1: creating a date and time stamped file name</p> </li> </ul> <pre><code>character (len=12)tmpstr\n\nwrite(tmpstr,\"(a12)\")(c_date(5:8)//c_time(1:4)//\".dat\") ! // does string concatination\nwrite(*,*)\"name of file= \",tmpstr\nopen(14,file=tmpstr)\nname of file= 03271114.dat\n</code></pre> <ul> <li>Example 2: Creating a format statement at run time (array of integers and a real)</li> </ul> <p><pre><code>! test_vect is an array that we do not know its length until run time\nnstate=9 ! the size of the array\nwrite(fstr,'(\"(\",i4,\"i1,1x,f10.5)\")')nstates\nwrite(*,*)\"format= \",fstr\nwrite(*,fstr)test_vect,fstr\nformat= ( 9i1,1x,f10.5)\n</code></pre> Any other ideas for writing an array when you do not know its length?</p> <ul> <li>Example 3: Reading from a string <pre><code>integer ht,minut,sec\nread(c_time,\"(3i2)\")hr,minut,sec\n</code></pre></li> </ul>"},{"location":"Documentation/languages/fortran/f90/#inquire-function","title":"Inquire function","text":"<ul> <li>Motivation<ul> <li>Need to get information about I/O</li> </ul> </li> <li> <p>Inquire statement has two forms</p> <ul> <li>Information about files (23 different requests can be done)</li> <li>Information about space required for binary output of a value</li> </ul> </li> <li> <p>Example: find the size of your real relative to the \"standard\" real</p> <ul> <li>Useful for inter language programming</li> <li>Useful for determining data types in MPI (MPI_REAL or MPI_DOUBLE_PRECISION)</li> </ul> </li> </ul> <pre><code>inquire(iolength=len_real)1.0\ninquire(iolength=len_b8)1.0_b8\nwrite(*,*)\"len_b8 \",len_b8\nwrite(*,*)\"len_real\",len_real\niratio=len_b8/len_real\nselect case (iratio)\ncase (1)\nmy_mpi_type=mpi_real\ncase(2)\nmy_mpi_type=mpi_double_precision\ncase default\nwrite(*,*)\"type undefined\"\nmy_mpi_type=0\nend select\n</code></pre>"},{"location":"Documentation/languages/fortran/f90/#an-example-run_2","title":"An example run","text":"<pre><code>len_b8 2\nlen_real 1\n</code></pre>"},{"location":"Documentation/languages/fortran/f90/#namelist","title":"Namelist","text":"<ul> <li>Now part of the standard</li> <li>Motivation<ul> <li>A convenient method of doing I/O</li> <li>Good for cases where you have similar runs but change one or two variables</li> <li>Good for formatted output</li> </ul> </li> <li> <p>Notes:</p> <ul> <li>A little flaky</li> <li>No options for overloading format</li> </ul> </li> <li> <p>Example: <pre><code>integer ncolor\nlogical force\nnamelist /the_input/ncolor,force\nncolor=4\nforce=.true.\nread(13,the_input)\nwrite(*,the_input)\n</code></pre> On input: <pre><code>&amp; THE_INPUT NCOLOR=4,FORCE = F /\n</code></pre> Output is <pre><code>&amp;THE_INPUT\nNCOLOR = 4,\nFORCE = F\n/\n</code></pre></p> </li> </ul>"},{"location":"Documentation/languages/fortran/f90/#vector-valued-functions","title":"Vector valued functions","text":"<ul> <li>Motivation<ul> <li>May want a function that returns a vector</li> </ul> </li> <li> <p>Notes</p> <ul> <li>Again requires an interface</li> <li>Use explicit or assumed size array</li> <li>Do not return a pointer to a vector unless you really want a pointer</li> </ul> </li> <li> <p>Example:</p> <ul> <li>Take an integer input vector which represents an integer in some base and     add 1</li> <li>Could be used in our program to find a \"brute force\" solution</li> </ul> </li> </ul> <pre><code>  function add1(vector,max) result (rtn)\ninteger, dimension(:),intent(in) ::  vector\ninteger,dimension(size(vector)) :: rtn\ninteger max\ninteger len\nlogical carry\nlen=size(vector)\nrtn=vector\ni=0\ncarry=.true.\ndo while(carry)         ! just continue until we do not do a carry\ni=i+1\nrtn(i)=rtn(i)+1\nif(rtn(i) .gt. max)then\n       if(i == len)then   ! role over set everything back to 0\nrtn=0\nelse\nrtn(i)=0\nendif\n   else\ncarry=.false.\nendif\n  enddo\nend function\n</code></pre>"},{"location":"Documentation/languages/fortran/f90/#usage","title":"Usage","text":"<pre><code>test_vect=0\ndo\ntest_vect=add1(test_vect,3)\nresult=fitness(test_vect)\nif(result .lt. 1.0_b8)then\n               write(*,*)test_vect\nstop\n           endif\n        enddo\n</code></pre>"},{"location":"Documentation/languages/fortran/f90/#complete-source-for-recent-discussions","title":"Complete source for recent discussions","text":"<ul> <li>recent.f90</li> <li>fort.13</li> </ul> <p>Exersize 5  Modify the program to use the random number generator given earlier.</p>"},{"location":"Documentation/languages/fortran/f90/#some-array-specific-intrinsic-functions","title":"Some array specific intrinsic functions","text":"<ul> <li>ALL True if all values are true (LOGICAL)</li> <li>ANY True if any value is true (LOGICAL)</li> <li>COUNT Number of true elements in an array (LOGICAL)</li> <li>DOT_PRODUCT Dot product of two rank one arrays</li> <li>MATMUL Matrix multiplication</li> <li>MAXLOC Location of a maximum value in an array</li> <li>MAXVAL Maximum value in an array</li> <li>MINLOC Location of a minimum value in an array</li> <li>MINVAL Minimum value in an array</li> <li>PACK Pack an array into an array of rank one</li> <li>PRODUCT Product of array elements</li> <li>RESHAPE  Reshape an array</li> <li>SPREAD Replicates array by adding a dimension</li> <li>SUM Sum of array elements</li> <li>TRANSPOSE Transpose an array of rank two</li> <li> <p>UNPACK Unpack an array of rank one into an array under a mask</p> </li> <li> <p>Examples</p> </li> </ul> <pre><code>program matrix\nreal w(10),x(10),mat(10,10)\ncall random_number(w)\ncall random_number(mat)\nx=matmul(w,mat)   ! regular matrix multiply  USE IT\nwrite(*,'(\"dot(x,x)=\",f10.5)'),dot_product(x,x)\nend program\nprogram allit\ncharacter(len=10):: f1=\"(3l1)\"\ncharacter(len=10):: f2=\"(3i2)\"\ninteger b(2,3),c(2,3),one_d(6)\nlogical l(2,3)\none_d=(/ 1,3,5 , 2,4,6 /)\nb=transpose(reshape((/ 1,3,5 , 2,4,6 /),shape=(/3,2/)))\nC=transpose(reshape((/ 0,3,5 , 7,4,8 /),shape=(/3,2/)))\nl=(b.ne.c)\nwrite(*,f2)((b(i,j),j=1,3),i=1,2)\nwrite(*,*)\nwrite(*,f2)((c(i,j),j=1,3),i=1,2)\nwrite(*,*)\nwrite(*,f1)((l(i,j),j=1,3),i=1,2)\nwrite(*,*)\nwrite(*,f1)all ( b .ne. C ) !is .false.\nwrite(*,f1)all ( b .ne. C, DIM=1) !is [.true., .false., .false.]\nwrite(*,f1)all ( b .ne. C, DIM=2) !is [.false., .false.]\nend\n</code></pre> <ul> <li>The output is:</li> </ul> <pre><code> 1 3 5\n 2 4 6\n 0 3 5\n 7 4 8\n TFF\n TFT\n F\n TFF\n FF\n</code></pre>"},{"location":"Documentation/languages/fortran/f90/#the-rest-of-our-ga","title":"The rest of our GA","text":"<ul> <li>Includes</li> <li>Reproduction</li> <li>Mutation</li> <li>Nothing new in either of these files</li> <li>Source and makefile \"git\"</li> <li>Source and makefile \"*tgz\"</li> </ul>"},{"location":"Documentation/languages/fortran/f90/#compiler-information","title":"Compiler Information","text":""},{"location":"Documentation/languages/fortran/f90/#gfortran","title":"gfortran","text":"<ul> <li>.f, .for, .ftn .f77<ul> <li>fixed-format Fortran source; compile</li> </ul> </li> <li>.f90, .f95<ul> <li>free-format Fortran source; compile</li> </ul> </li> <li>-fbacktrace<ul> <li>Add debug information for runtime traceback</li> </ul> </li> <li>-ffree-form -ffixed-form<ul> <li>source form</li> </ul> </li> <li>-O0, -O1, -O2, -O3<ul> <li>optimization level</li> </ul> </li> <li>.fpp, .FPP,  .F, .FOR, .FTN, .F90, .F95, .F03 or .F08<ul> <li>Fortran source file with preprocessor directives</li> </ul> </li> <li>-fopenmp<ul> <li>turn on OpenMP</li> </ul> </li> </ul>"},{"location":"Documentation/languages/fortran/f90/#intel","title":"Intel","text":"<ul> <li>.f, .for, .ftn<ul> <li>fixed-format Fortran source; compile</li> </ul> </li> <li>.f90, .f95<ul> <li>free-format Fortran source; compile</li> </ul> </li> <li>-O0, -O1, -O2, -O3, -O4<ul> <li>optimization level</li> </ul> </li> <li>.fpp, .F, .FOR, .FTN, .FPP, .F90<ul> <li>Fortran source file with preprocessor directives</li> </ul> </li> <li>-g<ul> <li>compile for debug     * -traceback -notraceback (default)</li> <li>Add debug information for runtime traceback</li> </ul> </li> <li>-nofree, -free<ul> <li>Source is fixed or free format</li> </ul> </li> <li>-fopenmp<ul> <li>turn on OpenMP</li> </ul> </li> </ul>"},{"location":"Documentation/languages/fortran/f90/#portland-group-x86","title":"Portland Group (x86)","text":"<ul> <li>.f, .for, .ftn<ul> <li>fixed-format Fortran source; compile</li> </ul> </li> <li>.f90, .f95, .f03<ul> <li>free-format Fortran source; compile</li> </ul> </li> <li>.cuf<ul> <li>free-format CUDA Fortran source; compile</li> </ul> </li> <li> <p>.CUF</p> <ul> <li>free-format CUDA Fortran source; preprocess, compile</li> </ul> </li> <li> <p>-O0, -O1, -O2, -O3, -O4</p> <ul> <li>optimization level</li> </ul> </li> <li> <p>-g</p> <ul> <li>compile for debug       *    -traceback (default) -notraceback</li> <li>Add debug information for runtime traceback</li> </ul> </li> <li>-Mfixed, -Mfree<ul> <li>Source is fixed or free format</li> </ul> </li> <li>-qmp<ul> <li>turn on OpenMP</li> </ul> </li> </ul>"},{"location":"Documentation/languages/fortran/f90/#ibm-xlf","title":"IBM xlf","text":"<ul> <li>xlf, xlf_r, f77, fort77<ul> <li>Compile FORTRAN 77 source files.  _r = thread safe</li> </ul> </li> <li>xlf90, xlf90_r, f90<ul> <li>Compile Fortran 90 source files.  _r = thread safe</li> </ul> </li> <li>xlf95, xlf95_r, f95<ul> <li>Compile Fortran 95 source files.  _r = thread safe</li> </ul> </li> <li>xlf2003, xlf2003_r,f2003  * Compile Fortran 2003 source files. _r = thread safe</li> <li>xlf2008, xlf2008_r, f2008     * Compile Fortran 2008 source files.</li> <li>.f, .f77, .f90, .f95, .f03, .f08<ul> <li>Fortran source file</li> </ul> </li> <li>.F, .F77, .F90, .F95,  .F03, .F08<ul> <li>Fortran source file with preprocessor directives</li> </ul> </li> <li>-qtbtable=full<ul> <li>Add debug information for runtime traceback</li> </ul> </li> <li>-qsmp=omp<ul> <li>turn on OpenMP</li> </ul> </li> <li>-O0, -O1, -O2, -O3, -O4, O5<ul> <li>optimization level</li> </ul> </li> <li>-g , g0, g1,...g9<ul> <li>compile for debug     </li> </ul> </li> </ul>"},{"location":"Documentation/languages/fortran/f90/#summary_1","title":"Summary","text":"<ul> <li> <p>Fortran 90 has features to:</p> <ul> <li>Enhance performance</li> <li>Enhance portability</li> <li>Enhance reliability</li> <li>Enhance maintainability</li> </ul> </li> <li> <p>Fortran 90 has new language elements</p> <ul> <li>Source form</li> <li>Derived data types</li> <li>Dynamic memory allocation functions</li> <li>Kind facility for portability and easy modification</li> <li>Many new intrinsic function</li> <li>Array assignments</li> </ul> </li> <li> <p>Examples</p> <ul> <li>Help show how things work</li> <li>Reference for future use</li> </ul> </li> </ul>"},{"location":"Documentation/languages/fortran/f90/#overview-of-f90","title":"Overview of F90","text":"<ol> <li>Introduction to Fortran Language</li> <li>Meta language used in this compact summary</li> <li>Structure of files that can be compiled</li> <li>Executable Statements and Constructs</li> <li>Declarations</li> <li>Key words - other than I/O</li> <li>Key words related to I/O</li> <li>Operators</li> <li>Constants</li> <li>Input/Output Statements</li> <li>Formats</li> <li>Intrinsic Functions</li> </ol>"},{"location":"Documentation/languages/fortran/f90/#introduction-to-fortran-language","title":"Introduction to Fortran Language","text":"<pre><code>  Brought to you by ANSI committee X3J3 and ISO-IEC/JTC1/SC22/WG5 (Fortran)\n  This is neither complete nor precisely accurate, but hopefully, after\n  a small investment of time it is easy to read and very useful.\n\n  This is the free form version of Fortran, no statement numbers,\n  no C in column 1, start in column 1 (not column 7),\n  typically indent 2, 3, or 4 spaces per each structure.\n  The typical extension is  .f90  .\n\n  Continue a statement on the next line by ending the previous line with\n  an ampersand  &amp;amp; .  Start the continuation with  &amp;amp;  for strings.\n\n  The rest of any line is a comment starting with an exclamation mark  ! .\n\n  Put more than one statement per line by separating statements with a\n  semicolon  ; . Null statements are OK, so lines can end with semicolons.\n\n  Separate words with space or any form of \"white space\" or punctuation.\n</code></pre>"},{"location":"Documentation/languages/fortran/f90/#meta-language-used-in-this-compact-summary","title":"Meta language used in this compact summary","text":"<pre><code>  &lt;xxx&gt; means fill in something appropriate for xxx and do not type\n        the  \"&lt;\"  or  \"&gt;\" .\n\n  ...  ellipsis means the usual, fill in something, one or more lines\n\n  [stuff] means supply nothing or at most one copy of \"stuff\"\n          [stuff1 [stuff2]] means if \"stuff1\" is included, supply nothing\n          or at most one copy of stuff2.\n\n  \"old\" means it is in the language, like almost every feature of past\n  Fortran standards, but should not be used to write new programs.\n</code></pre>"},{"location":"Documentation/languages/fortran/f90/#structure-of-files-that-can-be-compiled","title":"Structure of files that can be compiled","text":"<pre><code>  program &lt;name&gt;                  usually file name is  &lt;name&gt;.f90\n    use &lt;module_name&gt;             bring in any needed modules\n    implicit none                 good for error detection\n    &lt;declarations&gt;\n    &lt;executable statements&gt;       order is important, no more declarations\n  end program &lt;name&gt;\n\n\n  block data &lt;name&gt;               old\n    &lt;declarations&gt;                common, dimension, equivalence now obsolete\n  end block data &lt;name&gt;\n\n\n  module &lt;name&gt;                   bring back in with   use &lt;name&gt;\n    implicit none                 good for error detection\n    &lt;declarations&gt;                can have private and public and interface\n  end module &lt;name&gt;\n\n  subroutine &lt;name&gt;               use:  call &lt;name&gt;   to execute\n    implicit none                 good for error detection\n    &lt;declarations&gt;\n    &lt;executable statements&gt;\n  end subroutine &lt;name&gt;\n\n\n  subroutine &lt;name&gt;(par1, par2, ...) \n                                  use:  call &lt;name&gt;(arg1, arg2,... ) to execute\n    implicit none                 optional, good for error detection\n    &lt;declarations&gt;                par1, par2, ... are defined in declarations \n                                  and can be specified in, inout, pointer, etc.\n    &lt;executable statements&gt;\n    return                        optional, end causes automatic return\n    entry &lt;name&gt; (par...)         old, optional other entries\n  end subroutine &lt;name&gt;\n\n\n  function &lt;name&gt;(par1, par2, ...) result(&lt;rslt&gt;)\n                                  use: &lt;name&gt;(arg1, arg2, ... argn) as variable\n    implicit none                 optional, good for error detection\n    &lt;declarations&gt;                rslt, par1, ... are defined in declarations\n    &lt;executable statements&gt;\n    &lt;rslt&gt; = &lt;expression&gt;         required somewhere in execution\n    [return]                      optional, end causes automatic return\n  end function &lt;name&gt;\n\n                                  old\n  &lt;type&gt; function(...) &lt;name&gt;     use: &lt;name&gt;(arg1, arg2, ... argn) as variable\n    &lt;declarations&gt;\n    &lt;executable statements&gt;\n    &lt;name&gt; = &lt;expression&gt;         required somewhere in execution\n    [return]                      optional, end causes automatic return\n  end function &lt;name&gt;\n</code></pre>"},{"location":"Documentation/languages/fortran/f90/#executable-statements-and-constructs","title":"Executable Statements and Constructs","text":"<pre><code>  &lt;statement&gt; will mean exactly one statement in this section\n\n  a construct is multiple lines\n\n  &lt;label&gt; : &lt;statement&gt;      any statement can have a label (a name)\n\n  &lt;variable&gt; = &lt;expression&gt;  assignment statement\n\n  &lt;pointer&gt;  &gt;= &lt;variable&gt;   the pointer is now an alias for the variable\n  &lt;pointer1&gt; &gt;= &lt;pointer2&gt;    pointer1 now points same place as pointer2\n\n  stop                       can be in any executable statement group,\n  stop &lt;integer&gt;             terminates execution of the program,\n  stop &lt;string&gt;              can have optional integer or string\n\n  return                     exit from subroutine or function\n\n  do &lt;variable&gt;=&lt;from&gt;,&lt;to&gt; [,&lt;increment&amp;gt]   optional:  &lt;label&gt; : do ...\n     &lt;statements&gt;\n\n     exit                                   \\_optional   or exit &lt;label&amp;gt\n     if (&lt;boolean expression&gt;) exit         /\n                                            exit the loop\n     cycle                                  \\_optional   or cycle &lt;label&gt;\n     if (&lt;boolean expression&gt;) cycle        /\n                                            continue with next loop iteration\n  end do                                    optional:    end do &lt;name&gt;\n\n\n  do while (&lt;boolean expression&gt;)\n     ...                                   optional exit and cycle allowed\n  end do\n\n\n  do\n     ...                                   exit required to end the loop\n                                           optional  cycle  can be used\n  end do\n\n\n\n  if ( &lt;boolean expression&gt; ) &lt;statement&gt;  execute the statement if the\n                                           boolean expression is true\n\n  if ( &lt;boolean expression1&gt; ) then\n    ...                                    execute if expression1 is true\n  else if ( &lt;boolean expression2&gt; ) then\n    ...                                    execute if expression2 is true\n  else if ( &lt;boolean expression3&gt; ) then\n    ...                                    execute if expression3 is true\n  else\n    ...                                    execute if none above are true\n  end if\n\n\n  select case (&lt;expression&gt;)            optional &lt;name&gt; : select case ...\n     case (&lt;value&gt;)\n        &lt;statements&gt;                    execute if expression == value\n     case (&lt;value1&gt;:&lt;value2&gt;)           \n        &lt;statements&gt;                    execute if value1 &amp;le; expression &amp;le; value2\n     ...\n     case default\n        &lt;statements&gt;                    execute if no values above match\n  end select                            optional  end select &lt;name&gt;\n\n\n  real, dimension(10,12) :: A, R     a sample declaration for use with \"where\"\n    ...\n  where (A /= 0.0)                   conditional assignment, only assignment allowed\n     R = 1.0/A\n  elsewhere\n     R = 1.0                         elements of R set to 1.0 where A == 0.0\n  end where\n\n    go to &lt;statement number&gt;          old\n\n    go to (&lt;statement number list&gt;), &lt;expression&gt;   old\n\n    for I/O statements, see:  section 10.0  Input/Output Statements\n\n    many old forms of statements are not listed\n</code></pre>"},{"location":"Documentation/languages/fortran/f90/#declarations","title":"Declarations","text":"<pre><code>  There are five (5) basic types: integer, real, complex, character and logical.\n  There may be any number of user derived types.  A modern (not old) declaration\n  starts with a type, has attributes, then ::, then variable(s) names\n\n  integer i, pivot, query                             old\n\n  integer, intent (inout) :: arg1\n\n  integer (selected_int_kind (5)) :: i1, i2\n\n  integer, parameter :: m = 7\n\n  integer, dimension(0:4, -5:5, 10:100) :: A3D\n\n  double precision x                                 old\n\n  real  (selected_real_kind(15,300) :: x\n\n  complex :: z\n\n  logical, parameter :: what_if = .true.\n\n  character, parameter :: me = \"Jon Squire\"\n\n  type &lt;name&gt;       a new user type, derived type\n    declarations\n  end type &lt;name&gt;\n\n  type (&lt;name&gt;) :: stuff    declaring stuff to be of derived type &lt;name&gt;\n\n  real, dimension(:,:), allocatable, target :: A\n\n  real, dimension(:,:), pointer :: P\n\n  Attributes may be:\n\n    allocatable  no memory used here, allocate later\n    dimension    vector or multi dimensional array\n    external     will be defined outside this compilation\n    intent       argument may be  in, inout or out\n    intrinsic    declaring function to be an intrinsic\n    optional     argument is optional\n    parameter    declaring a constant, can not be changed later\n    pointer      declaring a pointer\n    private      in a module, a private declaration\n    public       in a module, a public declaration\n    save         keep value from one call to the next, static\n    target       can be pointed to by a pointer\n    Note:        not all combinations of attributes are legal\n</code></pre>"},{"location":"Documentation/languages/fortran/f90/#key-words-other-than-io","title":"Key words (other than I/O)","text":"<pre><code>  note: \"statement\" means key word that starts a statement, one line\n                    unless there is a continuation \"&amp;amp;\"\n        \"construct\" means multiple lines, usually ending with \"end ...\"\n        \"attribute\" means it is used in a statement to further define\n        \"old\"       means it should not be used in new code\n\n  allocatable          attribute, no space allocated here, later allocate\n  allocate             statement, allocate memory space now for variable\n  assign               statement, old, assigned go to\n  assignment           attribute, means subroutine is assignment (=)\n  block data           construct, old, compilation unit, replaced by module\n  call                 statement, call a subroutine\n  case                 statement, used in  select case structure\n  character            statement, basic type, intrinsic data type\n  common               statement, old, allowed overlaying of storage\n  complex              statement, basic type, intrinsic data type\n  contains             statement, internal subroutines and functions follow\n  continue             statement, old, a place to put a statement number\n  cycle                statement, continue the next iteration of a do loop\n  data                 statement, old, initialized variables and arrays\n  deallocate           statement, free up storage used by specified variable\n  default              statement, in a select case structure, all others\n  do                   construct, start a do loop\n  double precision     statement, old, replaced by selected_real_kind(15,300)\n  else                 construct, part of if   else if   else   end if\n  else if              construct, part of if   else if   else   end if\n  elsewhere            construct, part of where  elsewhere  end where\n  end block data       construct, old, ends block data\n  end do               construct, ends do\n  end function         construct, ends function\n  end if               construct, ends if\n  end interface        construct, ends interface\n  end module           construct, ends module\n  end program          construct, ends program\n  end select           construct, ends select case\n  end subroutine       construct, ends subroutine\n  end type             construct, ends type\n  end where            construct, ends where\n  entry                statement, old, another entry point in a procedure\n  equivalence          statement, old, overlaid storage\n  exit                 statement, continue execution outside of a do loop\n  external             attribute, old statement, means defines else where\n  function             construct, starts the definition of a function\n  go to                statement, old, requires fixed form statement number\n  if                   statement and construct, if(...) statement\n  implicit             statement, \"none\" is preferred to help find errors\n  in                   a keyword for intent, the argument is read only\n  inout                a keyword for intent, the argument is read/write\n  integer              statement, basic type, intrinsic data type\n  intent               attribute, intent(in) or intent(out) or intent(inout)\n  interface            construct, begins an interface definition\n  intrinsic            statement, says that following names are intrinsic\n  kind                 attribute, sets the kind of the following variables\n  len                  attribute, sets the length of a character string\n  logical              statement, basic type, intrinsic data type\n  module               construct, beginning of a module definition\n  namelist             statement, defines a namelist of input/output\n  nullify              statement, nullify(some_pointer) now points nowhere\n  only                 attribute, restrict what comes from a module\n  operator             attribute, indicates function is an operator, like +\n  optional             attribute, a parameter or argument is optional\n  out                  a keyword for intent, the argument will be written\n  parameter            attribute, old statement, makes variable real only\n  pause                old, replaced by stop\n  pointer              attribute, defined the variable as a pointer alias\n  private              statement and attribute, in a module, visible inside\n  program              construct, start of a main program\n  public               statement and attribute, in a module, visible outside\n  real                 statement, basic type, intrinsic data type\n  recursive            attribute, allows functions and derived type recursion\n  result               attribute, allows naming of function result  result(Y)\n  return               statement, returns from, exits, subroutine or function\n  save                 attribute, old statement, keep value between calls\n  select case          construct, start of a case construct\n  stop                 statement, terminate execution of the main procedure\n  subroutine           construct, start of a subroutine definition\n  target               attribute, allows a variable to take a pointer alias\n  then                 part of if construct\n  type                 construct, start of user defined type\n  type ( )             statement, declaration of a variable for a users type\n  use                  statement, brings in a module\n  where                construct, conditional assignment\n  while                construct, a while form of a do loop\n</code></pre>"},{"location":"Documentation/languages/fortran/f90/#key-words-related-to-io","title":"Key words related to I/O","text":"<pre><code>  backspace            statement, back up one record\n  close                statement, close a file\n  endfile              statement, mark the end of a file\n  format               statement, old, defines a format\n  inquire              statement, get the status of a unit\n  open                 statement, open or create a file\n  print                statement, performs output to screen\n  read                 statement, performs input\n  rewind               statement, move read or write position to beginning\n  write                statement, performs output\n</code></pre>"},{"location":"Documentation/languages/fortran/f90/#operators","title":"Operators","text":"<pre><code>  **    exponentiation\n  *     multiplication\n  /     division\n  +     addition\n  -     subtraction\n  //    concatenation\n  ==    .eq.  equality\n  /=    .ne.  not equal\n  &lt;     .lt.  less than\n  &gt;     .gt.  greater than\n  &lt;=    .le.  less than or equal\n  &gt;=    .ge.  greater than or equal\n  .not.       complement, negation\n  .and.       logical and\n  .or.        logical or\n  .eqv.       logical equivalence\n  .neqv.      logical not equivalence, exclusive or\n\n  .eq.  ==    equality, old\n  .ne.  /=    not equal. old\n  .lt.  &lt;     less than, old\n  .gt.  &gt;     greater than, old\n  .le.  &lt;=    less than or equal, old\n  .ge.  &gt;=    greater than or equal, old\n\n\n  Other punctuation:\n\n   /  ...  /  used in data, common, namelist and other statements\n   (/ ... /)  array constructor, data is separated by commas\n   6*1.0      in some contexts, 6 copies of 1.0\n   (i:j:k)    in some contexts, a list  i, i+k, i+2k, i+3k, ... i+nk&amp;le;j\n   (:j)       j and all below\n   (i:)       i and all above\n   (:)        undefined or all in range\n</code></pre>"},{"location":"Documentation/languages/fortran/f90/#constants","title":"Constants","text":"<pre><code>  Logical constants:\n\n    .true.      True\n    .false.     False\n\n  Integer constants:\n\n     0    1     -1     123456789\n\n  Real constants:\n\n     0.0   1.0   -1.0    123.456   7.1E+10   -52.715E-30\n\n  Complex constants:\n\n     (0.0, 0.0)    (-123.456E+30, 987.654E-29)\n\n  Character constants:\n\n      \"ABC\"   \"a\"  \"123'abc$%#@!\"    \" a quote \"\" \"\n      'ABC'   'a'  '123\"abc$%#@!'    ' a apostrophe '' '\n\n  Derived type values:\n\n      type name\n        character (len=30) :: last\n        character (len=30) :: first\n        character (len=30) :: middle\n      end type name\n\n      type address\n        character (len=40) :: street\n        character (len=40) :: more\n        character (len=20) :: city\n        character (len=2)  :: state\n        integer (selected_int_kind(5)) :: zip_code\n        integer (selected_int_kind(4)) :: route_code\n      end type address\n\n      type person\n        type (name) lfm\n        type (address) snail_mail\n      end type person\n\n      type (person) :: a_person = person( name(\"Squire\",\"Jon\",\"S.\"), &amp;amp;\n          address(\"106 Regency Circle\", \"\", \"Linthicum\", \"MD\", 21090, 1936))\n\n      a_person%snail_mail%route_code == 1936\n</code></pre>"},{"location":"Documentation/languages/fortran/f90/#inputoutput-statements","title":"Input/Output Statements","text":"<pre><code>    open (&lt;unit number&gt;)\n    open (unit=&lt;unit number&gt;, file=&lt;file name&gt;, iostat=&lt;variable&gt;)\n    open (unit=&lt;unit number&gt;, ... many more, see below )\n\n    close (&lt;unit number&gt;)\n    close (unit=&lt;unit number&gt;, iostat=&lt;variable&gt;,\n           err=&lt;statement number&gt;, status=\"KEEP\")\n\n    read (&lt;unit number&gt;) &lt;input list&gt;\n    read (unit=&lt;unit number&gt;, fmt=&lt;format&gt;, iostat=&lt;variable&gt;,\n          end=&lt;statement number&gt;, err=&lt;statement number&gt;) &lt;input list&gt;\n    read (unit=&lt;unit number&gt;, rec=&lt;record number&gt;) &lt;input list&gt;\n\n    write (&lt;unit number&gt;) &lt;output list&gt;\n    write (unit=&lt;unit number&gt;, fmt=&lt;format&gt;, iostat=&lt;variable&gt;,\n           err=&lt;statement number&gt;) &lt;output list&gt;\n    write (unit=&lt;unit number&gt;, rec=&lt;record number&gt;) &lt;output list&gt;\n\n    print *, &lt;output list&gt;\n\n    print \"(&lt;your format here, use apostrophe, not quote&gt;)\", &lt;output list&gt;\n\n    rewind &lt;unit number&gt;\n    rewind (&lt;unit number&gt;, err=&lt;statement number&gt;)\n\n    backspace &lt;unit number&gt;\n    backspace (&lt;unit number&gt;, iostat=&lt;variable&gt;)\n\n    endfile &lt;unit number&gt;\n    endfile (&lt;unit number&gt;, err=&lt;statement number&gt;, iostat=&lt;variable&gt;)\n\n    inquire ( &lt;unit number&gt;, exists = &lt;variable&gt;)\n    inquire ( file=&lt;\"name\"&gt;, opened = &lt;variable1&gt;, access = &lt;variable2&gt; )\n    inquire ( iolength = &lt;variable&gt; ) x, y, A   ! gives \"recl\" for \"open\"\n\n    namelist /&lt;name&gt;/ &lt;variable list&gt;      defines a name list\n    read(*,nml=&lt;name&gt;)                     reads some/all variables in namelist\n    write(*,nml=&lt;name&gt;)                    writes all variables in namelist\n    &amp;amp;&lt;name&gt; &lt;variable&gt;=&lt;value&gt; ... &lt;variable=value&gt; /  data for namelist read\n\n  Input / Output specifiers\n\n    access   one of  \"sequential\"  \"direct\"  \"undefined\"\n    action   one of  \"read\"  \"write\"  \"readwrite\"\n    advance  one of  \"yes\"  \"no\"  \n    blank    one of  \"null\"  \"zero\"\n    delim    one of  \"apostrophe\"  \"quote\"  \"none\"\n    end      =       &lt;integer statement number&gt;  old\n    eor      =       &lt;integer statement number&gt;  old\n    err      =       &lt;integer statement number&gt;  old\n    exist    =       &lt;logical variable&gt;\n    file     =       &lt;\"file name\"&gt;\n    fmt      =       &lt;\"(format)\"&gt; or &lt;character variable&gt; format\n    form     one of  \"formatted\"  \"unformatted\"  \"undefined\"\n    iolength =       &lt;integer variable, size of unformatted record&gt;\n    iostat   =       &lt;integer variable&gt; 0==good, negative==eof, positive==bad\n    name     =       &lt;character variable for file name&gt;\n    named    =       &lt;logical variable&gt;\n    nml      =       &lt;namelist name&gt;\n    nextrec  =       &lt;integer variable&gt;    one greater than written\n    number   =       &lt;integer variable unit number&gt;\n    opened   =       &lt;logical variable&gt;\n    pad      one of  \"yes\"  \"no\"\n    position one of  \"asis\"  \"rewind\"  \"append\"\n    rec      =       &lt;integer record number&gt;\n    recl     =       &lt;integer unformatted record size&gt;\n    size     =       &lt;integer variable&gt;  number of characters read before eor\n    status   one of  \"old\"  \"new\"  \"unknown\"  \"replace\"  \"scratch\"  \"keep\"\n    unit     =       &lt;integer unit number&gt;\n\n  Individual questions\n    direct      =    &lt;character variable&gt;  \"yes\"  \"no\"  \"unknown\"\n    formatted   =    &lt;character variable&gt;  \"yes\"  \"no\"  \"unknown\"\n    read        =    &lt;character variable&gt;  \"yes\"  \"no\"  \"unknown\"\n    readwrite   =    &lt;character variable&gt;  \"yes\"  \"no\"  \"unknown\"\n    sequential  =    &lt;character variable&gt;  \"yes\"  \"no\"  \"unknown\"\n    unformatted =    &lt;character variable&gt;  \"yes\"  \"no\"  \"unknown\"\n    write       =    &lt;character variable&gt;  \"yes\"  \"no\"  \"unknown\"\n</code></pre>"},{"location":"Documentation/languages/fortran/f90/#formats","title":"Formats","text":"<pre><code>    format                    an explicit format can replace * in any\n                              I/O statement. Include the format in\n                              apostrophes or quotes and keep the parenthesis.\n\n    examples:\n         print \"(3I5,/(2X,3F7.2/))\", &lt;output list&gt;\n         write(6, '(a,E15.6E3/a,G15.2)' ) &lt;output list&gt;\n         read(unit=11, fmt=\"(i4, 4(f3.0,TR1))\" ) &lt;input list&gt;\n\n    A format includes the opening and closing parenthesis.\n    A format consists of format items and format control items separated by comma.\n    A format may contain grouping parenthesis with an optional repeat count.\n\n  Format Items, data edit descriptors:\n\n    key:  w  is the total width of the field   (filled with *** if overflow)\n          m  is the least number of digits in the (sub)field (optional)\n          d  is the number of decimal digits in the field\n          e  is the number of decimal digits in the exponent subfield\n          c  is the repeat count for the format item\n          n  is number of columns\n\n    cAw     data of type character (w is optional)\n    cBw.m   data of type integer with binary base\n    cDw.d   data of type real -- same as E,  old double precision\n    cEw.d   or Ew.dEe  data of type real\n    cENw.d  or ENw.dEe  data of type real  -- exponent a multiple of 3\n    cESw.d  or ESw.dEe  data of type real  -- first digit non zero\n    cFw.d   data of type real  -- no exponent printed\n    cGw.d   or Gw.dEe  data of type real  -- auto format to F or E\n    nH      n characters follow the H,  no list item\n    cIw.m   data of type integer\n    cLw     data of type logical  --  .true.  or  .false.\n    cOw.m   data of type integer with octal base\n    cZw.m   data of type integer with hexadecimal base\n    \"&lt;string&gt;\"  literal characters to output, no list item\n    '&lt;string&gt;'  literal characters to output, no list item\n\n  Format Control Items, control edit descriptors:\n\n    BN      ignore non leading blanks in numeric fields\n    BZ      treat nonleading blanks in numeric fields as zeros\n    nP      apply scale factor to real format items   old\n    S       printing of optional plus signs is processor dependent\n    SP      print optional plus signs\n    SS      do not print optional plus signs\n    Tn      tab to specified column\n    TLn     tab left n columns\n    TRn     tab right n columns\n    nX      tab right n columns\n    /       end of record (implied / at end of all format statements)\n    :       stop format processing if no more list items\n\n  &lt;input list&gt; can be:\n    a variable\n    an array name\n    an implied do   ((A(i,j),j=1,n) ,i=1,m)    parenthesis and commas as shown\n\n    note: when there are more items in the input list than format items, the\n          repeat rules for formats applies.\n\n  &lt;output list&gt; can be:\n    a constant\n    a variable\n    an expression\n    an array name\n    an implied do   ((A(i,j),j=1,n) ,i=1,m)    parenthesis and commas as shown\n\n    note: when there are more items in the output list than format items, the\n          repeat rules for formats applies.\n\n  Repeat Rules for Formats:\n\n    Each format item is used with a list item.  They are used in order.\n    When there are more list items than format items, then the following\n    rule applies:  There is an implied end of record, /, at the closing\n    parenthesis of the format, this is processed.  Scan the format backwards\n    to the first left parenthesis.  Use the repeat count, if any, in front\n    of this parenthesis, continue to process format items and list items.\n\n    Note: an infinite loop is possible\n          print \"(3I5/(1X/))\", I, J, K, L    may never stop\n</code></pre>"},{"location":"Documentation/languages/fortran/f90/#intrinsic-functions","title":"Intrinsic Functions","text":"<pre><code>  Intrinsic Functions are presented in alphabetical order and then grouped\n  by topic.  The function name appears first. The argument(s) and result\n  give an indication of the type(s) of argument(s) and results.\n  [,dim=] indicates an optional argument  \"dim\".\n  \"mask\" must be logical and usually conformable.\n  \"character\" and \"string\" are used interchangeably.\n  A brief description or additional information may appear.\n</code></pre>"},{"location":"Documentation/languages/fortran/f90/#intrinsic-functions-alphabetical","title":"Intrinsic Functions (alphabetical):","text":"<pre><code>    abs(integer_real_complex) result(integer_real_complex)\n    achar(integer) result(character)  integer to character\n    acos(real) result(real)  arccosine  |real| &amp;le; 1.0   0&amp;le;result&amp;le;Pi\n    adjustl(character)  result(character) left adjust, blanks go to back\n    adjustr(character)  result(character) right adjust, blanks to front\n    aimag(complex) result(real)  imaginary part\n    aint(real [,kind=]) result(real)  truncate to integer toward zero\n    all(mask [,dim]) result(logical)  true if all elements of mask are true\n    allocated(array) result(logical)  true if array is allocated in memory\n    anint(real [,kind=]) result(real)  round to nearest integer\n    any(mask [,dim=}) result(logical)  true if any elements of mask are true\n    asin(real) result(real)  arcsine  |real| &amp;le; 1.0   -Pi/2&amp;le;result&amp;le;Pi/2\n    associated(pointer [,target=]) result(logical)  true if pointing\n    atan(real) result(real)  arctangent  -Pi/2&amp;le;result&amp;le;Pi/2 \n    atan2(y=real,x=real) result(real)  arctangent  -Pi&amp;le;result&amp;le;Pi\n    bit_size(integer) result(integer)  size in bits in model of argument\n    btest(i=integer,pos=integer) result(logical)  true if pos has a 1, pos=0..\n    ceiling(real) result(real)  truncate to integer toward infinity\n    char(integer [,kind=]) result(character)  integer to character [of kind]\n    cmplx(x=real [,y=real] [kind=]) result(complex)  x+iy\n    conjg(complex) result(complex)  reverse the sign of the imaginary part\n    cos(real_complex) result(real_complex)  cosine\n    cosh(real) result(real)  hyperbolic cosine\n    count(mask [,dim=]) result(integer)  count of true entries in mask\n    cshift(array,shift [,dim=]) circular shift elements of array, + is right\n    date_and_time([date=] [,time=] [,zone=] [,values=])  y,m,d,utc,h,m,s,milli\n    dble(integer_real_complex) result(real_kind_double)  convert to double\n    digits(integer_real) result(integer)  number of bits to represent model\n    dim(x=integer_real,y=integer_real) result(integer_real) proper subtraction\n    dot_product(vector_a,vector_b) result(integer_real_complex) inner product\n    dprod(x=real,y=real) result(x_times_y_double)  double precision product\n    eoshift(array,shift [,boundary=] [,dim=])  end-off shift using boundary\n    epsilon(real) result(real)  smallest positive number added to 1.0 /= 1.0\n    exp(real_complex) result(real_complex)  e raised to a power\n    exponent(real) result(integer)  the model exponent of the argument\n    floor(real) result(real)  truncate to integer towards negative infinity\n    fraction(real) result(real)  the model fractional part of the argument\n    huge(integer_real) result(integer_real)  the largest model number\n    iachar(character) result(integer)  position of character in ASCII sequence\n    iand(integer,integer) result(integer)  bit by bit logical and\n    ibclr(integer,pos) result(integer)  argument with pos bit cleared to zero\n    ibits(integer,pos,len) result(integer)  extract len bits starting at pos\n    ibset(integer,pos) result(integer)  argument with pos bit set to one\n    ichar(character) result(integer)  pos in collating sequence of character\n    ieor(integer,integer) result(integer)  bit by bit logical exclusive or\n    index(string,substring [,back=])  result(integer)  pos of substring\n    int(integer_real_complex) result(integer)  convert to integer\n    ior(integer,integer) result(integer)  bit by bit logical or\n    ishft(integer,shift) result(integer)  shift bits in argument by shift\n    ishftc(integer, shift) result(integer)  shift circular bits in argument\n    kind(any_intrinsic_type) result(integer)  value of the kind\n    lbound(array,dim) result(integer)  smallest subscript of dim in array\n    len(character) result(integer)  number of characters that can be in argument\n    len_trim(character) result(integer)  length without trailing blanks\n    lge(string_a,string_b) result(logical)  string_a &amp;ge; string_b\n    lgt(string_a,string_b) result(logical)  string_a &gt; string_b\n    lle(string_a,string_b) result(logical)  string_a &amp;le; string_b\n    llt(string_a,string_b) result(logical)  string_a &lt; string_b\n    log(real_complex) result(real_complex)  natural logarithm\n    log10(real) result(real)  logarithm base 10\n    logical(logical [,kind=])  convert to logical\n    matmul(matrix,matrix) result(vector_matrix)  on integer_real_complex_logical\n    max(a1,a2,a3,...) result(integer_real)  maximum of list of values\n    maxexponent(real) result(integer)  maximum exponent of model type\n    maxloc(array [,mask=]) result(integer_vector)  indices in array of maximum\n    maxval(array [,dim=] [,mask=])  result(array_element)  maximum value\n    merge(true_source,false_source,mask) result(source_type)  choose by mask\n    min(a1,a2,a3,...) result(integer-real)  minimum of list of values\n    minexponent(real) result(integer)  minimum(negative) exponent of model type\n    minloc(array [,mask=]) result(integer_vector)  indices in array of minimum\n    minval(array [,dim=] [,mask=])  result(array_element)  minimum value\n    mod(a=integer_real,p) result(integer_real)  a modulo p\n    modulo(a=integer_real,p) result(integer_real)  a modulo p\n    mvbits(from,frompos,len,to,topos) result(integer)  move bits\n    nearest(real,direction) result(real)  nearest value toward direction\n    nint(real [,kind=]) result(real)  round to nearest integer value\n    not(integer) result(integer)  bit by bit logical complement\n    pack(array,mask [,vector=]) result(vector)  vector of elements from array\n    present(argument) result(logical)  true if optional argument is supplied\n    product(array [,dim=] [,mask=]) result(integer_real_complex)  product\n    radix(integer_real) result(integer)  radix of integer or real model, 2\n    random_number(harvest=real_out)  subroutine, uniform random number 0 to 1\n    random_seed([size=] [,put=] [,get=])  subroutine to set random number seed\n    range(integer_real_complex) result(integer_real)  decimal exponent of model\n    real(integer_real_complex [,kind=]) result(real)  convert to real\n    repeat(string,ncopies) result(string)  concatenate n copies of string\n    reshape(source,shape,pad,order) result(array)  reshape source to array\n    rrspacing(real) result(real)  reciprocal of relative spacing of model\n    scale(real,integer) result(real)  multiply by  2**integer\n    scan(string,set [,back]) result(integer)  position of first of set in string\n    selected_int_kind(integer) result(integer)  kind number to represent digits\n    selected_real_kind(integer,integer) result(integer)  kind of digits, exp\n    set_exponent(real,integer) result(real)  put integer as exponent of real\n    shape(array) result(integer_vector)  vector of dimension sizes\n    sign(integer_real,integer_real) result(integer_real) sign of second on first\n    sin(real_complex) result(real_complex)  sine of angle in radians\n    sinh(real) result(real)  hyperbolic sine of argument\n    size(array [,dim=]) result(integer)  number of elements in dimension\n    spacing(real) result(real)  spacing of model numbers near argument\n    spread(source,dim,ncopies) result(array)  expand dimension of source by 1\n    sqrt(real_complex) result(real_complex)  square root of argument\n    sum(array [,dim=] [,mask=]) result(integer_real_complex)  sum of elements\n    system_clock([count=] [,count_rate=] [,count_max=])  subroutine, all out\n    tan(real) result(real)  tangent of angle in radians\n    tanh(real) result(real)  hyperbolic tangent of angle in radians\n    tiny(real) result(real)  smallest positive model representation\n    transfer(source,mold [,size]) result(mold_type)  same bits, new type\n    transpose(matrix) result(matrix)  the transpose of a matrix\n    trim(string) result(string)  trailing blanks are removed\n    ubound(array,dim) result(integer)  largest subscript of dim in array\n    unpack(vector,mask,field) result(v_type,mask_shape)  field when not mask\n    verify(string,set [,back]) result(integer)  pos in string not in set\n</code></pre>"},{"location":"Documentation/languages/fortran/f90/#intrinsic-functions-grouped-by-topic","title":"Intrinsic Functions (grouped by topic):","text":""},{"location":"Documentation/languages/fortran/f90/#intrinsic-functions-numeric","title":"Intrinsic Functions (Numeric)","text":"<pre><code>    abs(integer_real_complex) result(integer_real_complex)\n    acos(real) result(real)  arccosine  |real| &amp;le; 1.0   0&amp;le;result&amp;le;Pi\n    aimag(complex) result(real)  imaginary part\n    aint(real [,kind=]) result(real)  truncate to integer toward zero\n    anint(real [,kind=]) result(real)  round to nearest integer\n    asin(real) result(real)  arcsine  |real| &amp;le; 1.0   -Pi/2&amp;le;result&amp;le;Pi/2\n    atan(real) result(real)  arctangent  -Pi/2&amp;le;result&amp;le;Pi/2 \n    atan2(y=real,x=real) result(real)  arctangent  -Pi&amp;le;result&amp;le;Pi\n    ceiling(real) result(real)  truncate to integer toward infinity\n    cmplx(x=real [,y=real] [kind=]) result(complex)  x+iy\n    conjg(complex) result(complex)  reverse the sign of the imaginary part\n    cos(real_complex) result(real_complex)  cosine\n    cosh(real) result(real)  hyperbolic cosine\n    dble(integer_real_complex) result(real_kind_double)  convert to double\n    digits(integer_real) result(integer)  number of bits to represent model\n    dim(x=integer_real,y=integer_real) result(integer_real) proper subtraction\n    dot_product(vector_a,vector_b) result(integer_real_complex) inner product\n    dprod(x=real,y=real) result(x_times_y_double)  double precision product\n    epsilon(real) result(real)  smallest positive number added to 1.0 /= 1.0\n    exp(real_complex) result(real_complex)  e raised to a power\n    exponent(real) result(integer)  the model exponent of the argument\n    floor(real) result(real)  truncate to integer towards negative infinity\n    fraction(real) result(real)  the model fractional part of the argument\n    huge(integer_real) result(integer_real)  the largest model number\n    int(integer_real_complex) result(integer)  convert to integer\n    log(real_complex) result(real_complex)  natural logarithm\n    log10(real) result(real)  logarithm base 10\n    matmul(matrix,matrix) result(vector_matrix)  on integer_real_complex_logical\n    max(a1,a2,a3,...) result(integer_real)  maximum of list of values\n    maxexponent(real) result(integer)  maximum exponent of model type\n    maxloc(array [,mask=]) result(integer_vector)  indices in array of maximum\n    maxval(array [,dim=] [,mask=])  result(array_element)  maximum value\n    min(a1,a2,a3,...) result(integer-real)  minimum of list of values\n    minexponent(real) result(integer)  minimum(negative) exponent of model type\n    minloc(array [,mask=]) result(integer_vector)  indices in array of minimum\n    minval(array [,dim=] [,mask=])  result(array_element)  minimum value\n    mod(a=integer_real,p) result(integer_real)  a modulo p\n    modulo(a=integer_real,p) result(integer_real)  a modulo p\n    nearest(real,direction) result(real)  nearest value toward direction\n    nint(real [,kind=]) result(real)  round to nearest integer value\n    product(array [,dim=] [,mask=]) result(integer_real_complex)  product\n    radix(integer_real) result(integer)  radix of integer or real model, 2\n    random_number(harvest=real_out)  subroutine, uniform random number 0 to 1\n    random_seed([size=] [,put=] [,get=])  subroutine to set random number seed\n    range(integer_real_complex) result(integer_real)  decimal exponent of model\n    real(integer_real_complex [,kind=]) result(real)  convert to real\n    rrspacing(real) result(real)  reciprocal of relative spacing of model\n    scale(real,integer) result(real)  multiply by  2**integer\n    set_exponent(real,integer) result(real)  put integer as exponent of real\n    sign(integer_real,integer_real) result(integer_real) sign of second on first\n    sin(real_complex) result(real_complex)  sine of angle in radians\n    sinh(real) result(real)  hyperbolic sine of argument\n    spacing(real) result(real)  spacing of model numbers near argument\n    sqrt(real_complex) result(real_complex)  square root of argument\n    sum(array [,dim=] [,mask=]) result(integer_real_complex)  sum of elements\n    tan(real) result(real)  tangent of angle in radians\n    tanh(real) result(real)  hyperbolic tangent of angle in radians\n    tiny(real) result(real)  smallest positive model representation\n    transpose(matrix) result(matrix)  the transpose of a matrix\n</code></pre>"},{"location":"Documentation/languages/fortran/f90/#intrinsic-functions-logical-and-bit","title":"Intrinsic Functions (Logical and bit)","text":"<pre><code>    all(mask [,dim]) result(logical)  true if all elements of mask are true\n    any(mask [,dim=}) result(logical)  true if any elements of mask are true\n    bit_size(integer) result(integer)  size in bits in model of argument\n    btest(i=integer,pos=integer) result(logical)  true if pos has a 1, pos=0..\n    count(mask [,dim=]) result(integer)  count of true entries in mask\n    iand(integer,integer) result(integer)  bit by bit logical and\n    ibclr(integer,pos) result(integer)  argument with pos bit cleared to zero\n    ibits(integer,pos,len) result(integer)  extract len bits starting at pos\n    ibset(integer,pos) result(integer)  argument with pos bit set to one\n    ieor(integer,integer) result(integer)  bit by bit logical exclusive or\n    ior(integer,integer) result(integer)  bit by bit logical or\n    ishft(integer,shift) result(integer)  shift bits in argument by shift\n    ishftc(integer, shift) result(integer)  shift circular bits in argument\n    logical(logical [,kind=])  convert to logical\n    matmul(matrix,matrix) result(vector_matrix)  on integer_real_complex_logical\n    merge(true_source,false_source,mask) result(source_type)  choose by mask\n    mvbits(from,frompos,len,to,topos) result(integer)  move bits\n    not(integer) result(integer)  bit by bit logical complement\n    transfer(source,mold [,size]) result(mold_type)  same bits, new type\n</code></pre>"},{"location":"Documentation/languages/fortran/f90/#intrinsic-functions-character-or-string","title":"Intrinsic Functions (Character or string)","text":"<pre><code>    achar(integer) result(character)  integer to character\n    adjustl(character)  result(character) left adjust, blanks go to back\n    adjustr(character)  result(character) right adjust, blanks to front\n    char(integer [,kind=]) result(character)  integer to character [of kind]\n    iachar(character) result(integer)  position of character in ASCII sequence\n    ichar(character) result(integer)  pos in collating sequence of character\n    index(string,substring [,back=])  result(integer)  pos of substring\n    len(character) result(integer)  number of characters that can be in argument\n    len_trim(character) result(integer)  length without trailing blanks\n    lge(string_a,string_b) result(logical)  string_a &amp;ge; string_b\n    lgt(string_a,string_b) result(logical)  string_a &gt; string_b\n    lle(string_a,string_b) result(logical)  string_a &amp;le; string_b\n    llt(string_a,string_b) result(logical)  string_a &lt; string_b\n    repeat(string,ncopies) result(string)  concatenate n copies of string\n    scan(string,set [,back]) result(integer)  position of first of set in string\n    trim(string) result(string)  trailing blanks are removed\n    verify(string,set [,back]) result(integer)  pos in string not in set\n</code></pre>"},{"location":"Documentation/languages/fortran/f90/#fortran-95","title":"Fortran 95","text":"<ul> <li>New Features<ul> <li>The statement FORALL as an alternative to the DO-statement</li> <li>Partial nesting of FORALL and WHERE statements</li> <li>Masked ELSEWHERE</li> <li>Pure procedures</li> <li>Elemental procedures</li> <li>Pure procedures in specification expressions</li> <li>Revised MINLOC and MAXLOC</li> <li>Extensions to CEILING and FLOOR with the KIND keyword argument</li> <li>Pointer initialization</li> <li>Default initialization of derived type objects</li> <li>Increased compatibility with IEEE arithmetic</li> <li>A CPU_TIME intrinsic subroutine</li> <li>A function NULL to nullify a pointer</li> <li>Automatic deallocation of allocatable arrays at exit of scoping unit</li> <li>Comments in NAMELIST at input</li> <li>Minimal field at input</li> <li>Complete version of END INTERFACE</li> </ul> </li> <li>Deleted Features<ul> <li>real and double precision DO loop index variables</li> <li>branching to END IF from an outer block</li> <li>PAUSE statements</li> <li>ASSIGN statements and assigned GO TO statements and the use of an assigned     integer as a FORMAT specification</li> <li>Hollerith editing in FORMAT</li> <li>See http://www.nsc.liu.se/~boein/f77to90/f95.html#17.5</li> </ul> </li> </ul>"},{"location":"Documentation/languages/fortran/f90/#references","title":"References","text":"<ul> <li>http://www.fortran.com/fortran/ Pointer to everything Fortran</li> <li>http://meteora.ucsd.edu/~pierce/fxdr_home_page.html Subroutines to do unformatted I/O across platforms, provided by David Pierce at UCSD</li> <li>http://www.nsc.liu.se/~boein/f77to90/a5.html A good reference for intrinsic functions</li> <li>https://wg5-fortran.org/N1551-N1600/N1579.pdfNew Features of Fortran 2003</li> <li>https://wg5-fortran.org/N1701-N1750/N1729.pdfNew Features of Fortran 2008</li> <li>http://www.nsc.liu.se/~boein/f77to90/ Fortran 90 for the Fortran 77 Programmer</li> <li>Fortran 90 Handbook Complete ANSI/ISO Reference. Jeanne Adams, Walt Brainerd, Jeanne Martin, Brian Smith, Jerrold Wagener</li> <li>Fortran 90 Programming. T. Ellis, Ivor Philips, Thomas Lahey</li> <li>https://github.com/llvm/llvm-project/blob/master/flang/docs/FortranForCProgrammers.md</li> <li>FFT stuff</li> <li>Fortran 95 and beyond</li> </ul>"},{"location":"Documentation/languages/python/","title":"Python","text":""},{"location":"Documentation/languages/python/#eagle-tutorials","title":"Eagle tutorials","text":"<ul> <li>Python environments : Utilize a specific version of Python on Eagle and install packages</li> <li>Dask : Parallelize your Python code </li> <li>Jupyter notebooks : Run interactive notebooks on Eagle</li> </ul>"},{"location":"Documentation/languages/python/#hpc-python","title":"HPC Python","text":"<p>Links to External resources:</p> <ul> <li>MPI4PY Python bindings to use MPI to distribute computations across cluster nodes</li> <li>Dask Easily launch Dask workers on one node or across nodes</li> <li>Numba Optimize your Python code to run faster</li> <li>PyCUDA Utilize GPUs to accelerate computations</li> </ul>"},{"location":"Documentation/languages/python/NREL_python/","title":"Python on NREL HPC","text":"<p>By design, the HPC is a time-shared multi-machine system which necessarily warrants some nuanced consideration for how environments are managed relative to a single machine with a single user. Sometimes, the default workflow for environment creation and usage is not the most optimal for some use-cases.</p> <p>Below is a list of common pitfalls that users have encountered historically while using Python and Anaconda on NREL HPC. </p> <ul> <li>Running a SLURM job that uses a <code>conda</code> environment which is stored in <code>$HOME</code>.</li> <li>Exhausting the <code>$HOME</code> storage quota (50GB on the current HPC system) usually because of conda's package cache combined with their user environments.</li> <li>Trying to share a <code>conda</code> environment from another user's <code>/home</code> directory.</li> <li>Forgetting to install <code>jupyter</code> in a new conda environment, resulting in using the <code>base</code> installation's version which doesn't have your dependencies installed.</li> </ul> <p>Let's discuss strategies to mitigate or avoid these kinds of problems</p>"},{"location":"Documentation/languages/python/NREL_python/#installing-conda-environments-in-different-directories","title":"Installing Conda Environments in Different Directories","text":"<p>By default, <code>conda</code> will install new environments in <code>$HOME/.conda</code>. Generally speaking, this a sensible default\u2014it just happens to be the starting point to frequent issues that users have experienced historically. Something to consider is that <code>conda</code> has a <code>--prefix</code> flag which allows one to arbitrate where a conda environment gets installed to, notably allowing you to place environments on other file-systems and block devices besides the <code>/home</code> network-storage that is mounted on NREL HPC systems.</p> <p>For example, here is how one might create a project in their <code>/scratch</code> directory:</p> <pre><code>ENV_PREFIX=\"/scratch/$USER/demo_scratch_env\"\n\nimport os ; os.environ['ENV_PREFIX']=ENV_PREFIX  # Export this variable for cells below \n</code></pre> <pre><code>!conda create --quiet --use-local --yes \\\n    --prefix $ENV_PREFIX   # `--prefix` in action \\\n    python=3.7\n</code></pre> <pre><code>Collecting package metadata: ...working... done\nSolving environment: ...working... done\n\n## Package Plan ##\n\n  environment location: /scratch/mbartlet/demo_scratch_env\n\n\n\nPreparing transaction: ...working... done\nVerifying transaction: ...working... done\nExecuting transaction: ...working... done\n</code></pre> <pre><code>!ls -ld $ENV_PREFIX\n</code></pre> <pre><code>drwxr-xr-x. 3 mbartlet mbartlet 4096 Dec  3 11:10 /scratch/mbartlet/demo_scratch_env\n</code></pre> <pre><code># Delete the demo environment for cleanliness\n!conda-env remove --yes --quiet --prefix $ENV_PREFIX &amp;&gt;/dev/null\n</code></pre> <p>Below is a table which discusses the pros and cons of each block-device mount on NREL HPC as a location for storing your software environments.</p> Block-device mounts Situations where you would want to use this block device for your conda environments Caveats to consider when using this mount /home <code>$HOME/.conda</code> is the default location for environments. For one-off environments,  or if you don't create environments often, this is a reasonable location for your environments and doesn't require any extra flags or parameters. Files in \\$HOME will not be purged so long as you have an active NREL HPC account. However, \\$HOME is limited to a 50GB storage quota so you may have to take care to monitor your storage footprint. /scratch <code>/scratch</code> or <code>/projects</code> is ultimately where you want your environment to end up if your jobs have more than 1 node\u2014if your environment is in <code>/home</code> then every node in your job will be competing for read-access over a non-parallel network fabric to the source files of your environment. <code>/scratch</code> provides simultaneous access to all the nodes. A sensible approach is copying your environments from <code>/home</code> to <code>/scratch</code> as part of your job's initialization. <code>/scratch</code> storage is unlimited. <code>/scratch</code> is a parallel filesystem, meaning simultaneous filesystem operations by several nodes is possible and performant. However, the contents of <code>/scratch</code> are subject to purge after 28 days of inactivity. /projects This is a great place to put a conda environment that you anticipate sharing with your colleagues who are also working on the project. You can structure the permissions such that others in the project have read-only, write-only, or no access (we also encourage restoring these permissions at a later date so others on the project can manage your files without a hassle). <code>/projects</code> is also a parallel filesystem which reaps the same benefits as mentioned above. However, access to projects is contingent on having access to an HPC project allocation. Moreover, the storage quota allotted to each project is relative to the reasonableness of its requested needs, although a conda environment is very unlikely to have a significant storage footprint. <p>As mentioned above, let's demonstrate one might go about copying an environment from <code>/home</code> to <code>/scratch</code> in a SLURM job. The below cell will generate a nice code block based on variables used earlier in this notebook, as well as environment variables within your user account:</p> <pre><code># Acquire a default project handle to procedurally generate a SLURM job\nimport subprocess\n\ncommand = \"/nopt/nrel/utils/bin/hours_report | tail -1 | awk '{print $1}'\" # Grab a valid project handle\n\ncommand_array = [\n    '/bin/bash',\n    '-c',\n    command\n]\n\nproject_handle = subprocess.run(command_array, stdout=subprocess.PIPE).stdout.decode('utf-8')[:-1]\n\nimport os ; os.environ['DEFAULT_HANDLE'] = project_handle  # Export handle for cells below\n</code></pre> <pre><code>!echo $DEFAULT_HANDLE\n</code></pre> <pre><code>wks\n</code></pre> <pre><code>conda_home_env=\"py3\"\n</code></pre> <pre><code># Acquire info about the default conda environment\nimport subprocess\n\ncommand = f\"module load conda &amp;&amp; . activate {conda_home_env} &amp;&amp; echo $CONDA_PREFIX\"\n\ncommand_array = [\n    '/bin/bash',\n    '-lc',         # Have to run this from a login-shell\n    command\n]\n\nconda_home_env_prefix = subprocess.run(command_array, stdout=subprocess.PIPE).stdout.decode('utf-8')[:-1]\n\nimport os ; os.environ['CONDA_HOME_ENV_PREFIX'] = conda_home_env_prefix  # Export handle for cells below\n</code></pre> <pre><code>!echo $CONDA_HOME_ENV_PREFIX\n</code></pre> <pre><code>/home/mbartlet/.conda/envs/py3\n</code></pre> <pre><code>from IPython.display import Markdown as md\nfrom os import environ as env\n\nSCRATCH_ENV=f\"/scratch/{env['USER']}/home_conda_clone\"\n\nbody=f\"\"\"\n```bash\n#!/usr/bin/env bash\n#SBATCH --account {env['DEFAULT_HANDLE']}\n#SBATCH --time 5\n#SBATCH --partition debug\n#SBATCH --nodes 2\n\nexport SCRATCH_ENV=\"{SCRATCH_ENV}\"\nrsync -avz --ignore-existing \"{env['CONDA_HOME_ENV_PREFIX']}\" \"$SCRATCH_ENV\" &amp;&gt;/dev/null\n\nsrun bash -l &lt;&lt;EOF\nmodule purge\nmodule load conda\n. activate \"$SCRATCH_ENV\"\nwhich python\nEOF\n\nrm -rf \"$SCRATCH_ENV\"  # Optional clean-up\n\"\"\"\n</code></pre> <pre><code>#!/usr/bin/env bash\n#SBATCH --account wks\n#SBATCH --time 5\n#SBATCH --partition debug\n#SBATCH --nodes 2\n\nexport SCRATCH_ENV=\"/scratch/mbartlet/home_conda_clone\"\nrsync -avz --ignore-existing \"/home/mbartlet/.conda/envs/py3\" \"$SCRATCH_ENV\" &amp;&gt;/dev/null\n\nsrun bash -l &lt;&lt;EOF\nmodule purge\nmodule load conda\n. activate \"$SCRATCH_ENV\"\nwhich python\nEOF\n\nrm -rf \"$SCRATCH_ENV\"  # Optional clean-up\n</code></pre> <p>And after running what was generated above: <pre><code>[mbartlet@el1 ~] $ cat slurm-1845968.out\n/scratch/mbartlet/home_conda_clone/bin/python\n/scratch/mbartlet/home_conda_clone/bin/python\n</code></pre> Which shows both nodes sourced the environment from <code>/scratch</code></p>"},{"location":"Documentation/languages/python/dask/dask/","title":"Dask","text":"<p>Dask provides a way to parallelize Python code either on a single node or across the cluster. It is similar to the functionality provided by Apache Spark, with easier setup. It provides a similar API to other common Python packages such as NumPY, Pandas, and others. </p>"},{"location":"Documentation/languages/python/dask/dask/#dask-single-node","title":"Dask single node","text":"<p>Dask can be used locally on your laptop or an individual node. Additionally, it provides wrappers for multiprocessing and threadpools. The advantage of using <code>LocalCluster</code> though is you can easily drop in another cluster configuration to further parallelize. </p> <pre><code>from distributed import Client, LocalCluster\nimport dask\nimport time\nimport random \n\n@dask.delayed\ndef inc(x):\n    time.sleep(random.random())\n    return x + 1\n\n@dask.delayed\ndef dec(x):\n    time.sleep(random.random())\n    return x - 1\n\n@dask.delayed\ndef add(x, y):\n    time.sleep(random.random())\n    return x + y\n\ndef main ():\n   cluster = LocalCluster(n_workers=2)\n   client = Client(cluster)\n   zs = []\n   for i in range(256):\n      x = inc(i)\n      y = dec(x)\n      z = add(x, y)\n      zs.append(z)\n\n   result = dask.compute(*zs)\n   print (result)\n\n\nif __name__ == \"__main__\":\n   main()\n</code></pre>"},{"location":"Documentation/languages/python/dask/dask/#dask-mpi","title":"Dask MPI","text":"<p>Dask-MPI can be used to parallelize calculations across a number of nodes as part of a batch job submitted to slurm. Dask will automatically create a scheduler on rank 0 and workers will be created on all other ranks. </p>"},{"location":"Documentation/languages/python/dask/dask/#install","title":"Install","text":"<p>Note: The version of dask-mpi installed via Conda may be incompatible with the MPI libaries on Eagle. Use the pip install instead. </p> <pre><code>conda create -n daskmpi python=3.7\nconda activate daskmpi\npip install dask-mpi\n</code></pre> <p>Python script: This script holds the calculation to be performed in the test function. The script relies on the Dask cluster setup on MPI which is created in the  <pre><code>from distributed import Client, LocalCluster\nimport dask\nimport time\nfrom dask_mpi import initialize\nimport random \n\n@dask.delayed\ndef inc(x):\n    time.sleep(random.random())\n    return x + 1\n\n@dask.delayed\ndef dec(x):\n    time.sleep(random.random())\n    return x - 1\n\n@dask.delayed\ndef add(x, y):\n    time.sleep(random.random())\n    return x + y\n\ndef main ():\n   initialize(nanny=False,\n      interface='ib0',\n      protocol='tcp',\n      memory_limit=0.8,\n      local_directory='/tmp/scratch/dask',\n      nthreads=1)\n\n   client = Client()\n   zs = []\n   for i in range(256):\n      x = inc(i)\n      y = dec(x)\n      z = add(x, y)\n      zs.append(z)\n\n   result = dask.compute(*zs)\n   print (result)\n\n\nif __name__ == \"__main__\":\n   main()\n</code></pre></p> <p>Running the above script with MPI will automatically set a Dask worker on each MPI rank.  <pre><code>mpiexec -np 30 python dask_mpi.py\n</code></pre></p>"},{"location":"Documentation/languages/python/dask/dask/#dask-jobqueue","title":"Dask jobqueue","text":"<p>Dask can also run using the Slurm scheduler already installed on Eagle. The Jobqueue library can handle submission of a computation to the cluster. This is particularly useful when running an interactive notebook or similar and you need to scale workers. </p> <pre><code>import dask\nimport time\nfrom dask_jobqueue import SLURMCluster\nfrom distributed import Client\nimport random \n\n@dask.delayed\ndef inc(x):\n    time.sleep(random.random())\n    return x + 1\n\n@dask.delayed\ndef dec(x):\n    time.sleep(random.random())\n    return x - 1\n\n@dask.delayed\ndef add(x, y):\n    time.sleep(random.random())\n    return x + y\n\ndef main ():\n   cluster = SLURMCluster(\n      cores=18,\n      memory='24GB',\n      queue='short',\n      project='hpcapps',\n      walltime='00:30:00',\n      interface='ib0',\n      processes=18,\n   )\n   cluster.scale(jobs=2)\n\n   client = Client(cluster)\n   zs = []\n   for i in range(256):\n      x = inc(i)\n      y = dec(x)\n      z = add(x, y)\n      zs.append(z)\n\n\n   result = dask.compute(*zs)\n   print (result)\n\n\nif __name__ == \"__main__\":\n   main()\n</code></pre>"},{"location":"Documentation/languages/python/dask/dask/#references","title":"References","text":"<p>Dask documentation</p> <p>Dask Jobqueue</p> <p>Dask MPI</p>"},{"location":"blog/2020-12-01-numba/","title":"Speeding up Python Code with Numba","text":"<p>Numba is a just in time (JIT) compiler for Python and NumPy code. From their official website, \"Numba translates Python functions to optimized machine code at runtime using the industry-standard LLVM compiler library. Numba-compiled numerical algorithms in Python can approach the speeds of C or FORTRAN.\"</p> <pre><code>@jit(nopython=True)\ndef function_to_be_compiled():\n    # Standard numerical/NumPy code here\n    ...\n</code></pre> <p>Importantly, many functions require no changes or refactoring to gain this speedup.  In this getting-started guide, we build an example environment on Eagle, test the performance of a Numba-compiled function using the most common implementation of the <code>@jit</code> decorator, and discuss what sorts of functions will see performance improvements when compiled.</p>"},{"location":"blog/2021-05-06-tf/","title":"Faster Machine Learning with Custom Built TensorFlow on Eagle","text":"<p>TensorFlow is a widely used and powerful symbolic math library commonly used for a variety of machine learning techniques. TensorFlow has built in API support for regression, clustering, classification, hidden Markov models, neural networks, reinforcement learning, as well as, a variety of activation functions, loss function, and optimizers. TensorFlow has received growing adoption among scientists, researchers, and industry professionals for its broad applicability and flexibility.</p> <p>TensorFlow versions obtained from <code>pip</code> or <code>conda</code> installs may not be optimized for the CPU and GPU architectures found on Eagle. To address this, pre-compiled versions which are optimized both for the CPU and GPU architectures have been created and offer computational benefits compared to other installation approaches. These versions can easily be installed from the <code>wheels</code> provided in <code>/nopt/nrel/apps/wheels/</code> which contains different TensorFlow versions. </p> <p>Here is an example of how you can install an optimized version of TensorFlow to your environment.  <pre><code>pip install --upgrade --no-deps --force-reinstall /nopt/nrel/apps/wheels/tensorflow-2.4.0-cp38-cp38-linux_x86_64.whl\n</code></pre> These builds provide a significant advantage as illustrated below over the standard <code>conda</code> install of TensorFlow.  </p> <p>A recent tutorial was given on this topic, for more information see the recording or checkout the tutorial materials</p>"},{"location":"blog/2021-06-18-srun/","title":"Using srun to Launch Applications Under Slurm","text":""},{"location":"blog/2021-06-18-srun/#subjects-covered","title":"Subjects covered","text":"<ol> <li>Basics</li> <li>Pointers to Examples</li> <li>Why not just use mpiexec/mpirun?</li> <li>Simple runs</li> <li>Threaded (OpenMP) runs</li> <li>Hybrid MPI/OpenMPI</li> <li>MPMD - a simple distribution</li> <li>MPMD multinode</li> </ol>"},{"location":"blog/2021-06-18-srun/#1-basics","title":"1. Basics","text":"<p>Eagle uses the Slurm scheduler and applications run on a compute node must be run via the scheduler.  For batch runs users write a script and submit the script using the sbatch command.  The script tells the scheduler what resources are required including a limit on the time to run.  The script also normally contains \"charging\" or account information.</p> <p>Here is a very basic script that just runs hostname to list the nodes allocated for a job.</p> <pre><code>#!/bin/bash\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=1\n#SBATCH --time=00:01:00\n#SBATCH --account=hpcapps \n\n\nsrun hostname\n</code></pre> <p>Note we used the srun command to launch multiple (parallel) instances of our application hostname.  </p> <p>This article primarily discusses options for the srun command to enable good parallel execution.  In the script above we have asked for two nodes --nodes=2 and each node will run a single instance of hostname --ntasks-per-node=1.  If srun is not given options on the command line it will determine the number of tasks to run from the arguments in the header.  Thus our output from the script given above will be two lines, a list of nodes allocated for the job.  </p>"},{"location":"blog/2021-06-18-srun/#2-pointers-to-examples","title":"2. Pointers to examples","text":"<p>The page https://www.nrel.gov/hpc/eagle-batch-jobs.html has information about running jobs under Slurm including a link to example batch scripts.  The page https://github.com/NREL/HPC/tree/master/slurm has many slurm examples ranging from simple to complex.  This article is based on the second page.  </p>"},{"location":"blog/2021-06-18-srun/#3-why-not-just-use-mpiexecmpirun","title":"3. Why not just use mpiexec/mpirun?","text":"<p>The srun command is an integral part of the Slurm scheduling system.  It \"knows\" the configuration of the machine and recognizes the environmental variables set by the scheduler, such as cores per nodes.  Mpiexec and mpirun come with the MPI compilers. The amount of integration with the scheduler is implementation and install methodology dependent.  They may not enable the best performance for your applications.  In some cases they flat out just don't work correctly on Eagle.  For example, when trying to run MPMD applications (different programs running on different cores) using the mpt version of mpiexec, the same programs gets launched on all cores. </p>"},{"location":"blog/2021-06-18-srun/#4-simple-runs","title":"4. Simple runs","text":"<p>For our srun examples we will use two glorified \"Hello World\" programs, one in Fortran and the other in C.  They are essentially the same program written in the two languages.  They can be compiled as MPI, OpenMP, or as hybrid MPI/OpenMP.  They are available from the NREL HPC repository https://github.com/NREL/HPC.git in the slurm/source directory or by running the wget commands shown below.  </p> <p>wget https://raw.githubusercontent.com/NREL/HPC/master/slurm/source/fhostone.f90 wget https://raw.githubusercontent.com/NREL/HPC/master/slurm/source/mympi.f90 wget https://raw.githubusercontent.com/NREL/HPC/master/slurm/source/phostone.c wget https://raw.githubusercontent.com/NREL/HPC/master/slurm/source/makehello -O makefile</p> <p>After the files are downloaded you can build the programs </p>"},{"location":"blog/2021-06-18-srun/#using-the-mpt-mpi-compilers","title":"using the mpt MPI compilers","text":"<pre><code>module purge\nmodule load mpt gcc/10.1.0\nmake\n</code></pre>"},{"location":"blog/2021-06-18-srun/#or-using-intel-mpi-compilers","title":"or using Intel MPI compilers","text":"<pre><code>module purge\nmodule load intel-mpi gcc/10.1.0\nmake\n</code></pre> <p>You will end up with the executables:</p> <pre><code>fomp         - Fortran Openmp program\nfhybrid      - Fortran hybrid MPI/Openmp program\nfmpi         - Fortran MPI program\ncomp         - C hybrid Openmp program\nchybrid      - C hybrid MPI/Openmp program\ncmpi         - C MPI program\n</code></pre> <p>These programs have many options.  Running with the command line option -h will show them. Not all options are applicable for all versions.  Run without options the programs just print the hostname on which they were run.</p> <p>We look at our simple example again.  Here we ask for 2 nodes, 4 tasks per node for a total of 8 tasks.</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=\"hostname\"\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=4\n#SBATCH --ntasks=8\n#SBATCH --time=00:10:00\n\nsrun ./cmpi\n</code></pre> <p>This will produce (sorted) output like:</p> <pre><code>r105u33\nr105u33\nr105u33\nr105u33\nr105u37\nr105u37\nr105u37\nr105u37\n</code></pre> <p>In the above script we have nodes,ntasks-per-node and ntasks.  You do not need to specify all three parameters but values that are specified must be consistent.</p> <ul> <li>If nodes is not specified it will default to 1.  </li> <li>If ntasks is not specified it will default to 1 tasks per node.  </li> <li>You can put --ntasks-per-node and/or --ntasks on the srun line.  For example, to run a total of 9 tasks, 5 on one node and 4 on the second:</li> </ul> <pre><code>#!/bin/bash\n#SBATCH --job-name=\"hostname\"\n#SBATCH --nodes=2\n#SBATCH --time=00:10:00\n\nsrun --ntasks=9 ./cmpi\n</code></pre>"},{"location":"blog/2021-06-18-srun/#5-threaded-openmp-runs","title":"5. Threaded (OpenMP) runs","text":"<p>The variable used to tell the operating system how many threads to use for an OpenMP program is OMP_NUM_THREADS.  In the ideal world you could just set OMP_NUM_THREADS to a value, say 36, the number of cores on each Eagle node, and each thread would be assigned to a core.  Unfortunately without setting additional variables you will get the requested number of threads but threads might not be spread across all cores.  This can result in a significant slowdown.  For a program that is computationally intensive if two threads get mapped to the same core the runtime will increase 100%.  If all threads end up on the same core, the slowdown could actually be greater than the number of cores.  </p> <p>Our example programs, phostone.c and fhostone.f90, have a nice feature.  If you add -F to the command line they will produce a report showing on which core each thread runs. We are going to look at the C version of the code and compile it with both the Intel version of C, icc and with the Gnu compiler gcc.  </p> <p><pre><code>ml comp-intel/2020.1.217 gcc/10.1.0\ngcc -fopenmp -DNOMPI phostone.c -o comp.gcc\nicc -fopenmp -DNOMPI phostone.c -o comp.icc\n</code></pre> Run the script...</p> <p><pre><code>#!/bin/bash\n#SBATCH --job-name=\"hostname\"\n#SBATCH --cpus-per-task=36\n## ask for 10 minutes\n#SBATCH --time=00:10:00\n#SBATCH --nodes=1\n#SBATCH --partition=debug\nexport OMP_NUM_THREADS=36\n\nsrun ./comp.gcc -F &gt; gcc.out\nsrun ./comp.gcc -F &gt; icc.out\n</code></pre> Note we have added the line #SBATCH --cpus-per-task=36.  cpus-per-task should match the value of OMP_NUM_THREADS.</p> <p>We now look at the sorted head of each of the output files</p> <pre><code>el3:nslurm&gt; cat icc.out | sort -k6,6\ntask    thread             node name  first task    # on node  core\n0000      0030               r5i7n35        0000         0000  0000\n0000      0001               r5i7n35        0000         0000  0001\n0000      0034               r5i7n35        0000         0000  0001\n0000      0002               r5i7n35        0000         0000  0002\n0000      0035               r5i7n35        0000         0000  0002\n0000      0032               r5i7n35        0000         0000  0003\n. . .\n\nel3:nslurm&gt; cat gcc.out | sort -k6,6\ntask    thread             node name  first task    # on node  core\n0000      0031               r5i7n35        0000         0000  0000\n0000      0001               r5i7n35        0000         0000  0001\n0000      0002               r5i7n35        0000         0000  0002\n0000      0034               r5i7n35        0000         0000  0002\n0000      0003               r5i7n35        0000         0000  0003\n0000      0004               r5i7n35        0000         0000  0004\n. . .\n</code></pre> <p>The last column shows the core on which a thread is run.  We see that there is duplication of cores, potentially leading to poor performance.  </p> <p>There are two sets of environmental variables that can be used to map threads to cores.  One variable is specific to the Intel compilers, KMP_AFFINITY.  The others are general for OpenMP compilers and should work for any OpenMP compiler, OMP_PLACES and OMP_PROC_BIND.  These are documented at: </p> <p>https://software.intel.com/content/www/us/en/develop/documentation/cpp-compiler-developer-guide-and-reference/top/optimization-and-programming-guide/openmp-support/openmp-library-support/thread-affinity-interface-linux-and-windows.html</p> <p>https://www.openmp.org/spec-html/5.0/openmpse52.html</p> <p>https://www.openmp.org/spec-html/5.0/openmpse53.html</p> <p>We ran each version of our code 100 times with 5 different settings.  The settings were:</p> <ol> <li>export KMP_AFFINITY=verbose,scatter</li> <li>export KMP_AFFINITY=verbose,compact</li> <li>export OMP_PLACES=coresexport OMP_PROC_BIND=spread</li> <li>export OMP_PLACES=coresexport OMP_PROC_BIND=close</li> <li>NONE</li> </ol> <p>The table below shows the results of our runs.  In particular, it shows the minimum number of cores used with the particular settings.  36 is the desired value.  We see that for gcc the following settings worked well:  </p> <p>export OMP_PLACES=coresexport OMP_PROC_BIND=spread </p> <p>or  export OMP_PLACES=coresexport OMP_PROC_BIND=clone</p> <p>Setting KMP_AFFINITY did not work for gcc but for the Intel compiler KMP_AFFINITY also gave good results.  </p> Compiler Setting Workedmincores meancores maxcores gcc cores, close yes 36 36 36 gcc cores, spread yes 36 36 36 gcc KMP_AFFINITY=compactno 25 34.18 36 gcc KMP_AFFINITY=scatter no 26 34.56 36 gcc none no 28 34.14 36 icc cores, close yes 36 36 36 icc cores, spread yes 36 36 36 icc KMP_AFFINITY=compactyes 36 36 36 icc KMP_AFFINITY=scatter yes 36 36 36 icc none no 19 23.56 29 <p>So our final working script for OpenMP programs could be:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=\"hostname\"\n#SBATCH --cpus-per-task=36\n## ask for 10 minutes\n#SBATCH --time=00:10:00\n#SBATCH --nodes=1\n#SBATCH --partition=debug\nexport OMP_NUM_THREADS=36\n\nexport OMP_PLACES=cores\nexport OMP_PROC_BIND=close\n#export OMP_PROC_BIND=spread\n\nsrun ./comp.gcc -F &gt; gcc.out\nsrun ./comp.gcc -F &gt; icc.out\n</code></pre> <p>When a job is run the SLURM_CPUS_PER_TASK is set to cpus-per-task so you may want to </p> <pre><code>export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n</code></pre> <p>More on this in the next section.</p>"},{"location":"blog/2021-06-18-srun/#6-hybrid-mpiopenmpi","title":"6. Hybrid MPI/OpenMPI","text":"<p>The next script is just an extension of the last.  We now run hybrid, a combination of MPI and OpenMP.  Our base example programs, fhostame.f90 and phostname.c can be compiled in hybrid mode as well as in pure MPI and pure OpenMP.</p> <p>First we look at the (sorted) output from our program run in hybrid mode with 4 tasks on two nodes and 4 threads.  </p> <pre><code>MPI VERSION Intel(R) MPI Library 2019 Update 7 for Linux* OS\ntask    thread             node name  first task    # on node  core\n0000      0000                r5i0n4        0000         0000  0000\n0000      0001                r5i0n4        0000         0000  0004\n0000      0002                r5i0n4        0000         0000  0009\n0000      0003                r5i0n4        0000         0000  0014\n0001      0000                r5i0n4        0000         0001  0018\n0001      0001                r5i0n4        0000         0001  0022\n0001      0002                r5i0n4        0000         0001  0027\n0001      0003                r5i0n4        0000         0001  0032\n0002      0000               r5i0n28        0002         0000  0000\n0002      0001               r5i0n28        0002         0000  0004\n0002      0002               r5i0n28        0002         0000  0009\n0002      0003               r5i0n28        0002         0000  0014\n0003      0000               r5i0n28        0002         0001  0018\n0003      0001               r5i0n28        0002         0001  0022\n0003      0002               r5i0n28        0002         0001  0027\n0003      0003               r5i0n28        0002         0001  0032\ntotal time      3.009\n</code></pre> <p>The first column is the MPI task number followed by the thread, then the node.  The last column is the core on which that give task/thread was run.  We can cat a list of unique combinations of nodes and cores by piping the file into </p> <pre><code>grep ^0 | awk '{print $3, $6}' | sort -u | wc -l`\n</code></pre> <p>We get 16 which is the number of tasks times the number of threads.  That is, we have each task/thread assigned to its own core.  This will give good performance.  The script below runs  on a fixed number of tasks (4 = 2 per node * 2 nodes) and using from 1 to cpus-per-task=18 threads.  </p> <p>The variable SLURM_CPUS_PER_TASK is set by slurm to be cpus-per-task.  After the srun line we post process the output to report core usage.  </p> <p><pre><code>#!/bin/bash\n#SBATCH --account=hpcapps \n#SBATCH --time=00:10:00 \n#SBATCH --nodes=2 \n#SBATCH --partition=short \n#SBATCH --cpus-per-task=18\n#SBATCH --ntasks=4\n\nmodule purge\nmodule load intel-mpi/2020.1.217 gcc/10.1.0\n\n\nexport OMP_PLACES=cores\nexport OMP_PROC_BIND=spread\n\necho \"CPT TASKS THREADS  cores\"\nfor n in `seq 1 $SLURM_CPUS_PER_TASK` ; do\n    request=`python -c \"print($n*$SLURM_NTASKS)\"`\n    have=72\n    if ((request &lt;= have)); then\n      export OMP_NUM_THREADS=$n\n      srun  --ntasks-per-core=1 -n $SLURM_NTASKS ./phostone.icc -F -t 3 &gt; out.$SLURM_NTASKS.$OMP_NUM_THREADS\n# post process\n      cores=`cat out.$SLURM_NTASKS.$OMP_NUM_THREADS | grep ^0 | awk '{print $3, $6}' | sort -u | wc -l`\n      echo $SLURM_CPUS_PER_TASK \"    \" $SLURM_NTASKS \"    \" $OMP_NUM_THREADS \"    \" $cores\n    fi\ndone\n</code></pre> Our final output from this script is:</p> <pre><code>el3:stuff&gt; cat slurm-7002718.out\nCPT TASKS THREADS cores\n18      4      1      4\n18      4      2      8\n18      4      3      12\n18      4      4      16\n18      4      5      20\n18      4      6      24\n18      4      7      28\n18      4      8      32\n18      4      9      36\n18      4      10      40\n18      4      11      44\n18      4      12      48\n18      4      13      52\n18      4      14      56\n18      4      15      60\n18      4      16      64\n18      4      17      68\n18      4      18      72\nel3:stuff&gt; \n</code></pre> <p>The important lines are:</p> <pre><code>#SBATCH --cpus-per-task=18\n. . .\nexport OMP_PLACES=cores\nexport OMP_PROC_BIND=spread\n. . .\nsrun  --ntasks-per-core=1 -n $SLURM_NTASKS ./phostone.icc \n</code></pre> <p>We need to set cpus-per-task to tell slurm we are going to run multithreaded and how many cores we are going to use for our threads.  This should be set to the maximum number of threads per task we expect to use.</p> <p>We use the OMP variables to map threads to cores.  IMPORTANT: using KMP_AFFINTY will not give the desired results.  It will cause all threads for a task to be mapped to a single core.</p> <p>We can run this script for hybrid MPI/OpenMP programs as is or set the number of cpus-per-task and tasks on the sbatch command line.  For example:  </p> <pre><code>sbatch --cpus-per-task=9 --ntasks=8 simple\n</code></pre> <p>gives us:</p> <pre><code>el3:stuff&gt; cat slurm-7002858.out\nCPT TASKS THREADS  cores\n9      8      1      8\n9      8      2      16\n9      8      3      24\n9      8      4      32\n9      8      5      40\n9      8      6      48\n9      8      7      56\n9      8      8      64\n9      8      9      72\nel3:stuff&gt; \n</code></pre>"},{"location":"blog/2021-06-18-srun/#7-mpmd-a-simple-distribution","title":"7. MPMD - a simple distribution","text":"<p>Here we look at launching Multi Program Multi Data runs.   We use a the --multi-prog option with srun.  This involves creating a config_file that lists the  programs we are going to run along with the task ID.  See: https://computing.llnl.gov/tutorials/linux_clusters/multi-prog.html for a quick description of the format for the config_file.</p> <p>Here we create the file on the fly but it could be done beforehand.</p> <p>We have two MPI programs to run together, phostone and fhostone.  They are actually the same program written in C and Fortran. In the real world MPMD applications would maybe run a GUI or a manager for one task and rest doing compute.  </p> <p>The syntax for running MPMD programs is</p> <p>srun --multi-prog mapfile</p> <p>where mapfile is a config_file that lists the programs to run.</p> <p>It is possible to pass different arguments to each program as discussed in the link above.  Here we just add command line arguments for task 0.</p> <p>Our mapfile has 8 programs listed.  The even tasks are running phostone and the odd fhostone.  Our script uses two for loops to add lines to the mapfile and then uses sed to append command line arguments to the first line.  </p> <pre><code>#!/bin/bash\n#SBATCH --account=hpcapps \n#SBATCH --time=00:10:00 \n#SBATCH --nodes=1\n#SBATCH --partition=debug \n#SBATCH --cpus-per-task=1\n\n# create our mapfile\napp1=./phostone\nfor n in 0 2 4 6 ; do\n  echo $n $app1 &gt;&gt; mapfile\ndone\napp2=./fhostone\nfor n in 1 3 5 7 ; do\n  echo $n $app2 &gt;&gt; mapfile\ndone\n\n# add a command line option to the first line\n# sed does an in-place change to the first line\n# of our mapfile adding *-F*\nsed -i \"1 s/$/ -F /\" mapfile\n\ncat mapfile\n\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\nsrun -n8 --multi-prog mapfile\n</code></pre> <p>Here is the complete output including the mapfile and output from our two programs. Lines with three digits for core number were created by the Fortran version of the program.  </p> <pre><code>el3:stuff&gt; cat *7003104*\n0 ./phostone -F \n2 ./phostone\n4 ./phostone\n6 ./phostone\n1 ./fhostone\n3 ./fhostone\n5 ./fhostone\n7 ./fhostone\nMPI VERSION Intel(R) MPI Library 2019 Update 7 for Linux* OS\n\ntask    thread             node name  first task    # on node  core\n0000      0000               r1i7n35        0000         0000  0022\n0001      0000               r1i7n35        0001         0000   021\n0002      0000               r1i7n35        0000         0001  0027\n0003      0000               r1i7n35        0003         0000   023\n0004      0000               r1i7n35        0000         0002  0020\n0005      0000               r1i7n35        0005         0000   025\n0006      0000               r1i7n35        0000         0003  0026\n0007      0000               r1i7n35        0007         0000   019\nel3:stuff&gt; \n</code></pre>"},{"location":"blog/2021-06-18-srun/#8-mpmd-multinode","title":"8. MPMD multinode","text":"<p>Our final example again just extends the previous one.  We want to add the capability to launch different numbers of tasks on a set of nodes and at the same time have different programs on each of the nodes.  We create a mapfile to list the programs to run as was done above.  In this case for illustration  purposes we are running one copy of phostone and seven instances of fhostone.</p> <p>We add to that a hostfile that lists the nodes on which to run.  The hostfile has one host per MPI task.</p> <p><pre><code>#!/bin/bash\n#SBATCH --account=hpcapps \n#SBATCH --time=00:10:00 \n#SBATCH --nodes=2\n#SBATCH --partition=debug \n\nexport OMP_NUM_THREADS=1\n\n# Create our mapfile\nrm -rf mapfile\napp1=./phostone\nfor n in 0  ; do\n  echo $n $app1 &gt;&gt; mapfile\ndone\napp2=./fhostone\nfor n in 1 2 3 4 5 6 7 ; do\n  echo $n $app2 &gt;&gt; mapfile\ndone\n\n# Add a command line option to the first line\n# sed does an in-place change to the first line\n# of our mapfile adding *-F*\nsed -i \"1 s/$/ -F /\" mapfile\n\n# Count of each app to run on a node\ncounts=\"1 7\"\n\n# Get a list of nodes on a single line\nnodes=`scontrol show hostnames | tr '\\n' ' '`\n\n# Create our hostfile and tell slrum its name\nexport SLURM_HOSTFILE=hostlist\n\n# It is possible to do this in bash but\n# I think this is easier to understand\n# in python.  It uses the values for\n# counts and nodes set above.\npython -  &gt; $SLURM_HOSTFILE &lt;&lt; EOF\nc=\"$counts\".split()\nnodes=\"$nodes\".split()\nk=0\nfor i in c:\n  i=int(i)\n  node=nodes[k]\n  for j in range(0,i):\n      print(node)\n  k=k+1\nEOF\n\n\nsrun -n 8 --multi-prog mapfile\n</code></pre> Here is the output from our run including the mapfile and hostlist.  Notice that the first instance of the set of running programs is the C version.  It is the only thing running on the first nodes.  The rest of the MPI tasks are the Fortran version of the program running on the second node.</p> <pre><code>el3:stuff&gt; cat slurm-7003587.out | sort -k3,3 -k1,1\nMPI VERSION Intel(R) MPI Library 2019 Update 7 for Linux* OS\ntask    thread             node name  first task    # on node  core\n0000      0000               r102u34        0000         0000  0004\n0001      0000               r102u35        0001         0000   003\n0002      0000               r102u35        0002         0000   000\n0003      0000               r102u35        0001         0001   006\n0004      0000               r102u35        0004         0000   007\n0005      0000               r102u35        0001         0002   004\n0006      0000               r102u35        0002         0001   005\n0007      0000               r102u35        0001         0003   002\nel3:stuff&gt; cat mapfile\n0 ./phostone -F \n1 ./fhostone\n2 ./fhostone\n3 ./fhostone\n4 ./fhostone\n5 ./fhostone\n6 ./fhostone\n7 ./fhostone\nel3:stuff&gt; c\nel3:stuff&gt; cat hostlist\nr102u34\nr102u35\nr102u35\nr102u35\nr102u35\nr102u35\nr102u35\nr102u35\nel3:stuff&gt; \n</code></pre>"},{"location":"blog/2022-02-02-Changes_to_Slurm_srun_for_interactive_jobs/","title":"Changes to Slurm \"srun\" for Interactive Jobs","text":"<p>The Slurm job scheduler was upgraded during the recent system time on January 10, 2022.  One of the side effects of this was a change in the way Slurm handles job steps internally in certain cases.  This may affect the way some users run job steps with srun inside of interactive jobs <code>srun --pty $SHELL</code>, so we wanted to provide some guidance as we work on updating our documentation to reflect this change. </p> <p>When running an interactive job with <code>srun --pty $SHELL</code> and then launching job steps on a node, a second <code>srun</code> is often used \"inside\" the first srun to launch certain software.  For example, for users of Paraview, a Paraview server may be launched on an interactive node with <code>srun -n 8 pvserver --force-offscreen-rendering</code>.  (Certain GPU-enabled or MPI-enabled interactive software also functions in a similar manner.)</p> <p>This \"srun-inside-an-srun\" process will no longer function in the same way as in the past. Instead, the \"outer\" <code>srun</code> should be replaced with an <code>salloc</code> command. <code>salloc</code> will accept the same arguments as srun, but <code>--pty $SHELL</code> will no longer be required. <code>salloc</code> will automatically open a shell to the node once the job starts, and the \"inner\" <code>srun</code> can then be run successfully as normal.</p> <p>Other regular uses of srun and srun inside sbatch scripts should continue to behave as expected. </p> <p>For further technical details on this Slurm change, please see the Slurm 20.11 Release Notes regarding job steps, <code>srun</code>, and the new <code>--overlap</code> flag.</p>"},{"location":"blog/2022-03-05-local-io-performance/","title":"Eagle Local I/O Performance","text":"<p>We sometimes receive questions about disk types and I/O performance on compute nodes. Eagle has two network file systems. Qumulo provides /home and /nopt. It is NFS and is not considered a performance file system. /home has snapshots for restoration of lost data, but should not be used as a replacement for a source code repository like Git. Lustre is our performance file system, and it provides storage for the /scratch, /projects, /shared-projects and /datasets directories.</p> <p>Eagle also has two storage options on the compute nodes. /dev/shm is an in-memory space (shm: shared memory), which is fast, but you need to balance its usage with your job's memory usage as it is located directly in RAM. /tmp/scratch is physical storage. The type of storage and performance differ depending on the specific type of compute node. </p> <p>If we look under Eagle's Compute Node Hardware Details on the central NREL HPC website, there are nodes listed as having SATA drives, and nodes listed as having SSDs. Our SATA drives are still spinning disks, while SAS (serial attached SCSI) is how the SSDs are connected to the node. We would generally expect the nodes with SSDs to perform better. Let\u2019s test that out with a simple test. </p> <p>This following is a command we regularly use to verify Lustre OST (object storage target) performance. It\u2019s designed to write enough information so that you are seeing disk performance, and not just the performance of the storage controller of the disk: </p> <p><code>dd if=/dev/zero of=X bs=1M count=10k</code></p> <p>This is writing in file in chunks of 1M, 10k times, to X. It writes an 11GB file. The results:</p> <p>Lustre: 1.6 GB/s per OST</p> <p>Node /dev/shm: 2.8 GB/s</p> <p>Node SATA spinning disk: 2.4 GB/s</p> <p>Node SAS SSD: 2.4 GB/s</p> <p>Surprising! There is not a difference between the two local disks. Let\u2019s do the same test, but instead of writing in 1M chunks, we will write in 10M chunks which will write a 107GB file. For this case, Lustre and /dev/shm maintain performance, but here\u2019s what we get for the two local disk types:</p> <p>Node SATA spinning disk: 146 MB/s</p> <p>Node SAS SSD: 1.9 GB/s</p> <p>That is a rather drastic drop off in performance for the SATA disk. So how your data writes to disk can drastically affect performance. A lot of tiny files will look the same between the two disk types, one large continuous write would differ.</p>"},{"location":"blog/2022-10-04-python2to3/","title":"Running Legacy Python 2 Code on Eagle","text":""},{"location":"blog/2022-10-04-python2to3/#what-is-legacy-code","title":"What is Legacy Code?","text":"<p>One definition of \"legacy\" code or software is code was written in the past using currently outdated, obsolete, or otherwise deprecated, compilers, functions, methods, or methodology.</p> <p>While Python 2 was sunset on January 1, 2020 in favor of Python 3, there is still \"legacy\" Python 2 software that may need to be run on Eagle.  We always encourage Eagle users to upgrade their code to Python 3.x to continue receiving official updates, bug fixes, and security patches. But we do understand that there will always be code that is not worth porting to Python 3.  In those cases here are some options you have</p> <ol> <li>Set up a custom Python 2 environment</li> <li>Check for updates</li> <li>Find an alternative tool that fits your needs</li> <li>Convert Python 2 code to Python 3</li> </ol>"},{"location":"blog/2022-10-04-python2to3/#1-set-up-a-custom-python-2-environment-using-conda-or-containers","title":"1. Set up a custom Python 2 environment using Conda or containers","text":"<p>It is best to create this python environment within conda. For example,</p> <pre><code>conda create --name my_environment python=2\n</code></pre> <p>This conda environment can be made even more portable by using Docker or Apptainer. Currently, Eagle only supports Apptainer. Subsequently, the outputs of the Python 2 code can be incorporated, e.g., using a file-based approach.</p> <p>It is possible to run legacy Python 2 code in parallel within a container or a conda environment, but your mileage may vary.</p>"},{"location":"blog/2022-10-04-python2to3/#2-check-for-updates","title":"2. Check for updates","text":"<p>If the software or module is still under active development, it's highly likely that the authors have transitioned the software to Python 3.x.  If an updated version is available, you should strongly consider it over an outdated Python 2 version.  It will likely be more secure, have better performance, be more reliable, and have a longer shelf life for reproducibility in the future.</p>"},{"location":"blog/2022-10-04-python2to3/#3-find-an-alternative-tool-that-fits-your-needs","title":"3. Find an alternative tool that fits your needs","text":"<p>There are usually multiple alternatives to your software of concern that have the same or slightly different features. Some of the potential advantages and disadvantages include:</p> <p>Advantages</p> <ol> <li>If they are written in Python 3, they are newer and might offer better software support.</li> <li>They allow for efficient parallelism, enabling better use of Eagle.</li> <li>Leverages Python 3 features and packages, e.g., Abstract Base Classes.</li> </ol> <p>Disadvantages</p> <ol> <li>It may break your build environment.</li> <li>It may require a significant code rewrite to extract the best performance.</li> <li>Additional code may need to be written if the alternative does not have all the features as the Python 2 software.</li> </ol>"},{"location":"blog/2022-10-04-python2to3/#4-convert-python-2-code-to-python-3","title":"4. Convert Python 2 code to Python 3","text":"<p>This is an option when the Python 2 code under consideration does not have a lot of dependencies, you have the source code, and the software license allows you to make changes to the source code.  For scientific software, we do encourage developers to make this version jump as this enables code longevitiy and accessibility. Several online resource are available to enable porting your code to Python 3. Some of them include:</p> <ol> <li>Porting Python 2 Code to Python 3</li> <li>The Conservative Python 3 Porting Guide</li> <li>Supporting Python 3: An in-depth guide</li> <li>Python FAQ: How do I port to Python 3?</li> <li>How to Port Python 2 Code to Python 3</li> <li>2to3 \u2014 Automated Python 2 to 3 code translation</li> </ol> <p>Note that the above is not an exhasutive list and we encourage the user to look at other resources as well. Additionally, depending on the needs of your project, you can also reach out to HPC-Help@nrel.gov to help with this port.</p> <p>Remember, since Python 2 has been officially deprecated, more and more code is either being updated or rewritten entirely in Python 3 as time passes. Additionally, the community-maintained Python 2 packages in the default conda channels will likely disappear at some point in the future. Keeping your code modernized to the latest standards will help ensure both the longevity and reproducibility of your software and your results.</p>"},{"location":"blog/2022-12-19-windows_ssh/","title":"Workaround for Windows SSH \"Corrupted MAC on input\" Error","text":"<p>Some people who use Windows 10/11 computers to ssh to Eagle from a Windows command prompt, powershell, or via Visual Studio Code's SSH extension might receive an error message about a \"Corrupted MAC on input\" or \"message authentication code incorrect.\" This error is due to an outdated OpenSSL library included in Windows and a security-mandated change to ssh on Eagle. However, there is a functional workaround for this issue. (Note: If you are not experiencing the above error, you do not need and should not use the following workaround.)</p> <p>For command-line and Powershell ssh users, adding <code>-m hmac-sha2-512</code> to your ssh command will resolve the issue. For example: <code>ssh -m hmac-sha2-512 &lt;username&gt;@eagle.hpc.nrel.gov</code>.</p> <p>For VS Code SSH extension users, you will need to create an ssh config file on your local computer (~/.ssh/config), with a host entry for Eagle that specifies a new message authentication code:  <pre><code>Host eagle\n    HostName eagle.hpc.nrel.gov\n    MACs hmac-sha2-512\n</code></pre></p> <p>The configuration file will also apply to command-line ssh in Windows. This Visual Studio Blog post has further instructions on how to create the ssh configuration file for Windows and VS Code.</p>"},{"location":"blog/2023-01-10-using_specific_module_versions_on_hpc/","title":"Using Specific Module Versions on the HPC","text":"<p>Modules on NREL HPCs are updated to with newer versions with on a regular basis. Since Lmod, the underlying module system, sets the most recent version of a module as the default, a user's typical workflow may break if they are not specifying the exact module version in their scripts.</p> <p>For example, at the time of this writing, the current default module for Conda on Eagle is 4.9.2. If a user wishes to use conda v4.12.0, they must specify the version in the module command as</p> <pre><code>module load conda/4.12.0\n</code></pre> <p>The user can also create custom module files for their own use and point to them. For example, assuming a user has custom TCL or LUA module files in the following directory</p> <pre><code>/home/${USER}/private_modules/\n</code></pre> <p>They can use these module files by adding it to the module search path using the following command</p> <pre><code>module use -a /home/${USER}/private_modules/\n</code></pre> <p>Furthermore, if a user wishes to have these module paths available at all times, they can update their <code>.bash_profile</code> or <code>.bashrc</code> file in their home directory. For example by using the following command</p> <pre><code>echo 'module use -a /home/${USER}/private_modules/' &gt;&gt; /home/${USER}/.bash_profile\n</code></pre> <p>or a text editor. As a quick reminder, <code>.bash_profile</code> and <code>.bashrc</code> are simply configuration files that enable the user to customize your Linux or MacOS terminal experience, assuming they are using Bash as their shell (Similar to Command Prompt on Windows). The difference between <code>.bash_profile</code> and <code>.bashrc</code> is that the former is executed for login shells while the latter in executed for non-login shells, e.g., <code>.bash_profile</code> is executed when a user SSHs into Eagle, whereas <code>.bashrc</code> is executed when one opens a new terminal.</p>"}]}